{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Python Geospatial Engineering Practices","text":"<p>Hands-on modules covering async I/O, providers, APIs, testing, data, and performance for real-world geospatial systems. Each module is runnable with focused drills and tests.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ul> <li>Python 3.11+</li> <li>Create venv: <code>python -m venv .venv &amp;&amp; source .venv/bin/activate</code></li> <li>Install: <code>pip install -r requirements.txt</code></li> <li>Makefile (optional): <code>make install</code>, <code>make test</code>, <code>make run-api</code>, <code>make bench</code>, <code>make bench-local</code></li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>src/\n\u251c\u2500 day01_concurrency/      # Async I/O and bounded concurrency\n\u2502  \u251c\u2500 tile_fetcher.py      # CLI benchmark with retries/timeouts\n\u2502  \u2514\u2500 mock_tile_server.py  # Local FastAPI for safe latency/error simulation\n\u251c\u2500 day02_oop/              # Providers and clean interfaces\n\u2502  \u2514\u2500 providers/\n\u2502     \u251c\u2500 base.py | factory.py | csv_provider.py | geojson_provider.py | mvt_provider.py\n\u251c\u2500 day03_api/              # FastAPI drills (tiles + bbox streaming, metrics)\n\u2502  \u2514\u2500 app.py\n\u251c\u2500 day04_testing/          # Tests and examples\n\u2502  \u2514\u2500 tests/\n\u2502     \u251c\u2500 test_smoke.py | test_day03_api_validation.py | test_concurrency_day01.py\n\u251c\u2500 day05_data/             # Data + geospatial (Protobuf, R-tree)\n\u2502  \u2514\u2500 protobuf/road_segment.proto\n\u251c\u2500 day06_perf/             # Performance and observability\n\u2514\u2500 day07_mock/             # End-to-end mock project\n   \u2514\u2500 mock_test/\n      \u251c\u2500 api.py | road_network.py | roads.csv\n</code></pre>"},{"location":"#run-examples","title":"Run Examples","text":"<ul> <li>Day 1 benchmark: <code>python -m src.day01_concurrency.tile_fetcher --tile-count 50 --max-concurrency 20</code></li> <li>Local tiles (recommended): <code>make run-mock-tiles</code> then <code>make bench-local</code></li> <li>Day 3 API (dev): <code>uvicorn src.day03_api.app:app --reload</code> (docs at <code>/docs</code>, metrics at <code>/metrics</code>)</li> <li>Day 7 mock API: <code>uvicorn src.day07_mock.mock_test.api:app --reload</code></li> </ul>"},{"location":"#testing","title":"Testing","text":"<ul> <li>Run all: <code>pytest -q</code></li> <li>Targeted runs:</li> <li><code>pytest -q src/day04_testing/tests/test_smoke.py</code></li> <li><code>pytest -q src/day04_testing/tests/test_day03_api_validation.py</code></li> <li><code>pytest -q src/day04_testing/tests/test_concurrency_day01.py</code></li> </ul>"},{"location":"#docs-site-optional","title":"Docs Site (optional)","text":"<p>Generate a MkDocs + Material site from this repo: - <code>python convert_to_mkdocs.py</code> - <code>cd docs &amp;&amp; pip install -r requirements.txt &amp;&amp; mkdocs serve</code></p> <p>See <code>PLAN.md</code> for goals per day, <code>AGENTS.md</code> for contributor guidelines, and <code>MOCK_TEST.md</code> for the final mock brief.</p>"},{"location":"study_plan/","title":"Study Plan","text":"<p>Study plan content will be generated here.</p>"},{"location":"api/day01_tile_fetcher/","title":"Day 1 Tile Fetcher API","text":""},{"location":"api/day01_tile_fetcher/#src.day01_concurrency.tile_fetcher","title":"<code>src.day01_concurrency.tile_fetcher</code>","text":"<p>Tile Fetcher - Asynchronous Concurrency Example</p> <p>This module demonstrates key concepts in asynchronous programming and concurrent HTTP requests: - Async/await syntax and event loop management - Bounded concurrency with semaphores - Retry logic with exponential backoff - Performance benchmarking and comparison - Error handling for network operations</p> <p>The script fetches map tiles from OpenStreetMap servers and compares sequential vs concurrent performance.</p>"},{"location":"api/day01_tile_fetcher/#src.day01_concurrency.tile_fetcher.benchmark","title":"<code>benchmark(tile_count=20, max_concurrency=10, timeout_seconds=10.0, retries=2, global_timeout=None)</code>","text":"<p>Benchmark sequential vs concurrent tile fetching.</p> <p>This function demonstrates how to measure and compare performance between different approaches. It measures: - Total execution time - Individual request latencies - Throughput (tiles per second) - Percentile latencies (p50, p95, p99)</p> <p>Parameters:</p> Name Type Description Default <code>tile_count</code> <code>int</code> <p>Number of tiles to fetch for the benchmark</p> <code>20</code> <code>max_concurrency</code> <code>int</code> <p>Maximum concurrent requests for concurrent mode</p> <code>10</code> <code>timeout_seconds</code> <code>float</code> <p>Per-request timeout</p> <code>10.0</code> <code>retries</code> <code>int</code> <p>Number of retries per request</p> <code>2</code> <code>global_timeout</code> <code>Optional[float]</code> <p>Overall timeout for the entire benchmark</p> <code>None</code> Source code in <code>src/day01_concurrency/tile_fetcher.py</code> <pre><code>def benchmark(\n    tile_count: int = 20,\n    max_concurrency: int = 10,\n    timeout_seconds: float = 10.0,\n    retries: int = 2,\n    global_timeout: Optional[float] = None,\n) -&gt; None:\n    \"\"\"\n    Benchmark sequential vs concurrent tile fetching.\n\n    This function demonstrates how to measure and compare performance\n    between different approaches. It measures:\n    - Total execution time\n    - Individual request latencies\n    - Throughput (tiles per second)\n    - Percentile latencies (p50, p95, p99)\n\n    Args:\n        tile_count: Number of tiles to fetch for the benchmark\n        max_concurrency: Maximum concurrent requests for concurrent mode\n        timeout_seconds: Per-request timeout\n        retries: Number of retries per request\n        global_timeout: Overall timeout for the entire benchmark\n    \"\"\"\n    tiles = generate_sample_tiles(count=tile_count)\n\n    async def collect_latencies_seq() -&gt; List[float]:\n        \"\"\"Collect latencies for sequential fetching.\"\"\"\n        lats: List[float] = []\n        async with httpx.AsyncClient() as client:\n            for z, x, y in tiles:\n                start = time.perf_counter()  # High-precision timer\n                await fetch_tile(client, z, x, y, timeout_seconds=timeout_seconds, retries=retries)\n                lats.append(time.perf_counter() - start)\n        return lats\n\n    async def collect_latencies_con() -&gt; List[float]:\n        \"\"\"Collect latencies for concurrent fetching.\"\"\"\n        lats: List[float] = []\n        sem = asyncio.Semaphore(max_concurrency)\n        async with httpx.AsyncClient() as client:\n            async def one(z: int, x: int, y: int) -&gt; None:\n                \"\"\"Fetch one tile and record its latency.\"\"\"\n                async with sem:\n                    start = time.perf_counter()\n                    await fetch_tile(client, z, x, y, timeout_seconds=timeout_seconds, retries=retries)\n                    lats.append(time.perf_counter() - start)\n\n            # Create and run all tasks concurrently\n            await asyncio.gather(*(one(z, x, y) for z, x, y in tiles))\n        return lats\n\n    def run(coro):\n        \"\"\"Run a coroutine with optional global timeout.\"\"\"\n        if global_timeout is None:\n            return asyncio.run(coro)\n        try:\n            # Apply global timeout to prevent hanging benchmarks\n            return asyncio.run(asyncio.wait_for(coro, timeout=global_timeout))\n        except asyncio.TimeoutError:\n            print(\"Global timeout reached; partial results shown.\")\n            return []\n\n    # Benchmark sequential approach\n    print(f\"Benchmarking {tile_count} tiles...\")\n    seq_start = time.perf_counter()\n    seq_lats = run(collect_latencies_seq())\n    seq_time = time.perf_counter() - seq_start\n\n    # Benchmark concurrent approach\n    con_start = time.perf_counter()\n    con_lats = run(collect_latencies_con())\n    con_time = time.perf_counter() - con_start\n\n    def stats(name: str, lats: List[float], total: float) -&gt; None:\n        \"\"\"Print performance statistics for a benchmark run.\"\"\"\n        if not lats:\n            print(f\"{name}: no results\")\n            return\n\n        # Calculate throughput (tiles per second)\n        thr = len(lats) / total if total &gt; 0 else 0\n\n        # Print comprehensive performance metrics\n        print(\n            f\"{name}: {len(lats)} tiles in {total:.2f}s | throughput={thr:.2f} tps | \"\n            f\"p50={_percentile(lats,50)*1000:.0f}ms p95={_percentile(lats,95)*1000:.0f}ms \"\n            f\"p99={_percentile(lats,99)*1000:.0f}ms median={median(lats)*1000:.0f}ms\"\n        )\n\n    # Display results\n    print(\"\\n\" + \"=\"*60)\n    stats(\"Sequential\", seq_lats, seq_time)\n    stats(\"Concurrent\", con_lats, con_time)\n\n    # Show performance improvement\n    if seq_time &gt; 0 and con_time &gt; 0:\n        speedup = seq_time / con_time\n        print(f\"\\nConcurrent fetching is {speedup:.1f}x faster than sequential\")\n</code></pre>"},{"location":"api/day01_tile_fetcher/#src.day01_concurrency.tile_fetcher.fetch_tile","title":"<code>fetch_tile(client, z, x, y, timeout_seconds=10.0, retries=2, backoff_base=0.2, backoff_cap=2.0)</code>  <code>async</code>","text":"<p>Fetch a single tile with retry and backoff.</p> <p>This function demonstrates several important async programming concepts: 1. Async HTTP requests with httpx 2. Retry logic for transient failures 3. Exponential backoff to avoid overwhelming servers 4. Proper error handling and classification</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>AsyncClient</code> <p>HTTP client for making requests</p> required <code>z,</code> <code>(x, y)</code> <p>Tile coordinates</p> required <code>timeout_seconds</code> <code>float</code> <p>Per-request timeout</p> <code>10.0</code> <code>retries</code> <code>int</code> <p>Maximum number of retry attempts</p> <code>2</code> <code>backoff_base</code> <code>float</code> <p>Base delay for exponential backoff</p> <code>0.2</code> <code>backoff_cap</code> <code>float</code> <p>Maximum delay cap to prevent excessive waits</p> <code>2.0</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Tile image data as bytes</p> <p>Raises:</p> Type Description <code>RequestError</code> <p>For network-level errors</p> <code>HTTPStatusError</code> <p>For HTTP error responses</p> Source code in <code>src/day01_concurrency/tile_fetcher.py</code> <pre><code>async def fetch_tile(\n    client: httpx.AsyncClient,\n    z: int,\n    x: int,\n    y: int,\n    timeout_seconds: float = 10.0,\n    retries: int = 2,\n    backoff_base: float = 0.2,\n    backoff_cap: float = 2.0,\n) -&gt; bytes:\n    \"\"\"\n    Fetch a single tile with retry and backoff.\n\n    This function demonstrates several important async programming concepts:\n    1. Async HTTP requests with httpx\n    2. Retry logic for transient failures\n    3. Exponential backoff to avoid overwhelming servers\n    4. Proper error handling and classification\n\n    Args:\n        client: HTTP client for making requests\n        z, x, y: Tile coordinates\n        timeout_seconds: Per-request timeout\n        retries: Maximum number of retry attempts\n        backoff_base: Base delay for exponential backoff\n        backoff_cap: Maximum delay cap to prevent excessive waits\n\n    Returns:\n        Tile image data as bytes\n\n    Raises:\n        httpx.RequestError: For network-level errors\n        httpx.HTTPStatusError: For HTTP error responses\n    \"\"\"\n    url = TILE_URL_TEMPLATE.format(z=z, x=x, y=y)\n    attempt = 0\n\n    while True:\n        try:\n            # Make the HTTP request - this is async and won't block other coroutines\n            response = await client.get(url, timeout=timeout_seconds)\n            response.raise_for_status()  # Raises exception for 4xx/5xx status codes\n            return response.content\n\n        except (httpx.RequestError, httpx.HTTPStatusError) as e:\n            # Determine if this error is retryable\n            # Network errors (RequestError) are always retryable\n            # HTTP 5xx errors indicate server issues and are retryable\n            # HTTP 4xx errors are client errors and should not be retried\n            retryable = isinstance(e, httpx.RequestError) or (\n                isinstance(e, httpx.HTTPStatusError)\n                and 500 &lt;= e.response.status_code &lt; 600\n            )\n\n            if attempt &gt;= retries or not retryable:\n                # Either we've exhausted retries or this is a non-retryable error\n                raise\n\n            # Calculate backoff delay with jitter to prevent thundering herd\n            # Exponential backoff: delay = base * 2^attempt\n            # Jitter adds randomness to prevent synchronized retries\n            sleep_s = min(backoff_cap, backoff_base * (2 ** attempt)) + random.uniform(0, 0.1)\n            await asyncio.sleep(sleep_s)  # Non-blocking sleep\n            attempt += 1\n</code></pre>"},{"location":"api/day01_tile_fetcher/#src.day01_concurrency.tile_fetcher.fetch_tiles_concurrently","title":"<code>fetch_tiles_concurrently(tiles, max_concurrency=10, timeout_seconds=10.0, retries=2, client=None)</code>  <code>async</code>","text":"<p>Fetch tiles concurrently with bounded concurrency.</p> <p>This function demonstrates bounded concurrency using asyncio.Semaphore: - Limits the number of simultaneous requests to avoid overwhelming servers - Creates multiple tasks that run concurrently - Uses asyncio.gather to wait for all tasks to complete</p> <p>Key benefits of bounded concurrency: 1. Prevents resource exhaustion (too many open connections) 2. Respects server rate limits 3. Maintains good performance without being overly aggressive</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>Iterable[Tuple[int, int, int]]</code> <p>Iterable of (z, x, y) tile coordinates</p> required <code>max_concurrency</code> <code>int</code> <p>Maximum number of simultaneous requests</p> <code>10</code> <code>timeout_seconds</code> <code>float</code> <p>Per-request timeout</p> <code>10.0</code> <code>retries</code> <code>int</code> <p>Number of retries per request</p> <code>2</code> <code>client</code> <code>Optional[AsyncClient]</code> <p>Optional HTTP client (if None, creates a new one)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[bytes]</code> <p>List of tile data bytes in the same order as input tiles</p> Source code in <code>src/day01_concurrency/tile_fetcher.py</code> <pre><code>async def fetch_tiles_concurrently(\n    tiles: Iterable[Tuple[int, int, int]],\n    max_concurrency: int = 10,\n    timeout_seconds: float = 10.0,\n    retries: int = 2,\n    client: Optional[httpx.AsyncClient] = None,\n) -&gt; List[bytes]:\n    \"\"\"\n    Fetch tiles concurrently with bounded concurrency.\n\n    This function demonstrates bounded concurrency using asyncio.Semaphore:\n    - Limits the number of simultaneous requests to avoid overwhelming servers\n    - Creates multiple tasks that run concurrently\n    - Uses asyncio.gather to wait for all tasks to complete\n\n    Key benefits of bounded concurrency:\n    1. Prevents resource exhaustion (too many open connections)\n    2. Respects server rate limits\n    3. Maintains good performance without being overly aggressive\n\n    Args:\n        tiles: Iterable of (z, x, y) tile coordinates\n        max_concurrency: Maximum number of simultaneous requests\n        timeout_seconds: Per-request timeout\n        retries: Number of retries per request\n        client: Optional HTTP client (if None, creates a new one)\n\n    Returns:\n        List of tile data bytes in the same order as input tiles\n    \"\"\"\n    # Semaphore acts as a \"pool\" of available slots for concurrent operations\n    # Only max_concurrency coroutines can acquire the semaphore simultaneously\n    semaphore = asyncio.Semaphore(max_concurrency)\n\n    async def _bounded_fetch(z: int, x: int, y: int) -&gt; bytes:\n        \"\"\"Inner function that respects the concurrency limit.\"\"\"\n        # Acquire semaphore before making request, release when done\n        # This ensures we never exceed max_concurrency simultaneous requests\n        async with semaphore:\n            return await fetch_tile(client_obj, z, x, y, timeout_seconds=timeout_seconds, retries=retries)\n\n    if client is None:\n        # Create a new client context manager if none provided\n        # This ensures proper cleanup of resources\n        async with httpx.AsyncClient() as client_obj:\n            # Create a task for each tile - these will run concurrently\n            # asyncio.create_task schedules the coroutine to run\n            tasks = [asyncio.create_task(_bounded_fetch(z, x, y)) for (z, x, y) in tiles]\n            # Wait for all tasks to complete and return results\n            return await asyncio.gather(*tasks)\n    else:\n        # Use provided client (caller is responsible for cleanup)\n        client_obj = client\n        tasks = [asyncio.create_task(_bounded_fetch(z, x, y)) for (z, x, y) in tiles]\n        return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"api/day01_tile_fetcher/#src.day01_concurrency.tile_fetcher.fetch_tiles_sequential","title":"<code>fetch_tiles_sequential(tiles, timeout_seconds=10.0, retries=2, client=None)</code>  <code>async</code>","text":"<p>Fetch tiles sequentially (one at a time).</p> <p>This function serves as a baseline for performance comparison. While it's simpler than concurrent fetching, it's much slower because each request waits for the previous one to complete.</p> <p>Parameters:</p> Name Type Description Default <code>tiles</code> <code>Iterable[Tuple[int, int, int]]</code> <p>Iterable of (z, x, y) tile coordinates</p> required <code>timeout_seconds</code> <code>float</code> <p>Per-request timeout</p> <code>10.0</code> <code>retries</code> <code>int</code> <p>Number of retries per request</p> <code>2</code> <code>client</code> <code>Optional[AsyncClient]</code> <p>Optional HTTP client (if None, creates a new one)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[bytes]</code> <p>List of tile data bytes in the same order as input tiles</p> Source code in <code>src/day01_concurrency/tile_fetcher.py</code> <pre><code>async def fetch_tiles_sequential(\n    tiles: Iterable[Tuple[int, int, int]],\n    timeout_seconds: float = 10.0,\n    retries: int = 2,\n    client: Optional[httpx.AsyncClient] = None,\n) -&gt; List[bytes]:\n    \"\"\"\n    Fetch tiles sequentially (one at a time).\n\n    This function serves as a baseline for performance comparison.\n    While it's simpler than concurrent fetching, it's much slower\n    because each request waits for the previous one to complete.\n\n    Args:\n        tiles: Iterable of (z, x, y) tile coordinates\n        timeout_seconds: Per-request timeout\n        retries: Number of retries per request\n        client: Optional HTTP client (if None, creates a new one)\n\n    Returns:\n        List of tile data bytes in the same order as input tiles\n    \"\"\"\n    results: List[bytes] = []\n\n    if client is None:\n        # Create new client for this operation\n        async with httpx.AsyncClient() as client_obj:\n            # Process tiles one by one - this is the key difference from concurrent\n            for z, x, y in tiles:\n                results.append(\n                    await fetch_tile(client_obj, z, x, y, timeout_seconds=timeout_seconds, retries=retries)\n                )\n    else:\n        # Use provided client\n        for z, x, y in tiles:\n            results.append(\n                await fetch_tile(client, z, x, y, timeout_seconds=timeout_seconds, retries=retries)\n            )\n    return results\n</code></pre>"},{"location":"api/day01_tile_fetcher/#src.day01_concurrency.tile_fetcher.generate_sample_tiles","title":"<code>generate_sample_tiles(zoom=5, start_x=5, start_y=12, count=20)</code>","text":"<p>Generate a list of sample tile coordinates for testing.</p> <p>In web mapping, tiles are identified by three coordinates: - z: zoom level (higher = more detailed) - x: horizontal position within the zoom level - y: vertical position within the zoom level</p> <p>Parameters:</p> Name Type Description Default <code>zoom</code> <code>int</code> <p>Zoom level for the tiles</p> <code>5</code> <code>start_x</code> <code>int</code> <p>Starting x coordinate</p> <code>5</code> <code>start_y</code> <code>int</code> <p>Starting y coordinate  </p> <code>12</code> <code>count</code> <code>int</code> <p>Number of tiles to generate</p> <code>20</code> <p>Returns:</p> Type Description <code>List[Tuple[int, int, int]]</code> <p>List of (z, x, y) tuples representing tile coordinates</p> Source code in <code>src/day01_concurrency/tile_fetcher.py</code> <pre><code>def generate_sample_tiles(zoom: int = 5, start_x: int = 5, start_y: int = 12, count: int = 20) -&gt; List[Tuple[int, int, int]]:\n    \"\"\"\n    Generate a list of sample tile coordinates for testing.\n\n    In web mapping, tiles are identified by three coordinates:\n    - z: zoom level (higher = more detailed)\n    - x: horizontal position within the zoom level\n    - y: vertical position within the zoom level\n\n    Args:\n        zoom: Zoom level for the tiles\n        start_x: Starting x coordinate\n        start_y: Starting y coordinate  \n        count: Number of tiles to generate\n\n    Returns:\n        List of (z, x, y) tuples representing tile coordinates\n    \"\"\"\n    tiles: List[Tuple[int, int, int]] = []\n    x, y = start_x, start_y\n    for _ in range(count):\n        tiles.append((zoom, x, y))\n        x += 1\n        y += 1\n    return tiles\n</code></pre>"},{"location":"api/day03_api/","title":"Day 3 API","text":""},{"location":"api/day03_api/#src.day03_api.app","title":"<code>src.day03_api.app</code>","text":""},{"location":"day01_concurrency/","title":"Day 01 - Concurrency","text":""},{"location":"day01_concurrency/#learning-objectives","title":"Learning Objectives","text":"<p>This module introduces the foundational concepts of asynchronous programming and concurrent I/O operations in the context of distributed geospatial systems. By the end of this module, you will understand:</p> <ul> <li>Event-driven architecture and its role in high-throughput geospatial data pipelines</li> <li>Bounded concurrency patterns for sustainable resource utilization in distributed tile services</li> <li>Backpressure mechanisms and adaptive rate limiting for external API consumption</li> <li>Performance measurement methodologies for latency-sensitive geospatial applications</li> <li>Production-ready error handling including exponential backoff and circuit breaker patterns</li> </ul>"},{"location":"day01_concurrency/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"day01_concurrency/#concurrency-vs-parallelism-in-geospatial-context","title":"Concurrency vs Parallelism in Geospatial Context","text":"<p>In geospatial systems, we frequently deal with: - I/O-bound operations: Fetching tiles from CDNs, querying spatial databases, reading large raster files - CPU-bound operations: Geometric computations, coordinate transformations, spatial indexing - Network-bound operations: API calls to geocoding services, real-time positioning data</p> <p>Concurrency allows us to efficiently handle multiple I/O operations without blocking, making it ideal for tile fetching, API aggregation, and real-time data streams. Parallelism is better suited for computationally intensive tasks like spatial analysis across large datasets.</p>"},{"location":"day01_concurrency/#event-loop-architecture","title":"Event Loop Architecture","text":"<p>The asyncio event loop provides a single-threaded, non-blocking execution model:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Event Loop             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502Task1\u2502 \u2502Task2\u2502 \u2502Task3\u2502 \u2502... \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502       I/O Multiplexing          \u2502\n\u2502    (select/epoll/kqueue)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Benefits for Geospatial Applications: - Resource efficiency: Single thread handles thousands of tile requests - Predictable performance: No context switching overhead - Simplified reasoning: No race conditions or locks needed</p>"},{"location":"day01_concurrency/#bounded-concurrency-theory","title":"Bounded Concurrency Theory","text":"<p>Unbounded concurrency can lead to: - Resource exhaustion: Too many open file descriptors/connections - Server overwhelming: Violating rate limits on tile servers - Memory pressure: Accumulating response buffers faster than processing</p> <p>Semaphore Pattern provides controlled resource access: <pre><code>semaphore = asyncio.Semaphore(max_concurrency)\n\nasync def bounded_operation():\n    async with semaphore:  # Acquire permit\n        # Only max_concurrency operations run simultaneously\n        await actual_work()\n    # Permit automatically released\n</code></pre></p>"},{"location":"day01_concurrency/#retry-strategies-and-resilience","title":"Retry Strategies and Resilience","text":"<p>Exponential Backoff prevents cascading failures: - Linear backoff: 1s, 2s, 3s, 4s... (can cause thundering herd) - Exponential backoff: 1s, 2s, 4s, 8s... (better distribution) - Jittered exponential: Add randomness to prevent synchronized retries</p> <p>Error Classification for geospatial services: - Transient errors (5xx, timeouts): Retryable - Client errors (4xx, invalid coordinates): Non-retryable - Rate limiting (429): Retryable with longer backoff</p>"},{"location":"day01_concurrency/#system-architecture-context","title":"System Architecture Context","text":""},{"location":"day01_concurrency/#geospatial-data-pipeline-integration","title":"Geospatial Data Pipeline Integration","text":"<p>This module's patterns apply across geospatial pipeline components:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Data      \u2502    \u2502   Tile       \u2502    \u2502   Spatial   \u2502    \u2502  Real-time  \u2502\n\u2502 Ingestion   \u2502\u2500\u2500\u2500\u25b6\u2502  Processing  \u2502\u2500\u2500\u2500\u25b6\u2502   Analysis  \u2502\u2500\u2500\u2500\u25b6\u2502  Delivery   \u2502\n\u2502             \u2502    \u2502              \u2502    \u2502             \u2502    \u2502             \u2502\n\u2502\u2022 ETL jobs   \u2502    \u2502\u2022 Tile gen    \u2502    \u2502\u2022 Queries    \u2502    \u2502\u2022 WebSockets \u2502\n\u2502\u2022 Async I/O  \u2502    \u2502\u2022 Async proc  \u2502    \u2502\u2022 Async DB   \u2502    \u2502\u2022 Push notif \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Integration Points: - Data ingestion: Concurrent downloads from multiple sources - Tile processing: Parallel tile generation with bounded workers - Spatial analysis: Async database queries with connection pooling - Real-time delivery: WebSocket fan-out with backpressure handling</p>"},{"location":"day01_concurrency/#production-deployment-considerations","title":"Production Deployment Considerations","text":"<p>Infrastructure Scaling: - Horizontal scaling: Multiple async workers behind load balancer - Vertical scaling: Tune concurrency limits based on machine resources - CDN integration: Reduce load through intelligent caching strategies</p> <p>Monitoring and Observability: - Latency percentiles: p50, p95, p99 response times - Throughput metrics: Requests/second, concurrent connections - Error rates: By error type and upstream service - Resource utilization: Memory, file descriptors, connection pools</p>"},{"location":"day01_concurrency/#code-architecture-deep-dive","title":"Code Architecture Deep Dive","text":""},{"location":"day01_concurrency/#core-components-analysis","title":"Core Components Analysis","text":"<pre><code>async def fetch_tile(\n    client: httpx.AsyncClient,\n    z: int, x: int, y: int,\n    timeout_seconds: float = 10.0,\n    retries: int = 2,\n    backoff_base: float = 0.2,\n    backoff_cap: float = 2.0,\n) -&gt; bytes:\n</code></pre> <p>Design Decisions: - Dependency injection: <code>client</code> parameter allows reuse and testing - Explicit timeouts: Prevent hanging requests in production - Configurable retry logic: Adaptable to different service characteristics - Exponential backoff with cap: Prevents excessive wait times</p>"},{"location":"day01_concurrency/#concurrency-control-implementation","title":"Concurrency Control Implementation","text":"<pre><code>async def fetch_tiles_concurrently(\n    tiles: Iterable[Tuple[int, int, int]],\n    max_concurrency: int = 10,\n    # ...\n) -&gt; List[bytes]:\n    semaphore = asyncio.Semaphore(max_concurrency)\n\n    async def _bounded_fetch(z: int, x: int, y: int) -&gt; bytes:\n        async with semaphore:\n            return await fetch_tile(client_obj, z, x, y, ...)\n</code></pre> <p>Advanced Patterns: - Semaphore as resource pool: Controls concurrent operations - Context manager protocol: Ensures proper resource cleanup - Task creation vs gathering: <code>create_task()</code> schedules immediately, <code>gather()</code> waits for completion</p>"},{"location":"day01_concurrency/#performance-measurement-framework","title":"Performance Measurement Framework","text":"<pre><code>def benchmark(\n    tile_count: int = 20,\n    max_concurrency: int = 10,\n    timeout_seconds: float = 10.0,\n    retries: int = 2,\n    global_timeout: Optional[float] = None,\n) -&gt; None:\n</code></pre> <p>Metrics Collection: - Individual request latencies: Measure per-request performance - End-to-end timing: Total execution time including coordination overhead - Percentile analysis: Understanding tail latency behavior - Throughput calculation: Effective requests per second</p>"},{"location":"day01_concurrency/#advanced-implementation-patterns","title":"Advanced Implementation Patterns","text":""},{"location":"day01_concurrency/#connection-pool-management","title":"Connection Pool Management","text":"<pre><code>class TileClientPool:\n    def __init__(self, max_connections: int = 100):\n        self.client = httpx.AsyncClient(\n            limits=httpx.Limits(\n                max_connections=max_connections,\n                max_keepalive_connections=20\n            ),\n            timeout=httpx.Timeout(10.0)\n        )\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self.client.aclose()\n</code></pre>"},{"location":"day01_concurrency/#adaptive-rate-limiting","title":"Adaptive Rate Limiting","text":"<pre><code>class AdaptiveRateLimiter:\n    def __init__(self, initial_rate: float = 10.0):\n        self.current_rate = initial_rate\n        self.success_count = 0\n        self.error_count = 0\n\n    async def acquire(self):\n        await asyncio.sleep(1.0 / self.current_rate)\n\n    def on_success(self):\n        self.success_count += 1\n        if self.success_count &gt; 10:\n            self.current_rate *= 1.1  # Increase rate\n            self.success_count = 0\n\n    def on_error(self, error_type: str):\n        if error_type == \"rate_limit\":\n            self.current_rate *= 0.5  # Decrease rate\n</code></pre>"},{"location":"day01_concurrency/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>class CircuitBreaker:\n    def __init__(self, failure_threshold: int = 5, timeout: float = 60.0):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n\n    async def call(self, func, *args, **kwargs):\n        if self.state == \"OPEN\":\n            if time.time() - self.last_failure_time &gt; self.timeout:\n                self.state = \"HALF_OPEN\"\n            else:\n                raise Exception(\"Circuit breaker is OPEN\")\n\n        try:\n            result = await func(*args, **kwargs)\n            self.on_success()\n            return result\n        except Exception as e:\n            self.on_failure()\n            raise\n</code></pre>"},{"location":"day01_concurrency/#performance-optimization-strategies","title":"Performance Optimization Strategies","text":""},{"location":"day01_concurrency/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code>async def stream_tiles_to_disk(tiles: List[Tuple[int, int, int]]):\n    \"\"\"Process tiles without loading all into memory simultaneously.\"\"\"\n    async with httpx.AsyncClient() as client:\n        async for tile_data in bounded_tile_stream(client, tiles):\n            # Process one tile at a time\n            await save_tile_to_disk(tile_data)\n</code></pre>"},{"location":"day01_concurrency/#batch-processing-optimization","title":"Batch Processing Optimization","text":"<pre><code>async def fetch_tile_batch(\n    client: httpx.AsyncClient,\n    tile_batch: List[Tuple[int, int, int]],\n    batch_size: int = 50\n) -&gt; AsyncGenerator[bytes, None]:\n    \"\"\"Process tiles in optimally-sized batches.\"\"\"\n    for i in range(0, len(tile_batch), batch_size):\n        batch = tile_batch[i:i + batch_size]\n        results = await asyncio.gather(\n            *[fetch_tile(client, z, x, y) for z, x, y in batch],\n            return_exceptions=True\n        )\n        for result in results:\n            if isinstance(result, Exception):\n                # Handle individual failures\n                continue\n            yield result\n</code></pre>"},{"location":"day01_concurrency/#production-deployment-patterns","title":"Production Deployment Patterns","text":""},{"location":"day01_concurrency/#configuration-management","title":"Configuration Management","text":"<pre><code>@dataclass\nclass TileFetcherConfig:\n    max_concurrency: int = field(default_factory=lambda: int(os.getenv(\"MAX_CONCURRENCY\", \"10\")))\n    timeout_seconds: float = field(default_factory=lambda: float(os.getenv(\"TIMEOUT_SECONDS\", \"10.0\")))\n    retries: int = field(default_factory=lambda: int(os.getenv(\"RETRIES\", \"2\")))\n    base_url: str = field(default_factory=lambda: os.getenv(\"TILE_BASE_URL\", \"https://tile.openstreetmap.org\"))\n\n    def __post_init__(self):\n        if self.max_concurrency &lt; 1:\n            raise ValueError(\"max_concurrency must be &gt;= 1\")\n        if self.timeout_seconds &lt;= 0:\n            raise ValueError(\"timeout_seconds must be &gt; 0\")\n</code></pre>"},{"location":"day01_concurrency/#health-monitoring","title":"Health Monitoring","text":"<pre><code>class TileServiceHealth:\n    def __init__(self):\n        self.stats = {\n            \"requests_total\": 0,\n            \"requests_successful\": 0,\n            \"requests_failed\": 0,\n            \"avg_latency_ms\": 0.0,\n            \"current_concurrency\": 0\n        }\n\n    async def health_check(self) -&gt; Dict[str, Any]:\n        \"\"\"Endpoint for load balancer health checks.\"\"\"\n        success_rate = self.stats[\"requests_successful\"] / max(self.stats[\"requests_total\"], 1)\n\n        return {\n            \"status\": \"healthy\" if success_rate &gt; 0.95 else \"degraded\",\n            \"uptime_seconds\": time.time() - self.start_time,\n            \"stats\": self.stats\n        }\n</code></pre>"},{"location":"day01_concurrency/#running-the-module","title":"Running the Module","text":""},{"location":"day01_concurrency/#basic-usage","title":"Basic Usage","text":"<pre><code># Activate environment\nsource .venv/bin/activate\n\n# Run with default settings\npython -m src.day01_concurrency.tile_fetcher\n\n# Tune for your environment\npython -m src.day01_concurrency.tile_fetcher \\\n  --tile-count 100 \\\n  --max-concurrency 20 \\\n  --timeout 5 \\\n  --retries 3\n</code></pre>"},{"location":"day01_concurrency/#production-configuration","title":"Production Configuration","text":"<pre><code># Environment-based configuration\nexport TILE_BASE_URL=\"https://your-tile-server.com\"\nexport MAX_CONCURRENCY=50\nexport TIMEOUT_SECONDS=15\nexport RETRIES=3\n\npython -m src.day01_concurrency.tile_fetcher\n</code></pre>"},{"location":"day01_concurrency/#local-development-with-mock-server","title":"Local Development with Mock Server","text":"<pre><code># Terminal 1: Start mock tile server\nmake run-mock-tiles\n\n# Terminal 2: Run benchmark against local server\nTILE_BASE_URL=http://127.0.0.1:8001 \\\n  python -m src.day01_concurrency.tile_fetcher \\\n  --tile-count 100 \\\n  --max-concurrency 50 \\\n  --timeout 2\n</code></pre>"},{"location":"day01_concurrency/#performance-analysis-framework","title":"Performance Analysis Framework","text":""},{"location":"day01_concurrency/#benchmarking-methodology","title":"Benchmarking Methodology","text":"<ol> <li>Baseline Measurement: Sequential execution as performance floor</li> <li>Concurrency Scaling: Test different concurrency levels (1, 5, 10, 20, 50)</li> <li>Latency Distribution: Analyze p50, p95, p99 percentiles</li> <li>Throughput Analysis: Requests/second vs resource utilization</li> <li>Error Rate Impact: Performance under various failure rates</li> </ol>"},{"location":"day01_concurrency/#expected-results-analysis","title":"Expected Results Analysis","text":"<p>Typical Performance Characteristics: - Sequential: ~1-2 tiles/second (limited by round-trip time) - Concurrent (10): ~8-15 tiles/second (network bandwidth limited) - Concurrent (50): ~20-30 tiles/second (may hit rate limits)</p> <p>Performance Scaling Laws: - Amdahl's Law: Speedup limited by non-parallelizable components - Little's Law: Throughput = Concurrency / Average Latency - Universal Scalability Law: Accounts for contention and coherency costs</p>"},{"location":"day01_concurrency/#professional-development-exercises","title":"Professional Development Exercises","text":""},{"location":"day01_concurrency/#exercise-1-production-ready-error-handling","title":"Exercise 1: Production-Ready Error Handling","text":"<p>Implement comprehensive error handling with: - Structured logging with correlation IDs - Metrics collection for different error types - Graceful degradation strategies - Dead letter queue for failed tiles</p>"},{"location":"day01_concurrency/#exercise-2-advanced-backpressure-control","title":"Exercise 2: Advanced Backpressure Control","text":"<p>Design a dynamic concurrency controller that: - Monitors downstream service health - Adjusts concurrency based on error rates - Implements exponential backoff with jitter - Provides circuit breaker functionality</p>"},{"location":"day01_concurrency/#exercise-3-performance-optimization","title":"Exercise 3: Performance Optimization","text":"<p>Profile and optimize the tile fetcher: - Measure memory allocation patterns - Implement streaming for large tile sets - Add connection pooling with keep-alive - Benchmark different serialization formats</p>"},{"location":"day01_concurrency/#exercise-4-integration-testing","title":"Exercise 4: Integration Testing","text":"<p>Create comprehensive integration tests: - Mock tile server with configurable latency/errors - Property-based testing with Hypothesis - Load testing with realistic traffic patterns - Chaos engineering scenarios</p>"},{"location":"day01_concurrency/#industry-context-and-best-practices","title":"Industry Context and Best Practices","text":""},{"location":"day01_concurrency/#real-world-applications","title":"Real-World Applications","text":"<p>Mapping Services: - Google Maps: Massive tile serving infrastructure with global CDN - OpenStreetMap: Community-driven tile generation and distribution - Mapbox: Dynamic styling and real-time data integration</p> <p>Autonomous Vehicles: - High-definition map tile streaming for real-time navigation - Low-latency requirements for safety-critical applications - Redundant data sources with automatic failover</p> <p>Location-Based Services: - Real-time asset tracking with geofencing - Location analytics with privacy-preserving aggregation - Mobile app optimization for battery and bandwidth</p>"},{"location":"day01_concurrency/#ethical-considerations","title":"Ethical Considerations","text":"<p>Resource Responsibility: - Respect rate limits and terms of service - Implement proper caching to reduce server load - Consider carbon footprint of unnecessary requests</p> <p>Data Privacy: - Avoid logging sensitive location information - Implement proper data retention policies - Consider GDPR/CCPA compliance for user data</p>"},{"location":"day01_concurrency/#industry-standards","title":"Industry Standards","text":"<p>OGC Standards: - WMTS: Web Map Tile Service specification - TMS: Tile Map Service for standardized tile access - WMS: Web Map Service for dynamic map generation</p> <p>Performance Benchmarks: - Sub-100ms: Interactive mapping applications - Sub-50ms: Real-time navigation systems - Sub-10ms: Autonomous vehicle safety systems</p>"},{"location":"day01_concurrency/#further-reading-and-resources","title":"Further Reading and Resources","text":""},{"location":"day01_concurrency/#technical-references","title":"Technical References","text":"<ul> <li>asyncio Official Documentation</li> <li>httpx Advanced Usage</li> <li>Concurrency in Python by Matthew Fowler</li> </ul>"},{"location":"day01_concurrency/#performance-engineering","title":"Performance Engineering","text":"<ul> <li>High Performance Python by Micha Gorelick</li> <li>Site Reliability Engineering by Google</li> </ul>"},{"location":"day01_concurrency/#geospatial-engineering","title":"Geospatial Engineering","text":"<ul> <li>PostGIS in Action by Regina Obe</li> <li>Tile Map Service Specification</li> </ul>"},{"location":"day01_concurrency/#production-operations","title":"Production Operations","text":"<ul> <li>Designing Data-Intensive Applications by Martin Kleppmann</li> <li>Building Microservices by Sam Newman</li> </ul> <p>This module provides the foundation for understanding how asynchronous programming patterns enable scalable, resilient geospatial systems. The patterns learned here will be applied throughout the remaining modules as we build increasingly sophisticated geospatial services.</p>"},{"location":"day02_oop/","title":"Day 02 - Oop","text":""},{"location":"day02_oop/#learning-objectives","title":"Learning Objectives","text":"<p>This module introduces enterprise-grade software architecture patterns essential for building scalable, maintainable geospatial data processing systems. By the end of this module, you will understand:</p> <ul> <li>Interface Segregation and Dependency Inversion principles in distributed geospatial systems</li> <li>Provider Pattern implementation for multi-format data source abstraction</li> <li>Factory and Strategy patterns for runtime behavior selection and configuration</li> <li>Plugin architecture design enabling extensible data processing pipelines</li> <li>Clean Architecture principles for domain-driven geospatial service design</li> <li>Enterprise integration patterns for heterogeneous geospatial data ecosystems</li> </ul>"},{"location":"day02_oop/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"day02_oop/#solid-principles-in-geospatial-context","title":"SOLID Principles in Geospatial Context","text":"<p>Single Responsibility Principle (SRP): Each provider handles exactly one data format concern, making the system easier to test, debug, and maintain.</p> <p>Open/Closed Principle (OCP): The provider architecture is open for extension (new data formats) but closed for modification (existing providers remain unchanged).</p> <p>Liskov Substitution Principle (LSP): Any MapDataProvider implementation can be substituted for another without breaking client code.</p> <p>Interface Segregation Principle (ISP): Small, focused interfaces prevent clients from depending on methods they don't use.</p> <p>Dependency Inversion Principle (DIP): High-level geospatial processing logic depends on abstractions, not concrete implementations.</p>"},{"location":"day02_oop/#enterprise-architecture-context","title":"Enterprise Architecture Context","text":"<p>In large-scale geospatial systems, we encounter: - Multiple data formats: GeoJSON, Shapefile, MVT, PostGIS, Oracle Spatial - Varying data sources: Files, databases, APIs, streaming services - Different access patterns: Batch processing, real-time streaming, random access - Evolving requirements: New formats, changing schemas, performance optimizations</p> <p>The Provider Pattern enables: - Runtime format selection: Choose optimal format based on use case - Zero-downtime migrations: Gradually migrate between data sources - A/B testing: Compare performance between different implementations - Vendor independence: Avoid lock-in to specific data formats or services</p>"},{"location":"day02_oop/#goals","title":"Goals","text":"<ul> <li>Design enterprise-grade interfaces with minimal coupling and maximal cohesion</li> <li>Implement pluggable architecture supporting runtime format selection</li> <li>Apply dependency injection patterns for testable, configurable systems</li> <li>Understand clean architecture boundaries between domain logic and infrastructure</li> </ul>"},{"location":"day02_oop/#core-concepts","title":"Core Concepts","text":""},{"location":"day02_oop/#1-interface-via-abc","title":"1) Interface via ABC","text":"<p>ABCs define a contract implemented by concrete providers.</p> <pre><code>from abc import ABC, abstractmethod\n\nclass MapDataProvider(ABC):\n    @abstractmethod\n    def get_feature_by_id(self, feature_id: str):\n        \"\"\"Must be implemented by subclasses.\"\"\"\n        pass\n</code></pre> <p>Benefits: - Enforces consistency; enables polymorphism and easy mocking. - Documents expectations; keeps call sites stable as implementations evolve.</p>"},{"location":"day02_oop/#2-factory-and-strategy","title":"2) Factory and Strategy","text":"<p>Factory creates the right provider for a given content type; Strategy lets behavior vary (e.g., coordinate transforms) without touching callers.</p> <pre><code>class ProviderFactory:\n    @staticmethod\n    def create_provider(content_type: str) -&gt; MapDataProvider:\n        if content_type == \"geojson\":\n            return GeoJSONProvider()\n        elif content_type == \"mvt\":\n            return MVTProvider()\n        else:\n            raise ValueError(f\"Unknown content type: {content_type}\")\n</code></pre> <p>Strategy example:</p> <pre><code>class CoordinateTransformer:\n    def __init__(self, strategy: TransformationStrategy):\n        self.strategy = strategy\n\n    def transform(self, coordinates):\n        return self.strategy.transform(coordinates)\n</code></pre>"},{"location":"day02_oop/#3-package-layout","title":"3) Package Layout","text":"<p>Organize for clarity and import hygiene:</p> <pre><code>src/day02_oop/\n\u251c\u2500\u2500 __init__.py          # Package initialization\n\u251c\u2500\u2500 providers/           # Subpackage for providers\n\u2502   \u251c\u2500\u2500 __init__.py     # Exports main classes\n\u2502   \u251c\u2500\u2500 base.py         # Abstract base class\n\u2502   \u251c\u2500\u2500 geojson_provider.py\n\u2502   \u2514\u2500\u2500 mvt_provider.py\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"day02_oop/#code-walkthrough","title":"Code Walkthrough","text":""},{"location":"day02_oop/#1-abstract-base-class","title":"1) Abstract Base Class","text":"<pre><code>from abc import ABC, abstractmethod\nfrom typing import Any\n\nclass MapDataProvider(ABC):\n    @abstractmethod\n    def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        \"\"\"Retrieve a feature by its unique identifier.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def stream_features(self) -&gt; Any:\n        \"\"\"Stream all features from the data source.\"\"\"\n        raise NotImplementedError\n</code></pre> <p>Key Points: - <code>@abstractmethod</code> decorator marks methods that must be implemented - <code>raise NotImplementedError</code> provides a clear error if not implemented - Type hints help with IDE support and documentation</p>"},{"location":"day02_oop/#2-concrete-implementations","title":"2. Concrete Implementations","text":""},{"location":"day02_oop/#geojson-provider","title":"GeoJSON Provider","text":"<pre><code>import json\nfrom pathlib import Path\nfrom typing import Any, Iterable\nfrom .base import MapDataProvider\n\nclass GeoJSONProvider(MapDataProvider):\n    def __init__(self, path: str | Path) -&gt; None:\n        self._path = Path(path)\n        self._data = json.loads(self._path.read_text())\n        # Build an index for fast lookups\n        self._index = {\n            f[\"properties\"].get(\"id\"): f \n            for f in self._data.get(\"features\", [])\n        }\n\n    def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        return self._index.get(feature_id)\n\n    def stream_features(self) -&gt; Iterable[dict]:\n        yield from self._data.get(\"features\", [])\n</code></pre> <p>Features: - Indexing: Builds a lookup dictionary for O(1) feature retrieval - Path handling: Uses <code>pathlib.Path</code> for cross-platform compatibility - Memory efficiency: Yields features one at a time</p>"},{"location":"day02_oop/#mvt-provider","title":"MVT Provider","text":"<pre><code>from pathlib import Path\nfrom typing import Any, Iterable\nfrom .base import MapDataProvider\n\nclass MVTProvider(MapDataProvider):\n    def __init__(self, directory: str | Path) -&gt; None:\n        self._directory = Path(directory)\n\n    def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        # MVT files don't have feature IDs - would need external index\n        return None\n\n    def stream_features(self) -&gt; Iterable[bytes]:\n        # Yield raw MVT bytes from directory\n        for p in self._directory.rglob(\"*.mvt\"):\n            yield p.read_bytes()\n</code></pre> <p>Features: - Directory scanning: Uses <code>rglob</code> to find all <code>.mvt</code> files - Binary handling: Returns raw bytes for MVT data - Placeholder implementation: Shows where external indexing would be needed</p>"},{"location":"day02_oop/#3-package-exports","title":"3. Package Exports","text":"<pre><code># providers/__init__.py\nfrom .base import MapDataProvider\nfrom .geojson_provider import GeoJSONProvider\nfrom .mvt_provider import MVTProvider\n\n# This allows users to import directly from the package\n# from src.day02_oop.providers import MapDataProvider\n</code></pre>"},{"location":"day02_oop/#usage","title":"Usage","text":"<p>Create and use providers via the common interface. Note: <code>MVTProvider.stream_features()</code> yields raw bytes for demo purposes; <code>GeoJSONProvider</code> and <code>CSVProvider</code> yield dict features. <pre><code>from src.day02_oop.providers import GeoJSONProvider, MVTProvider, CSVProvider\n\n# Create providers\ngeojson_provider = GeoJSONProvider(\"data/roads.geojson\")\nmvt_provider = MVTProvider(\"data/tiles/\")\ncsv_provider = CSVProvider(\"src/day02_oop/providers/data/roads_sample.csv\")\n\n# Use polymorphically\nfor provider in [geojson_provider, mvt_provider, csv_provider]:\n    features = list(provider.stream_features())\n    print(f\"Provider {type(provider).__name__}: {len(features)} features\")\n</code></pre></p>"},{"location":"day02_oop/#testing-the-interface","title":"Testing the Interface","text":"<pre><code>def test_provider_interface(provider: MapDataProvider):\n    \"\"\"Test that any provider implements the required interface.\"\"\"\n    # This will work with any MapDataProvider subclass\n    features = list(provider.stream_features())\n    assert len(features) &gt;= 0\n\n    # Test feature lookup if supported\n    if provider.get_feature_by_id(\"test_id\") is not None:\n        print(\"Provider supports feature lookup\")\n</code></pre>"},{"location":"day02_oop/#exercises","title":"Exercises","text":""},{"location":"day02_oop/#1-implement-a-new-provider","title":"1) Implement a New Provider","text":"<p>Create a <code>CSVProvider</code> that reads from CSV files:</p> <pre><code>import csv\nfrom pathlib import Path\nfrom typing import Any, Iterable\nfrom .base import MapDataProvider\n\nclass CSVProvider(MapDataProvider):\n    def __init__(self, path: str | Path) -&gt; None:\n        self._path = Path(path)\n        # TODO: Implement CSV reading logic\n\n    def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        # TODO: Implement CSV lookup\n        pass\n\n    def stream_features(self) -&gt; Iterable[dict]:\n        # TODO: Implement CSV streaming\n        pass\n</code></pre>"},{"location":"day02_oop/#2-add-a-factory-method","title":"2) Add a Factory Method","text":"<p><pre><code>class ProviderFactory:\n    _providers = {\n        \"geojson\": GeoJSONProvider,\n        \"mvt\": MVTProvider,\n        \"csv\": CSVProvider,  # Add your new provider\n    }\n\n    @classmethod\n    def create_provider(cls, content_type: str, **kwargs) -&gt; MapDataProvider:\n        if content_type not in cls._providers:\n            raise ValueError(f\"Unknown provider type: {content_type}\")\n\n        provider_class = cls._providers[content_type]\n        return provider_class(**kwargs)\n\n    @classmethod\n    def register_provider(cls, name: str, provider_class: type):\n        \"\"\"Allow runtime registration of new providers.\"\"\"\n        cls._providers[name] = provider_class\n\nOr use the built-in minimal factory:\n```python\nfrom src.day02_oop.providers import ProviderFactory\n\ncsv = ProviderFactory.create(\"csv\", path=\"src/day02_oop/providers/data/roads_sample.csv\")\ngeojson = ProviderFactory.create(\"geojson\", path=\"data/roads.geojson\")\n</code></pre> <pre><code>### 3) Add Caching for Hot Paths\n\n```python\nfrom functools import lru_cache\n\nclass CachedGeoJSONProvider(GeoJSONProvider):\n    @lru_cache(maxsize=1000)\n    def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        return super().get_feature_by_id(feature_id)\n</code></pre></p>"},{"location":"day02_oop/#advanced","title":"Advanced","text":""},{"location":"day02_oop/#1-protocols-structural-typing","title":"1) Protocols (Structural Typing)","text":"<p>Instead of ABCs, you can use protocols for structural typing:</p> <pre><code>from typing import Protocol\n\nclass MapDataProvider(Protocol):\n    def get_feature_by_id(self, feature_id: str) -&gt; Any: ...\n    def stream_features(self) -&gt; Iterable[Any]: ...\n\n# Any class with these methods automatically implements the protocol\n</code></pre>"},{"location":"day02_oop/#2-generics-for-type-safety","title":"2) Generics for Type Safety","text":"<p>Make providers more type-safe with generics:</p> <pre><code>from typing import Generic, TypeVar\n\nT = TypeVar('T')\n\nclass MapDataProvider(ABC, Generic[T]):\n    @abstractmethod\n    def get_feature_by_id(self, feature_id: str) -&gt; T | None:\n        pass\n\n    @abstractmethod\n    def stream_features(self) -&gt; Iterable[T]:\n        pass\n\nclass GeoJSONProvider(MapDataProvider[dict]):\n    # Now the return types are properly typed\n    pass\n</code></pre>"},{"location":"day02_oop/#3-context-managers","title":"3) Context Managers","text":"<p>Add resource management to providers:</p> <pre><code>class ManagedGeoJSONProvider(GeoJSONProvider):\n    def __enter__(self):\n        # Could open file handles, database connections, etc.\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Clean up resources\n        pass\n</code></pre>"},{"location":"day02_oop/#best-practices","title":"Best Practices","text":""},{"location":"day02_oop/#1-interface-design","title":"1) Interface Design","text":"<ul> <li>Keep interfaces small and focused</li> <li>Use descriptive method names</li> <li>Document expected behavior clearly</li> <li>Consider backward compatibility</li> </ul>"},{"location":"day02_oop/#2-error-handling","title":"2) Error Handling","text":"<pre><code>class GeoJSONProvider(MapDataProvider):\n    def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        if not hasattr(self, '_index'):\n            raise RuntimeError(\"Provider not properly initialized\")\n\n        if not feature_id:\n            raise ValueError(\"Feature ID cannot be empty\")\n\n        return self._index.get(feature_id)\n</code></pre>"},{"location":"day02_oop/#3-testing","title":"3) Testing","text":"<pre><code>import pytest\nfrom unittest.mock import Mock\n\ndef test_provider_contract():\n    \"\"\"Test that providers follow the contract.\"\"\"\n    mock_provider = Mock(spec=MapDataProvider)\n\n    # These should work without errors\n    mock_provider.get_feature_by_id(\"test\")\n    list(mock_provider.stream_features())\n</code></pre>"},{"location":"day02_oop/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"day02_oop/#1-over-abstraction","title":"1) Over-Abstraction","text":"<pre><code># \u274c Too many abstract methods\nclass MapDataProvider(ABC):\n    @abstractmethod\n    def get_feature_by_id(self, feature_id: str): pass\n    @abstractmethod\n    def stream_features(self): pass\n    @abstractmethod\n    def get_feature_count(self): pass  # Maybe not needed?\n    @abstractmethod\n    def get_feature_bounds(self): pass  # Maybe not needed?\n\n# \u2705 Focus on core functionality\nclass MapDataProvider(ABC):\n    @abstractmethod\n    def get_feature_by_id(self, feature_id: str): pass\n    @abstractmethod\n    def stream_features(self): pass\n</code></pre>"},{"location":"day02_oop/#2-ignoring-resource-management","title":"2) Ignoring Resource Management","text":"<pre><code># \u274c File handles not properly closed\ndef stream_features(self):\n    with open(self._path) as f:\n        return json.load(f)[\"features\"]\n\n# \u2705 Proper resource management\ndef stream_features(self):\n    with open(self._path) as f:\n        data = json.load(f)\n        yield from data[\"features\"]\n</code></pre>"},{"location":"day02_oop/#next-steps","title":"Next Steps","text":"<ul> <li>Decide ABC vs Protocol for your team\u2019s typing philosophy.</li> <li>Implement and test <code>CSVProvider</code> or a PostGIS-backed provider.</li> <li>Add a simple factory and registration mechanism.</li> <li>Validate/normalize feature schemas at the boundary.</li> </ul>"},{"location":"day02_oop/#advanced-enterprise-patterns","title":"Advanced Enterprise Patterns","text":""},{"location":"day02_oop/#hexagonal-architecture-implementation","title":"Hexagonal Architecture Implementation","text":"<pre><code># Domain Layer - Core business logic\nclass SpatialQuery:\n    def __init__(self, bbox: BoundingBox, filters: Dict[str, Any]):\n        self.bbox = bbox\n        self.filters = filters\n\n# Application Layer - Use cases\nclass GeospatialService:\n    def __init__(self, data_provider: MapDataProvider, \n                 spatial_index: SpatialIndex):\n        self._provider = data_provider\n        self._index = spatial_index\n\n    async def query_features(self, query: SpatialQuery) -&gt; FeatureCollection:\n        # Pure business logic, no infrastructure concerns\n        candidate_ids = self._index.query_bbox(query.bbox)\n        features = []\n        for feature_id in candidate_ids:\n            feature = await self._provider.get_feature_by_id(feature_id)\n            if self._matches_filters(feature, query.filters):\n                features.append(feature)\n        return FeatureCollection(features)\n\n# Infrastructure Layer - External concerns\nclass PostGISProvider(MapDataProvider):\n    def __init__(self, connection_pool: asyncpg.Pool):\n        self._pool = connection_pool\n</code></pre>"},{"location":"day02_oop/#plugin-architecture-with-registry","title":"Plugin Architecture with Registry","text":"<pre><code>class ProviderRegistry:\n    \"\"\"Registry for managing provider plugins at runtime.\"\"\"\n\n    def __init__(self):\n        self._providers: Dict[str, Type[MapDataProvider]] = {}\n        self._configurations: Dict[str, Dict[str, Any]] = {}\n\n    def register(self, name: str, provider_class: Type[MapDataProvider], \n                 config: Dict[str, Any] = None):\n        \"\"\"Register a provider with optional configuration.\"\"\"\n        self._providers[name] = provider_class\n        self._configurations[name] = config or {}\n\n    def create(self, name: str, **override_config) -&gt; MapDataProvider:\n        \"\"\"Create provider instance with configuration.\"\"\"\n        if name not in self._providers:\n            raise ValueError(f\"Unknown provider: {name}\")\n\n        provider_class = self._providers[name]\n        config = {**self._configurations[name], **override_config}\n        return provider_class(**config)\n\n    def list_providers(self) -&gt; List[str]:\n        \"\"\"List all registered provider names.\"\"\"\n        return list(self._providers.keys())\n\n# Usage\nregistry = ProviderRegistry()\nregistry.register(\"geojson\", GeoJSONProvider, {\"buffer_size\": 8192})\nregistry.register(\"postgis\", PostGISProvider, {\"pool_size\": 20})\n\n# Runtime provider selection\nprovider = registry.create(\"geojson\", path=\"/data/roads.geojson\")\n</code></pre>"},{"location":"day02_oop/#configuration-driven-architecture","title":"Configuration-Driven Architecture","text":"<pre><code>@dataclass\nclass ProviderConfig:\n    \"\"\"Type-safe configuration for providers.\"\"\"\n    provider_type: str\n    connection_string: Optional[str] = None\n    file_path: Optional[str] = None\n    cache_enabled: bool = True\n    cache_ttl_seconds: int = 300\n    timeout_seconds: float = 30.0\n    retry_attempts: int = 3\n\n    def __post_init__(self):\n        if self.provider_type in [\"postgis\", \"oracle\"] and not self.connection_string:\n            raise ValueError(f\"{self.provider_type} requires connection_string\")\n        if self.provider_type in [\"geojson\", \"shapefile\"] and not self.file_path:\n            raise ValueError(f\"{self.provider_type} requires file_path\")\n\nclass ConfigurableProviderFactory:\n    \"\"\"Factory that creates providers from configuration.\"\"\"\n\n    @staticmethod\n    def from_config(config: ProviderConfig) -&gt; MapDataProvider:\n        if config.provider_type == \"geojson\":\n            provider = GeoJSONProvider(config.file_path)\n        elif config.provider_type == \"postgis\":\n            provider = PostGISProvider(config.connection_string)\n        else:\n            raise ValueError(f\"Unsupported provider: {config.provider_type}\")\n\n        if config.cache_enabled:\n            provider = CachedProvider(provider, ttl=config.cache_ttl_seconds)\n\n        if config.retry_attempts &gt; 0:\n            provider = RetryingProvider(provider, max_attempts=config.retry_attempts)\n\n        return provider\n</code></pre>"},{"location":"day02_oop/#decorator-pattern-for-cross-cutting-concerns","title":"Decorator Pattern for Cross-Cutting Concerns","text":"<pre><code>class CachedProvider(MapDataProvider):\n    \"\"\"Decorator that adds caching to any provider.\"\"\"\n\n    def __init__(self, wrapped_provider: MapDataProvider, \n                 cache: Optional[Dict] = None, ttl: int = 300):\n        self._provider = wrapped_provider\n        self._cache = cache or {}\n        self._ttl = ttl\n        self._timestamps = {}\n\n    def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        now = time.time()\n\n        # Check cache validity\n        if (feature_id in self._cache and \n            feature_id in self._timestamps and \n            now - self._timestamps[feature_id] &lt; self._ttl):\n            return self._cache[feature_id]\n\n        # Fetch from wrapped provider\n        feature = self._provider.get_feature_by_id(feature_id)\n\n        # Update cache\n        self._cache[feature_id] = feature\n        self._timestamps[feature_id] = now\n\n        return feature\n\nclass MetricsProvider(MapDataProvider):\n    \"\"\"Decorator that adds metrics collection.\"\"\"\n\n    def __init__(self, wrapped_provider: MapDataProvider, \n                 metrics_collector: 'MetricsCollector'):\n        self._provider = wrapped_provider\n        self._metrics = metrics_collector\n\n    async def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        start_time = time.time()\n        try:\n            result = await self._provider.get_feature_by_id(feature_id)\n            self._metrics.record_success(\"get_feature_by_id\", time.time() - start_time)\n            return result\n        except Exception as e:\n            self._metrics.record_error(\"get_feature_by_id\", str(e))\n            raise\n\n# Usage: Composable decorators\nbase_provider = GeoJSONProvider(\"data.geojson\")\ncached_provider = CachedProvider(base_provider, ttl=600)\nmonitored_provider = MetricsProvider(cached_provider, metrics_collector)\n</code></pre>"},{"location":"day02_oop/#command-pattern-for-data-operations","title":"Command Pattern for Data Operations","text":"<pre><code>class DataOperation(ABC):\n    \"\"\"Abstract command for data operations.\"\"\"\n\n    @abstractmethod\n    async def execute(self) -&gt; Any:\n        pass\n\n    @abstractmethod\n    async def undo(self) -&gt; None:\n        pass\n\nclass BulkInsertOperation(DataOperation):\n    def __init__(self, provider: MapDataProvider, features: List[Dict]):\n        self._provider = provider\n        self._features = features\n        self._inserted_ids = []\n\n    async def execute(self) -&gt; List[str]:\n        for feature in self._features:\n            feature_id = await self._provider.insert_feature(feature)\n            self._inserted_ids.append(feature_id)\n        return self._inserted_ids\n\n    async def undo(self) -&gt; None:\n        for feature_id in reversed(self._inserted_ids):\n            await self._provider.delete_feature(feature_id)\n\nclass DataOperationExecutor:\n    \"\"\"Executor that manages operations with undo capability.\"\"\"\n\n    def __init__(self):\n        self._history: List[DataOperation] = []\n\n    async def execute(self, operation: DataOperation) -&gt; Any:\n        result = await operation.execute()\n        self._history.append(operation)\n        return result\n\n    async def undo_last(self) -&gt; None:\n        if self._history:\n            operation = self._history.pop()\n            await operation.undo()\n</code></pre>"},{"location":"day02_oop/#event-driven-architecture","title":"Event-Driven Architecture","text":"<pre><code>class DataProviderEvent:\n    \"\"\"Base class for provider events.\"\"\"\n    def __init__(self, provider_name: str, timestamp: datetime):\n        self.provider_name = provider_name\n        self.timestamp = timestamp\n\nclass FeatureUpdatedEvent(DataProviderEvent):\n    def __init__(self, provider_name: str, feature_id: str, \n                 old_feature: Dict, new_feature: Dict):\n        super().__init__(provider_name, datetime.now())\n        self.feature_id = feature_id\n        self.old_feature = old_feature\n        self.new_feature = new_feature\n\nclass EventDrivenProvider(MapDataProvider):\n    \"\"\"Provider that emits events for all operations.\"\"\"\n\n    def __init__(self, wrapped_provider: MapDataProvider, \n                 event_bus: 'EventBus'):\n        self._provider = wrapped_provider\n        self._event_bus = event_bus\n\n    async def update_feature(self, feature_id: str, updates: Dict) -&gt; Dict:\n        old_feature = await self._provider.get_feature_by_id(feature_id)\n        new_feature = await self._provider.update_feature(feature_id, updates)\n\n        event = FeatureUpdatedEvent(\n            provider_name=self.__class__.__name__,\n            feature_id=feature_id,\n            old_feature=old_feature,\n            new_feature=new_feature\n        )\n        await self._event_bus.publish(event)\n\n        return new_feature\n</code></pre>"},{"location":"day02_oop/#industry-integration-patterns","title":"Industry Integration Patterns","text":""},{"location":"day02_oop/#multi-tenant-architecture","title":"Multi-Tenant Architecture","text":"<pre><code>class TenantAwareProvider(MapDataProvider):\n    \"\"\"Provider that implements multi-tenancy.\"\"\"\n\n    def __init__(self, base_provider: MapDataProvider, \n                 tenant_resolver: 'TenantResolver'):\n        self._base_provider = base_provider\n        self._tenant_resolver = tenant_resolver\n\n    async def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        tenant_id = self._tenant_resolver.get_current_tenant()\n        scoped_feature_id = f\"{tenant_id}:{feature_id}\"\n        return await self._base_provider.get_feature_by_id(scoped_feature_id)\n\n    async def stream_features(self) -&gt; AsyncGenerator[Dict, None]:\n        tenant_id = self._tenant_resolver.get_current_tenant()\n        async for feature in self._base_provider.stream_features():\n            if self._belongs_to_tenant(feature, tenant_id):\n                yield feature\n</code></pre>"},{"location":"day02_oop/#data-lineage-and-audit-trail","title":"Data Lineage and Audit Trail","text":"<pre><code>class AuditableProvider(MapDataProvider):\n    \"\"\"Provider that maintains audit trail.\"\"\"\n\n    def __init__(self, wrapped_provider: MapDataProvider, \n                 audit_store: 'AuditStore'):\n        self._provider = wrapped_provider\n        self._audit_store = audit_store\n\n    async def update_feature(self, feature_id: str, updates: Dict) -&gt; Dict:\n        # Record audit entry\n        audit_entry = AuditEntry(\n            operation=\"UPDATE\",\n            feature_id=feature_id,\n            changes=updates,\n            user_id=self._get_current_user(),\n            timestamp=datetime.now()\n        )\n\n        result = await self._provider.update_feature(feature_id, updates)\n        await self._audit_store.record(audit_entry)\n        return result\n</code></pre>"},{"location":"day02_oop/#geospatial-specific-enterprise-patterns","title":"Geospatial-Specific Enterprise Patterns","text":"<pre><code>class CoordinateSystemAwareProvider(MapDataProvider):\n    \"\"\"Provider that handles coordinate system transformations.\"\"\"\n\n    def __init__(self, base_provider: MapDataProvider, \n                 source_crs: int, target_crs: int):\n        self._provider = base_provider\n        self._transformer = Transformer.from_crs(source_crs, target_crs)\n\n    async def get_feature_by_id(self, feature_id: str) -&gt; Any:\n        feature = await self._provider.get_feature_by_id(feature_id)\n        return self._transform_feature(feature)\n\n    def _transform_feature(self, feature: Dict) -&gt; Dict:\n        geometry = feature[\"geometry\"]\n        if geometry[\"type\"] == \"Point\":\n            coords = geometry[\"coordinates\"]\n            x, y = self._transformer.transform(coords[0], coords[1])\n            geometry[\"coordinates\"] = [x, y]\n        # Handle other geometry types...\n        return feature\n\nclass SpatialIndexedProvider(MapDataProvider):\n    \"\"\"Provider with built-in spatial indexing.\"\"\"\n\n    def __init__(self, base_provider: MapDataProvider):\n        self._provider = base_provider\n        self._spatial_index = rtree.index.Index()\n        self._features_cache = {}\n        self._build_index()\n\n    async def query_bbox(self, min_x: float, min_y: float, \n                        max_x: float, max_y: float) -&gt; List[Dict]:\n        \"\"\"Efficient bounding box query using spatial index.\"\"\"\n        candidate_ids = list(self._spatial_index.intersection(\n            (min_x, min_y, max_x, max_y)\n        ))\n\n        features = []\n        for feature_id in candidate_ids:\n            if feature_id in self._features_cache:\n                feature = self._features_cache[feature_id]\n                if self._intersects_bbox(feature, min_x, min_y, max_x, max_y):\n                    features.append(feature)\n\n        return features\n</code></pre>"},{"location":"day02_oop/#testing-enterprise-architectures","title":"Testing Enterprise Architectures","text":""},{"location":"day02_oop/#contract-testing","title":"Contract Testing","text":"<pre><code>class ProviderContractTest:\n    \"\"\"Base test class that validates provider contracts.\"\"\"\n\n    def test_feature_retrieval_contract(self, provider: MapDataProvider):\n        \"\"\"Test that provider correctly implements feature retrieval.\"\"\"\n        # Arrange\n        test_feature_id = \"test_feature_123\"\n\n        # Act\n        feature = provider.get_feature_by_id(test_feature_id)\n\n        # Assert\n        if feature is not None:\n            assert \"properties\" in feature\n            assert \"geometry\" in feature\n            assert feature[\"type\"] == \"Feature\"\n\n    def test_streaming_contract(self, provider: MapDataProvider):\n        \"\"\"Test that provider correctly implements streaming.\"\"\"\n        feature_count = 0\n        for feature in provider.stream_features():\n            feature_count += 1\n            assert isinstance(feature, dict)\n            if feature_count &gt;= 10:  # Test first 10 features\n                break\n\n        assert feature_count &gt; 0  # Should have at least some features\n\n# Test different providers against the same contract\nclass TestGeoJSONProvider(ProviderContractTest):\n    @pytest.fixture\n    def provider(self):\n        return GeoJSONProvider(\"test_data.geojson\")\n\nclass TestPostGISProvider(ProviderContractTest):\n    @pytest.fixture\n    def provider(self):\n        return PostGISProvider(\"postgresql://test:test@localhost/testdb\")\n</code></pre>"},{"location":"day02_oop/#professional-development-exercises","title":"Professional Development Exercises","text":""},{"location":"day02_oop/#exercise-1-design-a-multi-source-data-aggregator","title":"Exercise 1: Design a Multi-Source Data Aggregator","text":"<p>Create a provider that aggregates data from multiple sources: - Combine real-time API data with static file data - Handle inconsistent schemas across sources - Implement conflict resolution strategies - Add performance monitoring and caching</p>"},{"location":"day02_oop/#exercise-2-build-a-plugin-ecosystem","title":"Exercise 2: Build a Plugin Ecosystem","text":"<p>Design a plugin system for custom data transformations: - Create a transformation plugin interface - Implement a plugin discovery mechanism - Add configuration management for plugins - Create sample plugins for common transformations</p>"},{"location":"day02_oop/#exercise-3-implement-event-sourcing","title":"Exercise 3: Implement Event Sourcing","text":"<p>Design an event-sourced provider that: - Captures all data mutations as events - Rebuilds state from event history - Supports point-in-time queries - Implements event replay for debugging</p>"},{"location":"day02_oop/#exercise-4-create-a-multi-tenant-gis-service","title":"Exercise 4: Create a Multi-Tenant GIS Service","text":"<p>Build a multi-tenant spatial data service: - Implement tenant isolation at the data layer - Add role-based access control - Create tenant-specific configuration - Implement cross-tenant analytics (where permitted)</p>"},{"location":"day02_oop/#industry-context-and-real-world-applications","title":"Industry Context and Real-World Applications","text":""},{"location":"day02_oop/#enterprise-gis-systems","title":"Enterprise GIS Systems","text":"<ul> <li>ESRI ArcGIS Enterprise: Multi-tier architecture with service-oriented design</li> <li>Oracle Spatial: Database-integrated spatial processing with pluggable engines</li> <li>PostGIS/PostgreSQL: Open-source spatial database with extension architecture</li> </ul>"},{"location":"day02_oop/#cloud-native-geospatial-services","title":"Cloud-Native Geospatial Services","text":"<ul> <li>AWS Location Services: Managed geospatial services with provider abstractions</li> <li>Google Maps Platform: API-driven architecture with multiple data sources</li> <li>Microsoft Azure Maps: Cloud-first design with hybrid deployment options</li> </ul>"},{"location":"day02_oop/#microservices-architecture","title":"Microservices Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Data Ingestion\u2502    \u2502  Spatial Query  \u2502    \u2502  Visualization  \u2502\n\u2502   Microservice  \u2502    \u2502  Microservice   \u2502    \u2502  Microservice   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\u2022 Provider Fabric\u2502    \u2502\u2022 Query Engine   \u2502    \u2502\u2022 Renderer       \u2502\n\u2502\u2022 ETL Pipeline   \u2502    \u2502\u2022 Cache Layer    \u2502    \u2502\u2022 Style Engine   \u2502\n\u2502\u2022 Data Validation\u2502    \u2502\u2022 Index Manager  \u2502    \u2502\u2022 Export Service \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Provider       \u2502\n                    \u2502  Registry       \u2502\n                    \u2502  Service        \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"day02_oop/#resources","title":"Resources","text":""},{"location":"day02_oop/#books-and-publications","title":"Books and Publications","text":"<ul> <li>Clean Architecture by Robert C. Martin: Fundamental principles of software architecture</li> <li>Patterns of Enterprise Application Architecture by Martin Fowler: Essential enterprise patterns</li> <li>Domain-Driven Design by Eric Evans: Domain modeling and bounded contexts</li> <li>Building Microservices by Sam Newman: Microservices architecture patterns</li> </ul>"},{"location":"day02_oop/#technical-references","title":"Technical References","text":"<ul> <li>Python ABCs Documentation</li> <li>Protocol-Based Typing</li> <li>Python Packaging Guide</li> <li>SOLID Principles in Python</li> </ul>"},{"location":"day02_oop/#industry-standards","title":"Industry Standards","text":"<ul> <li>OGC Web Feature Service (WFS)</li> <li>GeoPackage Specification</li> <li>Cloud Optimized GeoTIFF</li> </ul> <p>This module establishes the architectural foundation for building enterprise-grade geospatial systems. The patterns and principles learned here scale from small applications to global, distributed geospatial infrastructures.</p>"},{"location":"day03_api/","title":"Day 03 - Api","text":""},{"location":"day03_api/#learning-objectives","title":"Learning Objectives","text":"<p>This module covers enterprise-grade API design and implementation for geospatial services, focusing on scalability, reliability, and maintainability patterns used in production mapping and location-based systems. By the end of this module, you will understand:</p> <ul> <li>RESTful API design principles for geospatial resource modeling and hypermedia controls</li> <li>OpenAPI specification mastery including advanced schema composition and documentation generation</li> <li>Streaming architecture patterns for real-time geospatial data delivery and backpressure handling</li> <li>Production observability with structured logging, metrics collection, and distributed tracing</li> <li>Geospatial-specific API patterns including tiling protocols, spatial query optimization, and CRS handling</li> <li>Service mesh integration for microservices communication and traffic management</li> <li>API versioning strategies for evolving geospatial schemas and backward compatibility</li> </ul>"},{"location":"day03_api/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"day03_api/#geospatial-api-architecture-patterns","title":"Geospatial API Architecture Patterns","text":"<p>Resource-Oriented Design: Geospatial APIs model spatial entities as first-class resources with clear hierarchies and relationships:</p> <pre><code>/datasets/{dataset_id}                    # Spatial dataset resource\n\u251c\u2500\u2500 /features                            # Feature collection\n\u2502   \u251c\u2500\u2500 /{feature_id}                   # Individual feature\n\u2502   \u2514\u2500\u2500 /bbox/{bbox}                    # Spatial query\n\u251c\u2500\u2500 /tiles/{z}/{x}/{y}                  # Tile hierarchy\n\u2514\u2500\u2500 /metadata                           # Dataset metadata\n</code></pre> <p>Streaming-First Architecture: Large geospatial datasets require streaming patterns to prevent memory exhaustion and improve time-to-first-byte: - NDJSON streams for feature collections - Chunked tile responses for large raster data - Server-sent events for real-time location updates - WebSocket protocols for bidirectional spatial data exchange</p> <p>Spatial Query Optimization: - Bounding box queries with spatial indexing - Multi-resolution tiling for efficient data serving - Coordinate reference system (CRS) transformations at API boundaries - Geometry simplification based on zoom levels and client capabilities</p>"},{"location":"day03_api/#production-api-design-principles","title":"Production API Design Principles","text":"<p>Idempotency and Consistency: Geospatial updates must maintain spatial integrity and referential consistency across distributed systems.</p> <p>Eventual Consistency Models: Location-based services often require eventual consistency for performance, using patterns like: - CQRS (Command Query Responsibility Segregation) for read/write optimization - Event sourcing for audit trails and temporal queries - Conflict-free replicated data types (CRDTs) for distributed spatial editing</p> <p>Rate Limiting and Fair Usage: Tile servers and geocoding APIs implement sophisticated rate limiting: - Token bucket algorithms for burst handling - Geographic rate limiting based on request density - Adaptive throttling based on downstream service health</p>"},{"location":"day03_api/#core-concepts","title":"Core Concepts","text":""},{"location":"day03_api/#1-fastapi-overview","title":"1. FastAPI Overview","text":"<p>FastAPI uses type hints to validate inputs and auto-generate OpenAPI docs. It\u2019s async-first and great for streaming responses.</p>"},{"location":"day03_api/#2-pydantic-models","title":"2. Pydantic Models","text":"<p>Pydantic provides data validation using Python type annotations. It's the foundation for FastAPI's request/response validation.</p> <pre><code>from pydantic import BaseModel, Field\n\nclass TilePath(BaseModel):\n    z: int = Field(ge=0, le=22, description=\"Zoom level (0-22)\")\n    x: int = Field(description=\"Tile X coordinate\")\n    y: int = Field(description=\"Tile Y coordinate\")\n</code></pre> <p>Validation Features: - Type checking and conversion - Constraint validation (min/max values, regex patterns) - Custom validators - Automatic serialization/deserialization</p>"},{"location":"day03_api/#3-streaming-responses","title":"3. Streaming Responses","text":"<p>For large datasets, streaming responses prevent memory issues and improve user experience:</p> <pre><code>from fastapi.responses import StreamingResponse\n\nasync def stream_large_dataset():\n    async def generate():\n        for item in large_dataset:\n            yield f\"{item}\\n\"\n\n    return StreamingResponse(generate(), media_type=\"text/plain\")\n</code></pre>"},{"location":"day03_api/#code-walkthrough-this-repo","title":"Code Walkthrough (this repo)","text":""},{"location":"day03_api/#1-basic-fastapi-app-structure","title":"1. Basic FastAPI App Structure","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel, Field\n\napp = FastAPI(title=\"Spatial API Drills\")\n</code></pre> <p>Key Components: - <code>FastAPI()</code>: Main application instance - <code>title</code>: Sets the API title in OpenAPI docs - Additional metadata can be added for better documentation</p>"},{"location":"day03_api/#2-tile-endpoint","title":"2. Tile Endpoint","text":"<pre><code>@app.get(\"/tiles/{z}/{x}/{y}.mvt\")\nasync def get_tile(z: int, x: int, y: int):\n    if z &lt; 0 or z &gt; 22:\n        raise HTTPException(status_code=400, detail=\"Invalid zoom\")\n\n    # Demo stream of empty tile\n    async def streamer():\n        yield b\"\"  # replace with file streaming in drill\n\n    return StreamingResponse(\n        streamer(), \n        media_type=\"application/vnd.mapbox-vector-tile\"\n    )\n</code></pre> <p>Features: - Path Parameters: <code>{z}</code>, <code>{x}</code>, <code>{y}</code> are automatically parsed and validated - Validation: Zoom level is checked (0-22 range) - Error Handling: <code>HTTPException</code> returns proper HTTP status codes - Streaming: Uses <code>StreamingResponse</code> for efficient data delivery</p>"},{"location":"day03_api/#3-bounding-box-endpoint","title":"3. Bounding Box Endpoint","text":"<pre><code>@app.get(\"/stream-features\")\nasync def stream_features(\n    min_lat: float, \n    min_lon: float, \n    max_lat: float, \n    max_lon: float\n):\n    if min_lat &gt; max_lat or min_lon &gt; max_lon:\n        raise HTTPException(status_code=400, detail=\"Invalid bbox\")\n\n    async def streamer():\n        # demo NDJSON\n        yield b\"{}\\n\"\n\n    return StreamingResponse(\n        streamer(), \n        media_type=\"application/x-ndjson\"\n    )\n</code></pre> <p>Features: - Query Parameters: Automatically parsed from URL query string - Validation: Checks that bounding box coordinates are valid - NDJSON Format: Newline-delimited JSON for streaming</p>"},{"location":"day03_api/#run","title":"Run","text":""},{"location":"day03_api/#1-basic-setup","title":"1. Basic Setup","text":"<pre><code># Activate virtual environment\nsource .venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Run the API (dev)\nuvicorn src.day03_api.app:app --reload\n</code></pre>"},{"location":"day03_api/#access","title":"Access","text":"<ul> <li>API: http://localhost:8000</li> <li>Interactive Docs: http://localhost:8000/docs</li> <li>OpenAPI Schema: http://localhost:8000/openapi.json</li> <li>Prometheus Metrics: http://localhost:8000/metrics</li> </ul>"},{"location":"day03_api/#smoke-it-with-curl","title":"Smoke it with curl","text":"<pre><code># Test tile endpoint\ncurl \"http://localhost:8000/tiles/5/10/12.mvt\"\n\n# Test bbox endpoint\ncurl \"http://localhost:8000/stream-features?min_lat=37.0&amp;min_lon=-122.0&amp;max_lat=38.0&amp;max_lon=-121.0\"\n</code></pre>"},{"location":"day03_api/#exercises","title":"Exercises","text":""},{"location":"day03_api/#1-implement-file-streaming","title":"1) Implement File Streaming","text":"<p>Replace the placeholder tile streaming with actual file reading:</p> <pre><code>import os\nfrom pathlib import Path\n\n@app.get(\"/tiles/{z}/{x}/{y}.mvt\")\nasync def get_tile(z: int, x: int, y: int):\n    if z &lt; 0 or z &gt; 22:\n        raise HTTPException(status_code=400, detail=\"Invalid zoom\")\n\n    # Construct file path\n    tile_path = Path(f\"tiles/{z}/{x}/{y}.mvt\")\n\n    if not tile_path.exists():\n        raise HTTPException(status_code=404, detail=\"Tile not found\")\n\n    async def stream_tile():\n        with open(tile_path, \"rb\") as f:\n            while chunk := f.read(8192):  # 8KB chunks\n                yield chunk\n\n    return StreamingResponse(\n        stream_tile(),\n        media_type=\"application/vnd.mapbox-vector-tile\",\n        headers={\"Content-Length\": str(tile_path.stat().st_size)}\n    )\n</code></pre>"},{"location":"day03_api/#2-add-pydantic-request-models","title":"2) Add Pydantic Request Models","text":"<p>Create proper request models for validation:</p> <pre><code>from pydantic import BaseModel, Field, validator\nfrom typing import Optional\n\nclass BoundingBox(BaseModel):\n    min_lat: float = Field(..., ge=-90, le=90, description=\"Minimum latitude\")\n    min_lon: float = Field(..., ge=-180, le=180, description=\"Minimum longitude\")\n    max_lat: float = Field(..., ge=-90, le=90, description=\"Maximum latitude\")\n    max_lon: float = Field(..., ge=-180, le=180, description=\"Maximum longitude\")\n    limit: Optional[int] = Field(100, gt=0, le=1000, description=\"Maximum features to return\")\n    offset: Optional[int] = Field(0, ge=0, description=\"Number of features to skip\")\n\n    @validator('max_lat')\n    def max_lat_must_be_greater(cls, v, values):\n        if 'min_lat' in values and v &lt;= values['min_lat']:\n            raise ValueError('max_lat must be greater than min_lat')\n        return v\n\n    @validator('max_lon')\n    def max_lon_must_be_greater(cls, v, values):\n        if 'min_lon' in values and v &lt;= values['min_lon']:\n            raise ValueError('max_lon must be greater than min_lon')\n        return v\n\n@app.post(\"/stream-features\")\nasync def stream_features_post(bbox: BoundingBox):\n    # Now we have validated bbox data\n    async def streamer():\n        # TODO: Implement actual feature streaming\n        yield f'{{\"bbox\": {bbox.dict()}}}\\n'\n\n    return StreamingResponse(streamer(), media_type=\"application/x-ndjson\")\n</code></pre>"},{"location":"day03_api/#3-implement-real-feature-streaming","title":"3) Implement Real Feature Streaming","text":"<pre><code>import json\nfrom shapely.geometry import box\nfrom shapely.wkt import loads\n\n@app.get(\"/stream-features\")\nasync def stream_features(\n    min_lat: float, min_lon: float, max_lat: float, max_lon: float,\n    limit: int = 100, offset: int = 0\n):\n    if min_lat &gt; max_lat or min_lon &gt; max_lon:\n        raise HTTPException(status_code=400, detail=\"Invalid bbox\")\n\n    # Load features (in practice, this would come from a database)\n    features = load_features_from_source()\n\n    # Filter by bounding box\n    query_bbox = box(min_lon, min_lat, max_lon, max_lat)\n    filtered_features = [\n        f for f in features \n        if query_bbox.intersects(loads(f[\"geometry\"]))\n    ]\n\n    # Apply pagination\n    paginated_features = filtered_features[offset:offset + limit]\n\n    async def streamer():\n        for feature in paginated_features:\n            yield json.dumps(feature) + \"\\n\"\n\n    return StreamingResponse(\n        streamer(),\n        media_type=\"application/x-ndjson\",\n        headers={\n            \"X-Total-Count\": str(len(filtered_features)),\n            \"X-Page-Size\": str(len(paginated_features)),\n            \"X-Page-Offset\": str(offset)\n        }\n    )\n</code></pre>"},{"location":"day03_api/#advanced","title":"Advanced","text":""},{"location":"day03_api/#1-response-models","title":"1) Response Models","text":"<p>Define explicit response models for better documentation:</p> <pre><code>from typing import List, Optional\n\nclass Feature(BaseModel):\n    type: str = \"Feature\"\n    properties: dict\n    geometry: dict\n\nclass FeatureCollection(BaseModel):\n    type: str = \"FeatureCollection\"\n    features: List[Feature]\n    total_count: int\n    page_size: int\n    page_offset: int\n\n@app.get(\"/features\", response_model=FeatureCollection)\nasync def get_features(\n    min_lat: float, min_lon: float, max_lat: float, max_lon: float,\n    limit: int = 100, offset: int = 0\n):\n    # Implementation here\n    pass\n</code></pre>"},{"location":"day03_api/#2-custom-response-classes","title":"2) Custom Response Classes","text":"<p>Create custom response types for specific use cases:</p> <pre><code>from fastapi.responses import Response\nimport json\n\nclass GeoJSONResponse(Response):\n    def __init__(self, content, **kwargs):\n        super().__init__(\n            content=json.dumps(content),\n            media_type=\"application/geo+json\",\n            **kwargs\n        )\n\n@app.get(\"/features/geojson\")\nasync def get_features_geojson():\n    features = {\"type\": \"FeatureCollection\", \"features\": []}\n    return GeoJSONResponse(features)\n</code></pre>"},{"location":"day03_api/#3-middleware-and-dependencies","title":"3) Middleware and Dependencies","text":"<p>Add cross-cutting concerns:</p> <pre><code>from fastapi import Depends, Request\nimport time\n\nasync def log_request(request: Request):\n    start_time = time.time()\n    yield\n    process_time = time.time() - start_time\n    print(f\"{request.method} {request.url.path} took {process_time:.3f}s\")\n\n@app.get(\"/tiles/{z}/{x}/{y}.mvt\", dependencies=[Depends(log_request)])\nasync def get_tile(z: int, x: int, y: int):\n    # Implementation here\n    pass\n</code></pre>"},{"location":"day03_api/#best-practices","title":"Best Practices","text":""},{"location":"day03_api/#1-error-handling","title":"1. Error Handling","text":"<pre><code>from fastapi import HTTPException, status\n\n@app.get(\"/tiles/{z}/{x}/{y}.mvt\")\nasync def get_tile(z: int, x: int, y: int):\n    try:\n        # Implementation\n        pass\n    except FileNotFoundError:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"Tile {z}/{x}/{y} not found\"\n        )\n    except Exception as e:\n        raise HTTPException(\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n            detail=\"Internal server error\"\n        )\n</code></pre>"},{"location":"day03_api/#2-input-validation","title":"2. Input Validation","text":"<pre><code>from pydantic import validator\n\nclass TileRequest(BaseModel):\n    z: int = Field(..., ge=0, le=22)\n    x: int = Field(..., ge=0)\n    y: int = Field(..., ge=0)\n\n    @validator('x', 'y')\n    def validate_coordinates(cls, v, values):\n        if 'z' in values:\n            max_coord = 2 ** values['z'] - 1\n            if v &gt; max_coord:\n                raise ValueError(f'Coordinate {v} exceeds maximum for zoom {values[\"z\"]}')\n        return v\n</code></pre>"},{"location":"day03_api/#3-performance-considerations","title":"3. Performance Considerations","text":"<pre><code># Use async for I/O operations\nasync def load_tile_data(z: int, x: int, y: int):\n    # Async file reading or database query\n    pass\n\n# Use streaming for large responses\nasync def stream_large_dataset():\n    async def generator():\n        async for item in async_data_source():\n            yield process_item(item)\n\n    return StreamingResponse(generator())\n</code></pre>"},{"location":"day03_api/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"day03_api/#1-forgetting-to-await","title":"1. Forgetting to Await","text":"<pre><code># \u274c This won't work\n@app.get(\"/data\")\nasync def get_data():\n    result = fetch_from_database()  # Missing await\n    return result\n\n# \u2705 Correct async usage\n@app.get(\"/data\")\nasync def get_data():\n    result = await fetch_from_database()\n    return result\n</code></pre>"},{"location":"day03_api/#2-blocking-operations","title":"2. Blocking Operations","text":"<pre><code># \u274c This blocks the event loop\n@app.get(\"/slow\")\nasync def slow_endpoint():\n    time.sleep(10)  # Blocks everything\n    return {\"message\": \"done\"}\n\n# \u2705 Use asyncio.sleep or run in thread\n@app.get(\"/slow\")\nasync def slow_endpoint():\n    await asyncio.sleep(10)  # Non-blocking\n    return {\"message\": \"done\"}\n</code></pre>"},{"location":"day03_api/#3-memory-issues-with-large-data","title":"3. Memory Issues with Large Data","text":"<pre><code># \u274c Loads everything into memory\n@app.get(\"/large-dataset\")\nasync def get_large_dataset():\n    data = load_all_data()  # Could be GB of data\n    return data\n\n# \u2705 Stream the data\n@app.get(\"/large-dataset\")\nasync def get_large_dataset():\n    async def stream():\n        async for item in load_data_stream():\n            yield json.dumps(item) + \"\\n\"\n\n    return StreamingResponse(stream())\n</code></pre>"},{"location":"day03_api/#next-steps","title":"Next Steps","text":"<p>After completing this day: 1. Implement actual file streaming for tiles 2. Add database integration for features 3. Implement proper error handling and logging 4. Add authentication and rate limiting 5. Consider adding caching (Redis, etc.)</p>"},{"location":"day03_api/#enterprise-production-patterns","title":"Enterprise Production Patterns","text":""},{"location":"day03_api/#microservices-architecture-for-geospatial-systems","title":"Microservices Architecture for Geospatial Systems","text":"<pre><code># Service discovery and configuration\nclass ServiceRegistry:\n    def __init__(self):\n        self._services = {}\n        self._health_checks = {}\n\n    async def register_service(self, name: str, endpoint: str, \n                              health_check: Callable):\n        self._services[name] = endpoint\n        self._health_checks[name] = health_check\n\n    async def discover_service(self, name: str) -&gt; Optional[str]:\n        if name in self._services:\n            # Check health before returning\n            is_healthy = await self._health_checks[name]()\n            return self._services[name] if is_healthy else None\n        return None\n\n# Circuit breaker for downstream services\nclass CircuitBreaker:\n    def __init__(self, failure_threshold: int = 5, \n                 timeout_seconds: int = 60):\n        self.failure_threshold = failure_threshold\n        self.timeout_seconds = timeout_seconds\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n\n    async def call(self, func: Callable, *args, **kwargs):\n        if self.state == \"OPEN\":\n            if time.time() - self.last_failure_time &gt; self.timeout_seconds:\n                self.state = \"HALF_OPEN\"\n            else:\n                raise HTTPException(503, \"Service temporarily unavailable\")\n\n        try:\n            result = await func(*args, **kwargs)\n            if self.state == \"HALF_OPEN\":\n                self.state = \"CLOSED\"\n                self.failure_count = 0\n            return result\n        except Exception as e:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n\n            if self.failure_count &gt;= self.failure_threshold:\n                self.state = \"OPEN\"\n\n            raise e\n\n# Geospatial-specific middleware\n@app.middleware(\"http\")\nasync def geospatial_middleware(request: Request, call_next):\n    # Add spatial context to request\n    if \"bbox\" in request.query_params:\n        request.state.spatial_context = parse_bbox(request.query_params[\"bbox\"])\n\n    # Add coordinate system info\n    crs = request.headers.get(\"Accept-CRS\", \"EPSG:4326\")\n    request.state.target_crs = crs\n\n    response = await call_next(request)\n\n    # Add spatial headers to response\n    if hasattr(request.state, \"spatial_context\"):\n        response.headers[\"Content-CRS\"] = crs\n        response.headers[\"Content-Bbox\"] = format_bbox(request.state.spatial_context)\n\n    return response\n</code></pre>"},{"location":"day03_api/#advanced-error-handling-and-resilience","title":"Advanced Error Handling and Resilience","text":"<pre><code>class GeospatialError(Exception):\n    \"\"\"Base exception for geospatial operations.\"\"\"\n    def __init__(self, message: str, error_code: str, \n                 details: Dict[str, Any] = None):\n        super().__init__(message)\n        self.error_code = error_code\n        self.details = details or {}\n\nclass InvalidGeometryError(GeospatialError):\n    \"\"\"Raised when geometry is invalid or malformed.\"\"\"\n    pass\n\nclass SpatialIndexError(GeospatialError):\n    \"\"\"Raised when spatial index operations fail.\"\"\"\n    pass\n\n@app.exception_handler(GeospatialError)\nasync def geospatial_error_handler(request: Request, exc: GeospatialError):\n    return JSONResponse(\n        status_code=400,\n        content={\n            \"error\": {\n                \"code\": exc.error_code,\n                \"message\": str(exc),\n                \"details\": exc.details,\n                \"request_id\": request.headers.get(\"X-Request-ID\"),\n                \"timestamp\": datetime.utcnow().isoformat()\n            }\n        }\n    )\n\n# Retry decorator for external service calls\ndef retry_with_backoff(max_retries: int = 3, base_delay: float = 1.0):\n    def decorator(func):\n        @functools.wraps(func)\n        async def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return await func(*args, **kwargs)\n                except (httpx.RequestError, asyncio.TimeoutError) as e:\n                    if attempt == max_retries - 1:\n                        raise\n\n                    delay = base_delay * (2 ** attempt) + random.uniform(0, 0.1)\n                    await asyncio.sleep(delay)\n\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"day03_api/#advanced-streaming-and-real-time-patterns","title":"Advanced Streaming and Real-Time Patterns","text":"<pre><code>class SpatialEventStream:\n    \"\"\"Real-time spatial event streaming.\"\"\"\n\n    def __init__(self):\n        self._subscribers: Dict[str, Set[Queue]] = {}\n        self._spatial_index = rtree.index.Index()\n\n    async def subscribe_to_region(self, client_queue: Queue, \n                                  bbox: Tuple[float, float, float, float]):\n        \"\"\"Subscribe client to events in a spatial region.\"\"\"\n        region_id = f\"bbox_{bbox[0]}_{bbox[1]}_{bbox[2]}_{bbox[3]}\"\n\n        if region_id not in self._subscribers:\n            self._subscribers[region_id] = set()\n\n        self._subscribers[region_id].add(client_queue)\n\n        # Add region to spatial index\n        self._spatial_index.insert(id(client_queue), bbox)\n\n    async def publish_spatial_event(self, event: Dict, location: Point):\n        \"\"\"Publish event to all subscribers in the area.\"\"\"\n        # Find subscribers whose regions intersect with event location\n        bbox = (location.x, location.y, location.x, location.y)\n        intersecting_queues = list(self._spatial_index.intersection(bbox))\n\n        for queue_id in intersecting_queues:\n            # Find the actual queue object and send event\n            for region_subscribers in self._subscribers.values():\n                for queue in region_subscribers:\n                    if id(queue) == queue_id:\n                        try:\n                            await queue.put(event)\n                        except:\n                            # Clean up disconnected clients\n                            region_subscribers.discard(queue)\n\n@app.websocket(\"/ws/spatial-events\")\nasync def spatial_event_websocket(websocket: WebSocket):\n    await websocket.accept()\n    client_queue = asyncio.Queue()\n\n    try:\n        # Get initial subscription parameters\n        bbox_data = await websocket.receive_json()\n        bbox = tuple(bbox_data[\"bbox\"])\n\n        # Subscribe to spatial events\n        await spatial_stream.subscribe_to_region(client_queue, bbox)\n\n        # Send events to client\n        while True:\n            event = await client_queue.get()\n            await websocket.send_json(event)\n\n    except WebSocketDisconnect:\n        # Clean up subscription\n        pass\n</code></pre>"},{"location":"day03_api/#security-and-authentication-patterns","title":"Security and Authentication Patterns","text":"<pre><code># JWT-based authentication with spatial scopes\nclass SpatialJWTBearer(HTTPBearer):\n    def __init__(self, spatial_authority: str):\n        super().__init__(auto_error=True)\n        self.spatial_authority = spatial_authority\n\n    async def __call__(self, request: Request):\n        credentials = await super().__call__(request)\n        token = credentials.credentials\n\n        # Decode and validate JWT\n        try:\n            payload = jwt.decode(token, SPATIAL_SECRET_KEY, algorithms=[\"HS256\"])\n\n            # Check spatial permissions\n            spatial_scopes = payload.get(\"spatial_scopes\", [])\n            required_scope = self._determine_spatial_scope(request)\n\n            if required_scope not in spatial_scopes:\n                raise HTTPException(403, \"Insufficient spatial permissions\")\n\n            return payload\n\n        except jwt.PyJWTError:\n            raise HTTPException(401, \"Invalid authentication token\")\n\n    def _determine_spatial_scope(self, request: Request) -&gt; str:\n        \"\"\"Determine required spatial scope based on request.\"\"\"\n        if \"bbox\" in request.query_params:\n            bbox = parse_bbox(request.query_params[\"bbox\"])\n            if self._is_sensitive_area(bbox):\n                return \"sensitive_area_access\"\n\n        return \"general_spatial_access\"\n\n# API key rate limiting with geographic awareness\nclass GeographicRateLimiter:\n    def __init__(self, redis_client):\n        self.redis = redis_client\n\n    async def check_rate_limit(self, api_key: str, \n                              location: Optional[Point] = None) -&gt; bool:\n        \"\"\"Check rate limits with geographic weighting.\"\"\"\n        base_key = f\"rate_limit:{api_key}\"\n\n        # Standard rate limiting\n        current_count = await self.redis.get(f\"{base_key}:count\")\n        if current_count and int(current_count) &gt; 1000:  # 1000 req/hour\n            return False\n\n        # Geographic rate limiting for high-value areas\n        if location and self._is_high_traffic_area(location):\n            geo_key = f\"{base_key}:geo:{self._get_grid_cell(location)}\"\n            geo_count = await self.redis.get(geo_key)\n            if geo_count and int(geo_count) &gt; 100:  # 100 req/hour per grid cell\n                return False\n\n        return True\n</code></pre>"},{"location":"day03_api/#performance-optimization-and-caching","title":"Performance Optimization and Caching","text":"<pre><code>class SpatialCache:\n    \"\"\"Multi-level spatial caching system.\"\"\"\n\n    def __init__(self, redis_client, memory_cache_size: int = 1000):\n        self.redis = redis_client\n        self.memory_cache = LRUCache(memory_cache_size)\n        self.bloom_filter = BloomFilter(capacity=100000, error_rate=0.1)\n\n    async def get_features_in_bbox(self, bbox: Tuple[float, float, float, float], \n                                   zoom_level: int) -&gt; Optional[List[Dict]]:\n        \"\"\"Get cached features with spatial and zoom-level awareness.\"\"\"\n        cache_key = f\"features:{self._hash_bbox(bbox)}:z{zoom_level}\"\n\n        # Check bloom filter first\n        if cache_key not in self.bloom_filter:\n            return None\n\n        # Check memory cache\n        if cache_key in self.memory_cache:\n            return self.memory_cache[cache_key]\n\n        # Check Redis cache\n        cached_data = await self.redis.get(cache_key)\n        if cached_data:\n            features = json.loads(cached_data)\n            self.memory_cache[cache_key] = features\n            return features\n\n        return None\n\n    async def cache_features(self, bbox: Tuple[float, float, float, float],\n                           zoom_level: int, features: List[Dict],\n                           ttl_seconds: int = 3600):\n        \"\"\"Cache features with appropriate TTL based on data volatility.\"\"\"\n        cache_key = f\"features:{self._hash_bbox(bbox)}:z{zoom_level}\"\n\n        # Add to bloom filter\n        self.bloom_filter.add(cache_key)\n\n        # Cache in memory\n        self.memory_cache[cache_key] = features\n\n        # Cache in Redis with TTL\n        await self.redis.setex(\n            cache_key, \n            ttl_seconds, \n            json.dumps(features, cls=GeospatialJSONEncoder)\n        )\n\n# Geometry simplification middleware\n@app.middleware(\"http\")\nasync def geometry_optimization_middleware(request: Request, call_next):\n    response = await call_next(request)\n\n    # Simplify geometries based on zoom level and viewport\n    if (response.headers.get(\"content-type\") == \"application/geo+json\" and\n        \"zoom\" in request.query_params):\n\n        zoom_level = int(request.query_params[\"zoom\"])\n        tolerance = calculate_simplification_tolerance(zoom_level)\n\n        # Simplify response geometries\n        simplified_content = simplify_geojson_response(\n            response.content, tolerance\n        )\n\n        return Response(\n            content=simplified_content,\n            media_type=response.media_type,\n            headers=dict(response.headers)\n        )\n\n    return response\n</code></pre>"},{"location":"day03_api/#observability-and-monitoring","title":"Observability and Monitoring","text":"<pre><code># Structured logging for geospatial operations\nclass SpatialLogger:\n    def __init__(self):\n        self.logger = structlog.get_logger()\n\n    def log_spatial_query(self, bbox: Tuple[float, float, float, float],\n                         feature_count: int, query_time_ms: float,\n                         user_id: str, request_id: str):\n        self.logger.info(\n            \"spatial_query_executed\",\n            bbox=bbox,\n            feature_count=feature_count,\n            query_time_ms=query_time_ms,\n            user_id=user_id,\n            request_id=request_id,\n            bbox_area=calculate_bbox_area(bbox)\n        )\n\n# Distributed tracing for spatial operations\nasync def trace_spatial_operation(operation_name: str):\n    with tracer.start_as_current_span(operation_name) as span:\n        # Add spatial context to trace\n        if hasattr(request.state, \"spatial_context\"):\n            bbox = request.state.spatial_context\n            span.set_attribute(\"spatial.bbox\", str(bbox))\n            span.set_attribute(\"spatial.area\", calculate_bbox_area(bbox))\n\n        yield span\n\n# Custom metrics for geospatial APIs\nSPATIAL_QUERY_HISTOGRAM = Histogram(\n    \"spatial_query_duration_seconds\",\n    \"Time spent executing spatial queries\",\n    labelnames=[\"query_type\", \"zoom_level\", \"result_size_category\"]\n)\n\nSPATIAL_CACHE_HIT_COUNTER = Counter(\n    \"spatial_cache_hits_total\",\n    \"Number of spatial cache hits\",\n    labelnames=[\"cache_level\", \"zoom_level\"]\n)\n\n@app.middleware(\"http\")\nasync def spatial_metrics_middleware(request: Request, call_next):\n    start_time = time.time()\n\n    response = await call_next(request)\n\n    duration = time.time() - start_time\n\n    # Record spatial query metrics\n    if \"/bbox\" in request.url.path:\n        zoom_level = request.query_params.get(\"zoom\", \"unknown\")\n        result_count = response.headers.get(\"X-Feature-Count\", \"0\")\n        result_category = categorize_result_size(int(result_count))\n\n        SPATIAL_QUERY_HISTOGRAM.labels(\n            query_type=\"bbox\",\n            zoom_level=zoom_level,\n            result_size_category=result_category\n        ).observe(duration)\n\n    return response\n</code></pre>"},{"location":"day03_api/#real-world-integration-examples","title":"Real-World Integration Examples","text":""},{"location":"day03_api/#integration-with-postgis","title":"Integration with PostGIS","text":"<pre><code>class PostGISSpatialService:\n    \"\"\"Production-ready PostGIS integration.\"\"\"\n\n    def __init__(self, connection_pool: asyncpg.Pool):\n        self.pool = connection_pool\n\n    async def query_features_in_bbox(\n        self, \n        bbox: Tuple[float, float, float, float],\n        table_name: str,\n        limit: int = 1000,\n        offset: int = 0,\n        geometry_column: str = \"geom\"\n    ) -&gt; List[Dict]:\n        \"\"\"Efficient spatial query with PostGIS.\"\"\"\n\n        query = f\"\"\"\n        SELECT \n            ST_AsGeoJSON({geometry_column}) as geometry,\n            jsonb_build_object(\n                'id', id,\n                'properties', properties\n            ) as feature\n        FROM {table_name}\n        WHERE ST_Intersects(\n            {geometry_column},\n            ST_MakeEnvelope($1, $2, $3, $4, 4326)\n        )\n        ORDER BY ST_Area({geometry_column}) DESC\n        LIMIT $5 OFFSET $6\n        \"\"\"\n\n        async with self.pool.acquire() as connection:\n            rows = await connection.fetch(\n                query, bbox[0], bbox[1], bbox[2], bbox[3], limit, offset\n            )\n\n            features = []\n            for row in rows:\n                geometry = json.loads(row[\"geometry\"])\n                feature_data = json.loads(row[\"feature\"])\n\n                features.append({\n                    \"type\": \"Feature\",\n                    \"geometry\": geometry,\n                    \"properties\": feature_data[\"properties\"],\n                    \"id\": feature_data[\"id\"]\n                })\n\n            return features\n</code></pre>"},{"location":"day03_api/#cloud-storage-integration","title":"Cloud Storage Integration","text":"<pre><code>class CloudOptimizedGeoTIFFService:\n    \"\"\"Service for streaming Cloud Optimized GeoTIFF data.\"\"\"\n\n    def __init__(self, s3_client):\n        self.s3 = s3_client\n\n    async def stream_cog_tile(\n        self,\n        bucket: str,\n        key: str,\n        z: int, x: int, y: int,\n        bands: List[int] = None\n    ) -&gt; StreamingResponse:\n        \"\"\"Stream tile from Cloud Optimized GeoTIFF.\"\"\"\n\n        # Calculate byte range for tile\n        tile_bounds = calculate_tile_bounds(z, x, y)\n        byte_range = await self._calculate_cog_byte_range(\n            bucket, key, tile_bounds, bands\n        )\n\n        # Stream partial object from S3\n        async def tile_generator():\n            response = await self.s3.get_object(\n                Bucket=bucket,\n                Key=key,\n                Range=f\"bytes={byte_range[0]}-{byte_range[1]}\"\n            )\n\n            async for chunk in response[\"Body\"]:\n                yield chunk\n\n        return StreamingResponse(\n            tile_generator(),\n            media_type=\"image/tiff\",\n            headers={\n                \"Content-Length\": str(byte_range[1] - byte_range[0] + 1),\n                \"Accept-Ranges\": \"bytes\"\n            }\n        )\n</code></pre>"},{"location":"day03_api/#industry-standards-and-compliance","title":"Industry Standards and Compliance","text":""},{"location":"day03_api/#ogc-compliance","title":"OGC Compliance","text":"<pre><code>class OGCCompliantFeatureService:\n    \"\"\"OGC Web Feature Service (WFS) compliant implementation.\"\"\"\n\n    @app.get(\"/wfs\", response_class=XMLResponse)\n    async def get_capabilities():\n        \"\"\"Return WFS capabilities document.\"\"\"\n        capabilities = generate_wfs_capabilities()\n        return XMLResponse(content=capabilities)\n\n    @app.get(\"/wfs/features\", response_model=FeatureCollection)\n    async def get_features(\n        service: str = Query(\"WFS\"),\n        version: str = Query(\"2.0.0\"),\n        request: str = Query(\"GetFeature\"),\n        typeName: str = Query(...),\n        bbox: Optional[str] = Query(None),\n        maxFeatures: int = Query(1000),\n        outputFormat: str = Query(\"application/geopackage+sqlite3\")\n    ):\n        \"\"\"OGC WFS GetFeature operation.\"\"\"\n\n        if service != \"WFS\":\n            raise HTTPException(400, \"Invalid service parameter\")\n\n        # Validate and parse parameters according to OGC spec\n        parsed_bbox = parse_ogc_bbox(bbox) if bbox else None\n\n        # Execute spatial query\n        features = await spatial_service.query_features(\n            type_name=typeName,\n            bbox=parsed_bbox,\n            limit=maxFeatures\n        )\n\n        return FeatureCollection(features=features)\n</code></pre>"},{"location":"day03_api/#professional-development-exercises","title":"Professional Development Exercises","text":""},{"location":"day03_api/#exercise-1-build-a-multi-tenant-geospatial-api","title":"Exercise 1: Build a Multi-Tenant Geospatial API","text":"<p>Design and implement a multi-tenant spatial data API: - Implement tenant isolation at the database level - Add role-based access control for spatial operations - Create tenant-specific rate limiting - Implement cross-tenant spatial analytics (where permitted)</p>"},{"location":"day03_api/#exercise-2-real-time-location-tracking-service","title":"Exercise 2: Real-Time Location Tracking Service","text":"<p>Create a real-time location tracking and geofencing service: - WebSocket-based real-time location updates - Spatial event triggers and notifications - Historical trajectory storage and querying - Privacy controls and data retention policies</p>"},{"location":"day03_api/#exercise-3-distributed-tile-caching-system","title":"Exercise 3: Distributed Tile Caching System","text":"<p>Build a distributed tile caching and generation system: - Multi-level caching (CDN, Redis, local) - On-demand tile generation with queuing - Cache invalidation strategies - Performance monitoring and auto-scaling</p>"},{"location":"day03_api/#exercise-4-geospatial-data-pipeline-api","title":"Exercise 4: Geospatial Data Pipeline API","text":"<p>Design an API for managing geospatial ETL pipelines: - Job scheduling and dependency management - Progress tracking and error handling - Data quality validation and reporting - Integration with external data sources</p>"},{"location":"day03_api/#resources","title":"Resources","text":""},{"location":"day03_api/#technical-standards","title":"Technical Standards","text":"<ul> <li>OGC Web Feature Service (WFS) 2.0</li> <li>OGC API - Features</li> <li>Tile Map Service (TMS) Specification</li> <li>Cloud Optimized GeoTIFF</li> </ul>"},{"location":"day03_api/#fastapi-and-python","title":"FastAPI and Python","text":"<ul> <li>FastAPI Documentation</li> <li>Pydantic Documentation</li> <li>OpenAPI Specification</li> <li>Streaming Responses in FastAPI</li> </ul>"},{"location":"day03_api/#geospatial-technologies","title":"Geospatial Technologies","text":"<ul> <li>PostGIS Documentation</li> <li>GDAL/OGR Python Bindings</li> <li>Shapely Documentation</li> <li>GeoJSON Specification</li> </ul>"},{"location":"day03_api/#production-operations","title":"Production Operations","text":"<ul> <li>Site Reliability Engineering</li> <li>Microservices Patterns by Chris Richardson</li> <li>Building Event-Driven Microservices</li> </ul> <p>This module provides the foundation for building production-grade geospatial APIs that can scale to serve millions of users while maintaining data integrity, performance, and reliability standards expected in enterprise environments.</p>"},{"location":"day04_testing/","title":"Day 04 - Testing","text":""},{"location":"day04_testing/#learning-objectives","title":"Learning Objectives","text":"<p>This module covers comprehensive testing strategies essential for enterprise geospatial systems, emphasizing quality engineering practices that ensure reliability, performance, and maintainability at scale. By the end of this module, you will understand:</p> <ul> <li>Test-Driven Development (TDD) and Behavior-Driven Development (BDD) for geospatial domain modeling</li> <li>Advanced pytest patterns including fixtures, parametrization, plugins, and custom test discovery</li> <li>Property-based testing with Hypothesis for robust geospatial algorithm validation</li> <li>Integration testing strategies for distributed geospatial services and external API dependencies</li> <li>Performance testing methodologies including load, stress, spike, and endurance testing</li> <li>Mutation testing and fuzzing for ensuring test quality and edge case coverage</li> <li>Test automation in CI/CD pipelines with geographic data and spatial analysis workflows</li> <li>Chaos engineering principles for testing system resilience under failure conditions</li> </ul>"},{"location":"day04_testing/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"day04_testing/#quality-engineering-in-geospatial-systems","title":"Quality Engineering in Geospatial Systems","text":"<p>Spatial Data Complexity: Geospatial systems face unique testing challenges: - Coordinate precision: Floating-point arithmetic and transformation accuracy - Topology validation: Ensuring geometric integrity across operations - Scale variability: Testing from millimeter precision to global datasets - Temporal dimensions: Time-series spatial data and change detection - Multi-format support: Ensuring consistency across GeoJSON, WKT, MVT, etc.</p> <p>Distributed System Testing: Modern geospatial architectures require sophisticated testing approaches: - Service mesh testing: Inter-service communication and failure scenarios - Event-driven architecture validation: Async message processing and ordering - Spatial indexing correctness: R-tree, quadtree, and H3 index validation - Cache coherence testing: Multi-level caching with spatial invalidation</p>"},{"location":"day04_testing/#testing-pyramid-for-geospatial-applications","title":"Testing Pyramid for Geospatial Applications","text":"<pre><code>                    /\\\n                   /  \\     Manual Exploratory Testing\n                  /____\\    (Map visual validation, UX testing)\n                 /      \\   \n                /        \\  End-to-End Integration Tests\n               /__________\\ (Full pipeline, external services)\n              /            \\\n             /              \\ Contract/API Tests  \n            /________________\\ (Service boundaries, protocols)\n           /                  \\\n          /                    \\ Component/Service Tests\n         /______________________\\ (Business logic, spatial operations)\n        /                        \\\n       /                          \\ Unit Tests\n      /____________________________\\ (Pure functions, algorithms, data structures)\n</code></pre> <p>Optimal Distribution (Geospatial Systems): - Unit Tests (70%): Spatial algorithms, coordinate transformations, geometric operations - Component Tests (20%): Provider integrations, spatial queries, cache behavior - Integration Tests (8%): API contracts, database transactions, external service mocks - E2E Tests (2%): Critical user workflows, visual map rendering validation</p>"},{"location":"day04_testing/#core-concepts","title":"Core Concepts","text":""},{"location":"day04_testing/#1-tdd-cycle","title":"1) TDD Cycle","text":"<p>Red \u2192 Green \u2192 Refactor. Target small, observable units and keep test names behavior-focused.</p>"},{"location":"day04_testing/#2-pytest-essentials","title":"2) pytest Essentials","text":"<p>pytest is a powerful testing framework that makes testing simple and scalable:</p> <pre><code>import pytest\n\ndef test_simple():\n    assert 2 + 2 == 4\n\ndef test_with_fixture(sample_data):\n    assert len(sample_data) &gt; 0\n\n@pytest.mark.parametrize(\"input,expected\", [\n    (1, 2),\n    (2, 4),\n    (3, 6)\n])\ndef test_doubling(input, expected):\n    assert input * 2 == expected\n</code></pre>"},{"location":"day04_testing/#3-testing-pyramid","title":"3) Testing Pyramid","text":"<pre><code>    /\\\n   /  \\     E2E Tests (Few, Slow)\n  /____\\    \n /      \\   Integration Tests (Some, Medium)\n/________\\  Unit Tests (Many, Fast)\n</code></pre> <ul> <li>Unit Tests: Test individual functions/methods in isolation</li> <li>Integration Tests: Test how components work together</li> <li>End-to-End Tests: Test complete user workflows</li> </ul>"},{"location":"day04_testing/#code-walkthrough-this-repo","title":"Code Walkthrough (this repo)","text":""},{"location":"day04_testing/#1-api-smoke-tests","title":"1) API Smoke Tests","text":"<p><code>src/day04_testing/tests/test_smoke.py</code> verifies the Day 3 API routes.</p> <pre><code>from fastapi.testclient import TestClient\nfrom src.day03_api.app import app\n\ndef test_tiles_smoke():\n    client = TestClient(app)\n    resp = client.get(\"/tiles/0/0/0.mvt\")\n    assert resp.status_code == 200\n\ndef test_stream_features_invalid_bbox():\n    client = TestClient(app)\n    resp = client.get(\"/stream-features\", params={\n        \"min_lat\": 1, \"min_lon\": 0, \n        \"max_lat\": 0, \"max_lon\": 1\n    })\n    assert resp.status_code == 400\n</code></pre> <p>Key Points: - <code>TestClient</code>: FastAPI's testing utility that simulates HTTP requests - Arrange-Act-Assert: Clear test structure - Descriptive names: Test names explain what they're testing</p>"},{"location":"day04_testing/#2-test-client-usage","title":"2) Test Client Usage","text":"<pre><code>from fastapi.testclient import TestClient\n\n# Create a test client\nclient = TestClient(app)\n\n# Test GET requests\nresponse = client.get(\"/endpoint\")\nresponse = client.get(\"/endpoint\", params={\"param\": \"value\"})\n\n# Test POST requests\nresponse = client.post(\"/endpoint\", json={\"data\": \"value\"})\n\n# Test headers\nresponse = client.get(\"/endpoint\", headers={\"Authorization\": \"Bearer token\"})\n\n# Assertions\nassert response.status_code == 200\nassert response.json()[\"key\"] == \"expected_value\"\nassert \"expected_header\" in response.headers\n</code></pre>"},{"location":"day04_testing/#running-tests","title":"Running Tests","text":"<pre><code># Activate virtual environment\nsource .venv/bin/activate\n\n# Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test file\npytest src/day04_testing/tests/test_smoke.py\n\n# Run specific test function\npytest src/day04_testing/tests/test_smoke.py::test_tiles_smoke\n</code></pre>"},{"location":"day04_testing/#options","title":"Options","text":"<pre><code># Run tests and show coverage\npytest --cov=src\n\n# Run tests in parallel\npytest -n auto\n\n# Stop on first failure\npytest -x\n\n# Show local variables on failure\npytest -l\n\n# Run only tests matching a pattern\npytest -k \"bbox\"\n</code></pre>"},{"location":"day04_testing/#exercises","title":"Exercises","text":""},{"location":"day04_testing/#1-unit-tests-for-roadnetwork","title":"1) Unit Tests for RoadNetwork","text":"<p>Create <code>test_road_network.py</code> to test the core data processing:</p> <pre><code>import pytest\nfrom src.day07_mock.mock_test.road_network import RoadNetwork\nfrom unittest.mock import patch, mock_open\nimport csv\n\nclass TestRoadNetwork:\n    @pytest.fixture\n    def sample_csv_data(self):\n        return \"\"\"road_id,name,geometry,speed_limit,road_type,last_updated\nR001,El Camino Real,\"LINESTRING(-122.123 37.456, -122.124 37.457)\",50,arterial,2024-01-15T10:00:00Z\nR002,Page Mill Rd,\"LINESTRING(-122.124 37.457, -122.125 37.458)\",40,residential,2024-01-15T10:00:00Z\"\"\"\n\n    @pytest.fixture\n    def road_network(self, sample_csv_data):\n        with patch(\"builtins.open\", mock_open(read_data=sample_csv_data)):\n            with patch(\"csv.DictReader\") as mock_reader:\n                mock_reader.return_value = [\n                    {\n                        \"road_id\": \"R001\",\n                        \"name\": \"El Camino Real\",\n                        \"geometry\": \"LINESTRING(-122.123 37.456, -122.124 37.457)\",\n                        \"speed_limit\": \"50\",\n                        \"road_type\": \"arterial\",\n                        \"last_updated\": \"2024-01-15T10:00:00Z\"\n                    },\n                    {\n                        \"road_id\": \"R002\",\n                        \"name\": \"Page Mill Rd\",\n                        \"geometry\": \"LINESTRING(-122.124 37.457, -122.125 37.458)\",\n                        \"speed_limit\": \"40\",\n                        \"road_type\": \"residential\",\n                        \"last_updated\": \"2024-01-15T10:00:00Z\"\n                    }\n                ]\n                network = RoadNetwork(\"dummy_path.csv\")\n                return network\n\n    def test_loads_roads_correctly(self, road_network):\n        assert len(road_network._roads) == 2\n        assert \"R001\" in road_network._roads\n        assert \"R002\" in road_network._roads\n\n    def test_find_roads_in_bbox(self, road_network):\n        roads = road_network.find_roads_in_bbox(37.45, -122.13, 37.46, -122.12)\n        assert len(roads) == 2\n\n    def test_get_connected_roads(self, road_network):\n        connected = road_network.get_connected_roads(\"R001\")\n        assert \"R002\" in connected\n\n    def test_update_road(self, road_network):\n        updated = road_network.update_road(\"R001\", speed_limit=60)\n        assert updated.speed_limit == 60\n        assert updated.road_type == \"arterial\"  # Unchanged\n</code></pre>"},{"location":"day04_testing/#2-api-tests","title":"2) API Tests","text":"<p>Create comprehensive API tests:</p> <pre><code>import pytest\nfrom fastapi.testclient import TestClient\nfrom src.day07_mock.mock_test.api import app\nimport json\n\nclass TestAPI:\n    @pytest.fixture\n    def client(self):\n        return TestClient(app)\n\n    def test_roads_bbox_valid(self, client):\n        response = client.get(\"/roads/bbox\", params={\n            \"min_lat\": 37.0, \"min_lon\": -122.0,\n            \"max_lat\": 38.0, \"max_lon\": -121.0\n        })\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"type\"] == \"FeatureCollection\"\n        assert \"features\" in data\n\n    def test_roads_bbox_invalid(self, client):\n        response = client.get(\"/roads/bbox\", params={\n            \"min_lat\": 38.0, \"min_lon\": -122.0,\n            \"max_lat\": 37.0, \"max_lon\": -121.0  # Invalid bbox\n        })\n        assert response.status_code == 400\n\n    def test_roads_bbox_pagination(self, client):\n        response = client.get(\"/roads/bbox\", params={\n            \"min_lat\": 37.0, \"min_lon\": -122.0,\n            \"max_lat\": 38.0, \"max_lon\": -121.0,\n            \"limit\": 5, \"offset\": 0\n        })\n        assert response.status_code == 200\n\n    def test_connected_roads(self, client):\n        response = client.get(\"/roads/R001/connected\")\n        assert response.status_code == 200\n        assert isinstance(response.json(), list)\n\n    def test_update_road(self, client):\n        update_data = {\"speed_limit\": 55}\n        response = client.post(\"/roads/R001/update\", json=update_data)\n        assert response.status_code == 200\n        data = response.json()\n        assert data[\"properties\"][\"speed_limit\"] == 55\n\n    def test_update_road_invalid(self, client):\n        update_data = {\"speed_limit\": -5}  # Invalid speed\n        response = client.post(\"/roads/R001/update\", json=update_data)\n        assert response.status_code == 422  # Validation error\n</code></pre>"},{"location":"day04_testing/#3-property-based-testing-with-hypothesis","title":"3) Property-Based Testing with Hypothesis","text":"<pre><code>import pytest\nfrom hypothesis import given, strategies as st\nfrom shapely.geometry import box\nfrom src.day07_mock.mock_test.road_network import RoadNetwork\n\nclass TestRoadNetworkProperties:\n    @given(\n        min_lat=st.floats(min_value=-90, max_value=90),\n        min_lon=st.floats(min_value=-180, max_value=180),\n        max_lat=st.floats(min_value=-90, max_value=90),\n        max_lon=st.floats(min_value=-180, max_value=180)\n    )\n    def test_bbox_query_always_returns_list(self, min_lat, min_lon, max_lat, max_lon):\n        # Skip invalid bboxes\n        if min_lat &gt; max_lat or min_lon &gt; max_lon:\n            return\n\n        # Create a minimal network for testing\n        network = RoadNetwork(\"dummy_path.csv\")\n        result = network.find_roads_in_bbox(min_lat, min_lon, max_lat, max_lon)\n        assert isinstance(result, list)\n\n    @given(\n        road_id=st.text(min_size=1, max_size=10)\n    )\n    def test_connected_roads_always_returns_list(self, road_id):\n        network = RoadNetwork(\"dummy_path.csv\")\n        result = network.get_connected_roads(road_id)\n        assert isinstance(result, list)\n        assert all(isinstance(x, str) for x in result)\n</code></pre>"},{"location":"day04_testing/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"day04_testing/#1-fixtures-and-dependency-injection","title":"1) Fixtures and Dependency Injection","text":"<pre><code>import pytest\nfrom unittest.mock import Mock\n\n@pytest.fixture\ndef mock_database():\n    \"\"\"Mock database connection for testing.\"\"\"\n    db = Mock()\n    db.query.return_value.filter.return_value.all.return_value = [\n        {\"id\": 1, \"name\": \"Test Road\"}\n    ]\n    return db\n\n@pytest.fixture\ndef sample_geojson():\n    \"\"\"Sample GeoJSON data for testing.\"\"\"\n    return {\n        \"type\": \"FeatureCollection\",\n        \"features\": [\n            {\n                \"type\": \"Feature\",\n                \"properties\": {\"id\": \"R001\", \"name\": \"Test Road\"},\n                \"geometry\": {\n                    \"type\": \"LineString\",\n                    \"coordinates\": [[-122.123, 37.456], [-122.124, 37.457]]\n                }\n            }\n        ]\n    }\n\ndef test_with_fixtures(mock_database, sample_geojson):\n    # Use the fixtures in your test\n    assert mock_database.query.called\n    assert len(sample_geojson[\"features\"]) == 1\n</code></pre>"},{"location":"day04_testing/#2-parametrized-tests","title":"2) Parametrized Tests","text":"<pre><code>import pytest\n\n@pytest.mark.parametrize(\"bbox,expected_count\", [\n    ((37.0, -122.0, 38.0, -121.0), 2),  # Large bbox\n    ((37.45, -122.13, 37.46, -122.12), 2),  # Small bbox\n    ((0.0, 0.0, 1.0, 1.0), 0),  # No roads in this area\n])\ndef test_bbox_queries(bbox, expected_count):\n    min_lat, min_lon, max_lat, max_lon = bbox\n    network = RoadNetwork(\"test_data.csv\")\n    roads = network.find_roads_in_bbox(min_lat, min_lon, max_lat, max_lon)\n    assert len(roads) == expected_count\n\n@pytest.mark.parametrize(\"invalid_bbox\", [\n    (38.0, -122.0, 37.0, -121.0),  # min_lat &gt; max_lat\n    (37.0, -121.0, 38.0, -122.0),  # min_lon &gt; max_lon\n])\ndef test_invalid_bbox_raises_error(invalid_bbox):\n    min_lat, min_lon, max_lat, max_lon = invalid_bbox\n    network = RoadNetwork(\"test_data.csv\")\n\n    with pytest.raises(ValueError):\n        network.find_roads_in_bbox(min_lat, min_lon, max_lat, max_lon)\n</code></pre>"},{"location":"day04_testing/#3-mocking-external-dependencies","title":"3) Mocking External Dependencies","text":"<pre><code>from unittest.mock import patch, MagicMock\nimport httpx\n\ndef test_tile_fetching_with_mock():\n    with patch(\"httpx.AsyncClient\") as mock_client:\n        # Setup mock response\n        mock_response = MagicMock()\n        mock_response.content = b\"fake_tile_data\"\n        mock_response.raise_for_status.return_value = None\n\n        mock_client_instance = MagicMock()\n        mock_client_instance.get.return_value = mock_response\n        mock_client.return_value.__aenter__.return_value = mock_client_instance\n\n        # Test the function\n        from src.day01_concurrency.tile_fetcher import fetch_tile\n        import asyncio\n\n        result = asyncio.run(fetch_tile(mock_client_instance, 5, 10, 12))\n        assert result == b\"fake_tile_data\"\n</code></pre>"},{"location":"day04_testing/#load-testing-lightweight","title":"Load Testing (lightweight)","text":""},{"location":"day04_testing/#1-pytest-benchmark","title":"1) pytest-benchmark","text":"<pre><code>import pytest\nfrom fastapi.testclient import TestClient\nfrom src.day07_mock.mock_test.api import app\n\ndef test_api_performance(benchmark):\n    client = TestClient(app)\n\n    def make_request():\n        return client.get(\"/roads/bbox\", params={\n            \"min_lat\": 37.0, \"min_lon\": -122.0,\n            \"max_lat\": 38.0, \"max_lon\": -121.0\n        })\n\n    result = benchmark(make_request)\n    assert result.status_code == 200\n\ndef test_concurrent_requests(benchmark):\n    import asyncio\n    import httpx\n\n    async def make_concurrent_requests():\n        async with httpx.AsyncClient() as client:\n            tasks = [\n                client.get(\"http://localhost:8000/roads/bbox\", params={\n                    \"min_lat\": 37.0, \"min_lon\": -122.0,\n                    \"max_lat\": 38.0, \"max_lon\": -121.0\n                })\n                for _ in range(10)\n            ]\n            responses = await asyncio.gather(*tasks)\n            return responses\n\n    # This would need a running server\n    # result = benchmark(lambda: asyncio.run(make_concurrent_requests()))\n</code></pre>"},{"location":"day04_testing/#2-load-testing-with-locust","title":"2. Load Testing with Locust","text":"<p>Create <code>locustfile.py</code> for more sophisticated load testing:</p> <pre><code>from locust import HttpUser, task, between\n\nclass RoadNetworkUser(HttpUser):\n    wait_time = between(1, 3)\n\n    @task(3)\n    def test_bbox_query(self):\n        self.client.get(\"/roads/bbox\", params={\n            \"min_lat\": 37.0, \"min_lon\": -122.0,\n            \"max_lat\": 38.0, \"max_lon\": -121.0\n        })\n\n    @task(1)\n    def test_connected_roads(self):\n        self.client.get(\"/roads/R001/connected\")\n\n    @task(1)\n    def test_update_road(self):\n        self.client.post(\"/roads/R001/update\", json={\"speed_limit\": 55})\n</code></pre>"},{"location":"day04_testing/#test-coverage","title":"Test Coverage","text":""},{"location":"day04_testing/#1-measuring-coverage","title":"1. Measuring Coverage","text":"<pre><code># Install coverage tools\npip install pytest-cov\n\n# Run tests with coverage\npytest --cov=src --cov-report=html\n\n# Generate detailed coverage report\npytest --cov=src --cov-report=term-missing\n</code></pre>"},{"location":"day04_testing/#2-coverage-configuration","title":"2. Coverage Configuration","text":"<p>Create <code>.coveragerc</code> file:</p> <pre><code>[run]\nsource = src\nomit = \n    */tests/*\n    */__init__.py\n\n[report]\nexclude_lines =\n    pragma: no cover\n    def __repr__\n    raise AssertionError\n    raise NotImplementedError\n</code></pre>"},{"location":"day04_testing/#best-practices","title":"Best Practices","text":""},{"location":"day04_testing/#1-test-organization","title":"1. Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/           # Unit tests\n\u2502   \u251c\u2500\u2500 test_road_network.py\n\u2502   \u2514\u2500\u2500 test_api.py\n\u251c\u2500\u2500 integration/    # Integration tests\n\u2502   \u2514\u2500\u2500 test_end_to_end.py\n\u2514\u2500\u2500 conftest.py    # Shared fixtures\n</code></pre>"},{"location":"day04_testing/#2-test-naming","title":"2. Test Naming","text":"<pre><code># \u2705 Descriptive test names\ndef test_find_roads_in_bbox_returns_empty_list_for_area_with_no_roads():\n    pass\n\ndef test_update_road_speed_limit_updates_correct_field():\n    pass\n\n# \u274c Unclear test names\ndef test_bbox():\n    pass\n\ndef test_update():\n    pass\n</code></pre>"},{"location":"day04_testing/#3-test-data-management","title":"3. Test Data Management","text":"<pre><code>@pytest.fixture(scope=\"session\")\ndef sample_roads_data():\n    \"\"\"Load test data once for all tests.\"\"\"\n    return load_test_data_from_file(\"sample_roads.csv\")\n\n@pytest.fixture\ndef road_network(sample_roads_data):\n    \"\"\"Create fresh network for each test.\"\"\"\n    with patch(\"builtins.open\", mock_open(read_data=sample_roads_data)):\n        return RoadNetwork(\"dummy_path.csv\")\n</code></pre>"},{"location":"day04_testing/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"day04_testing/#1-testing-implementation-details","title":"1. Testing Implementation Details","text":"<pre><code># \u274c Testing internal state\ndef test_internal_index():\n    network = RoadNetwork(\"data.csv\")\n    assert \"_index\" in network.__dict__  # Too specific\n\n# \u2705 Testing public behavior\ndef test_can_find_road_by_id():\n    network = RoadNetwork(\"data.csv\")\n    road = network.get_road_by_id(\"R001\")\n    assert road is not None\n</code></pre>"},{"location":"day04_testing/#2-over-mocking","title":"2. Over-Mocking","text":"<pre><code># \u274c Mocking everything\n@patch(\"pathlib.Path\")\n@patch(\"csv.DictReader\")\n@patch(\"shapely.wkt.loads\")\ndef test_over_mocked(mock_loads, mock_reader, mock_path):\n    # Test becomes brittle and hard to maintain\n\n# \u2705 Mock only external dependencies\ndef test_with_minimal_mocking():\n    with patch(\"builtins.open\", mock_open(read_data=sample_csv)):\n        network = RoadNetwork(\"data.csv\")\n        # Test the actual logic\n</code></pre>"},{"location":"day04_testing/#3-ignoring-edge-cases","title":"3. Ignoring Edge Cases","text":"<pre><code># \u274c Only testing happy path\ndef test_bbox_query():\n    roads = network.find_roads_in_bbox(37.0, -122.0, 38.0, -121.0)\n    assert len(roads) &gt; 0\n\n# \u2705 Test edge cases too\ndef test_bbox_query_empty_area():\n    roads = network.find_roads_in_bbox(0.0, 0.0, 1.0, 1.0)\n    assert len(roads) == 0\n\ndef test_bbox_query_invalid_coordinates():\n    with pytest.raises(ValueError):\n        network.find_roads_in_bbox(38.0, -122.0, 37.0, -121.0)\n</code></pre>"},{"location":"day04_testing/#next-steps","title":"Next Steps","text":"<p>After completing this day: 1. Achieve &gt;80% test coverage on core functionality 2. Add integration tests with real data 3. Implement performance benchmarks 4. Set up continuous integration (CI) pipeline 5. Add property-based tests for complex logic</p>"},{"location":"day04_testing/#advanced-testing-patterns","title":"Advanced Testing Patterns","text":""},{"location":"day04_testing/#chaos-engineering-for-geospatial-systems","title":"Chaos Engineering for Geospatial Systems","text":"<pre><code>class SpatialChaosExperiment:\n    \"\"\"Chaos engineering for testing spatial system resilience.\"\"\"\n\n    def __init__(self, spatial_service: SpatialService):\n        self.service = spatial_service\n        self.failures_injected = []\n\n    async def inject_coordinate_drift(self, drift_factor: float = 0.001):\n        \"\"\"Inject small coordinate drifts to test precision handling.\"\"\"\n        original_transform = self.service.coordinate_transformer.transform\n\n        def drifted_transform(x: float, y: float) -&gt; Tuple[float, float]:\n            drift_x = x + (random.random() - 0.5) * drift_factor\n            drift_y = y + (random.random() - 0.5) * drift_factor\n            return original_transform(drift_x, drift_y)\n\n        self.service.coordinate_transformer.transform = drifted_transform\n        self.failures_injected.append(\"coordinate_drift\")\n\n    async def simulate_spatial_index_corruption(self):\n        \"\"\"Simulate partial spatial index corruption.\"\"\"\n        # Randomly remove some entries from spatial index\n        if hasattr(self.service, '_spatial_index'):\n            index = self.service._spatial_index\n            entries = list(index.intersection(index.bounds))\n            corrupted_entries = random.sample(entries, len(entries) // 10)\n\n            for entry_id in corrupted_entries:\n                try:\n                    index.delete(entry_id, index.get_bounds(entry_id))\n                except:\n                    pass  # Index might already be corrupted\n\n        self.failures_injected.append(\"index_corruption\")\n\n    async def induce_memory_pressure(self, target_mb: int = 100):\n        \"\"\"Create memory pressure to test resource handling.\"\"\"\n        memory_hog = []\n        try:\n            # Allocate memory in chunks\n            for _ in range(target_mb):\n                memory_hog.append(b'x' * (1024 * 1024))  # 1MB chunks\n\n            # Hold memory for test duration\n            await asyncio.sleep(1)\n        finally:\n            del memory_hog\n\n        self.failures_injected.append(\"memory_pressure\")\n\n@pytest.fixture\nasync def chaos_experiment(spatial_service):\n    experiment = SpatialChaosExperiment(spatial_service)\n    yield experiment\n\n    # Cleanup: restore original state if possible\n    if hasattr(spatial_service, '_reset_to_clean_state'):\n        await spatial_service._reset_to_clean_state()\n\nclass TestSpatialResilience:\n    \"\"\"Test suite for spatial system resilience under chaos conditions.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_bbox_query_under_coordinate_drift(self, chaos_experiment):\n        \"\"\"Test that bbox queries remain stable under coordinate drift.\"\"\"\n        # Inject coordinate drift\n        await chaos_experiment.inject_coordinate_drift(drift_factor=0.0001)\n\n        # Original query\n        bbox = (37.0, -122.0, 38.0, -121.0)\n        results_before = await chaos_experiment.service.query_bbox(bbox)\n\n        # Query with drift should still return reasonable results\n        results_after = await chaos_experiment.service.query_bbox(bbox)\n\n        # Results should be similar (allowing for some drift tolerance)\n        assert abs(len(results_before) - len(results_after)) &lt;= 2\n\n    @pytest.mark.asyncio\n    async def test_spatial_operations_during_memory_pressure(self, chaos_experiment):\n        \"\"\"Test spatial operations continue working under memory pressure.\"\"\"\n        # Create memory pressure\n        memory_task = asyncio.create_task(\n            chaos_experiment.induce_memory_pressure(target_mb=50)\n        )\n\n        try:\n            # Perform spatial operations during memory pressure\n            bbox = (37.0, -122.0, 38.0, -121.0)\n            results = await chaos_experiment.service.query_bbox(bbox)\n\n            # Should not fail completely, might have reduced performance\n            assert isinstance(results, list)\n\n        finally:\n            await memory_task\n</code></pre>"},{"location":"day04_testing/#mutation-testing-for-spatial-algorithms","title":"Mutation Testing for Spatial Algorithms","text":"<pre><code># Install: pip install mutmut\n# Usage: mutmut run --paths-to-mutate=src/spatial_algorithms/\n\nclass TestSpatialAlgorithmRobustness:\n    \"\"\"Test suite designed to catch mutations in spatial algorithms.\"\"\"\n\n    def test_distance_calculation_edge_cases(self):\n        \"\"\"Comprehensive tests to catch distance calculation mutations.\"\"\"\n        from src.spatial_algorithms import calculate_distance\n\n        # Test identical points\n        assert calculate_distance(0, 0, 0, 0) == 0\n\n        # Test symmetric property\n        d1 = calculate_distance(1, 1, 2, 2)\n        d2 = calculate_distance(2, 2, 1, 1)\n        assert abs(d1 - d2) &lt; 1e-10\n\n        # Test triangle inequality\n        p1, p2, p3 = (0, 0), (1, 0), (1, 1)\n        d12 = calculate_distance(*p1, *p2)\n        d23 = calculate_distance(*p2, *p3)\n        d13 = calculate_distance(*p1, *p3)\n        assert d12 + d23 &gt;= d13 - 1e-10\n\n        # Test known distances\n        assert abs(calculate_distance(0, 0, 3, 4) - 5.0) &lt; 1e-10\n\n        # Test extreme values\n        huge_val = 1e10\n        assert calculate_distance(0, 0, huge_val, 0) == huge_val\n\n    def test_bbox_intersection_mutations(self):\n        \"\"\"Tests designed to catch bbox intersection logic mutations.\"\"\"\n        from src.spatial_algorithms import bbox_intersects\n\n        # Test complete overlap\n        bbox1 = (0, 0, 2, 2)\n        bbox2 = (1, 1, 3, 3)\n        assert bbox_intersects(bbox1, bbox2) == True\n\n        # Test no overlap\n        bbox1 = (0, 0, 1, 1)\n        bbox2 = (2, 2, 3, 3)\n        assert bbox_intersects(bbox1, bbox2) == False\n\n        # Test edge touching (depends on implementation)\n        bbox1 = (0, 0, 1, 1)\n        bbox2 = (1, 1, 2, 2)\n        result = bbox_intersects(bbox1, bbox2)\n        assert isinstance(result, bool)  # Should not crash\n\n        # Test degenerate bboxes\n        bbox1 = (0, 0, 0, 0)  # Point\n        bbox2 = (0, 0, 1, 1)\n        result = bbox_intersects(bbox1, bbox2)\n        assert isinstance(result, bool)\n</code></pre>"},{"location":"day04_testing/#fuzzing-for-geospatial-input-validation","title":"Fuzzing for Geospatial Input Validation","text":"<pre><code>import hypothesis.strategies as st\nfrom hypothesis import given, assume, settings, HealthCheck\n\nclass TestGeospatialInputFuzzing:\n    \"\"\"Fuzz testing for geospatial input handling.\"\"\"\n\n    @given(\n        lat=st.floats(min_value=-90, max_value=90, allow_nan=False),\n        lon=st.floats(min_value=-180, max_value=180, allow_nan=False)\n    )\n    def test_coordinate_validation_never_crashes(self, lat, lon):\n        \"\"\"Coordinate validation should never crash, regardless of input.\"\"\"\n        from src.spatial_validation import validate_coordinate\n\n        # Should either return True or raise a specific exception\n        try:\n            result = validate_coordinate(lat, lon)\n            assert isinstance(result, bool)\n        except (ValueError, TypeError) as e:\n            # Acceptable to raise validation errors\n            assert str(e)  # Error message should not be empty\n\n    @given(\n        coords=st.lists(\n            st.tuples(\n                st.floats(min_value=-180, max_value=180, allow_nan=False),\n                st.floats(min_value=-90, max_value=90, allow_nan=False)\n            ),\n            min_size=3,\n            max_size=1000\n        )\n    )\n    @settings(suppress_health_check=[HealthCheck.too_slow], deadline=5000)\n    def test_polygon_validation_fuzzing(self, coords):\n        \"\"\"Polygon validation should handle arbitrary coordinate sequences.\"\"\"\n        from src.spatial_validation import validate_polygon\n\n        # Ensure polygon is closed\n        if coords and coords[0] != coords[-1]:\n            coords.append(coords[0])\n\n        try:\n            result = validate_polygon(coords)\n            assert isinstance(result, bool)\n\n            # If valid, polygon should have basic properties\n            if result:\n                assert len(coords) &gt;= 4  # Minimum for closed polygon\n                assert coords[0] == coords[-1]  # Must be closed\n\n        except (ValueError, TypeError, OverflowError) as e:\n            # Acceptable exceptions for invalid input\n            pass\n\n    @given(\n        geojson_like=st.recursive(\n            st.one_of(\n                st.floats(allow_nan=False, allow_infinity=False),\n                st.text(),\n                st.booleans(),\n                st.none()\n            ),\n            lambda children: st.one_of(\n                st.lists(children, max_size=20),\n                st.dictionaries(st.text(max_size=50), children, max_size=10)\n            ),\n            max_leaves=100\n        )\n    )\n    def test_geojson_parser_robustness(self, geojson_like):\n        \"\"\"GeoJSON parser should handle arbitrary JSON-like structures.\"\"\"\n        from src.geojson_parser import parse_geojson\n\n        try:\n            result = parse_geojson(geojson_like)\n            # If parsing succeeds, result should be valid\n            assert hasattr(result, 'type') or result is None\n        except (ValueError, TypeError, KeyError) as e:\n            # Expected for invalid GeoJSON\n            pass\n</code></pre>"},{"location":"day04_testing/#performance-and-load-testing-framework","title":"Performance and Load Testing Framework","text":"<pre><code>class SpatialLoadTestFramework:\n    \"\"\"Framework for load testing spatial services.\"\"\"\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url\n        self.session = httpx.AsyncClient()\n        self.metrics = {\n            'requests_sent': 0,\n            'requests_succeeded': 0,\n            'requests_failed': 0,\n            'total_latency': 0,\n            'min_latency': float('inf'),\n            'max_latency': 0,\n            'latencies': []\n        }\n\n    async def generate_spatial_workload(\n        self,\n        concurrent_users: int = 50,\n        requests_per_user: int = 100,\n        test_duration_seconds: int = 300\n    ):\n        \"\"\"Generate realistic spatial query workload.\"\"\"\n\n        async def user_session(user_id: int):\n            \"\"\"Simulate individual user behavior.\"\"\"\n            requests_made = 0\n            start_time = time.time()\n\n            while (requests_made &lt; requests_per_user and \n                   time.time() - start_time &lt; test_duration_seconds):\n\n                # Generate realistic spatial query\n                bbox = self._generate_realistic_bbox()\n\n                request_start = time.time()\n                try:\n                    response = await self.session.get(\n                        f\"{self.base_url}/api/features/bbox\",\n                        params={\n                            'min_lat': bbox[0], 'min_lon': bbox[1],\n                            'max_lat': bbox[2], 'max_lon': bbox[3]\n                        },\n                        timeout=30.0\n                    )\n\n                    latency = time.time() - request_start\n                    self._record_success(latency)\n\n                    if response.status_code != 200:\n                        self._record_failure(f\"HTTP {response.status_code}\")\n\n                except Exception as e:\n                    self._record_failure(str(e))\n\n                requests_made += 1\n\n                # Realistic user think time\n                await asyncio.sleep(random.uniform(0.1, 2.0))\n\n        # Run concurrent user sessions\n        tasks = [\n            asyncio.create_task(user_session(i)) \n            for i in range(concurrent_users)\n        ]\n\n        await asyncio.gather(*tasks, return_exceptions=True)\n        return self._generate_report()\n\n    def _generate_realistic_bbox(self) -&gt; Tuple[float, float, float, float]:\n        \"\"\"Generate realistic bounding boxes based on typical usage patterns.\"\"\"\n        # Focus on populated areas with varying zoom levels\n        city_centers = [\n            (37.7749, -122.4194),  # San Francisco\n            (40.7128, -74.0060),   # New York\n            (51.5074, -0.1278),    # London\n            (35.6762, 139.6503),   # Tokyo\n        ]\n\n        center = random.choice(city_centers)\n\n        # Random zoom level affects bbox size\n        zoom_level = random.randint(10, 18)\n        size = 1.0 / (2 ** (zoom_level - 10))  # Smaller at higher zoom\n\n        return (\n            center[0] - size,\n            center[1] - size,\n            center[0] + size,\n            center[1] + size\n        )\n\n@pytest.mark.asyncio\n@pytest.mark.load_test\nasync def test_spatial_api_under_load():\n    \"\"\"Load test for spatial API endpoints.\"\"\"\n    framework = SpatialLoadTestFramework(\"http://localhost:8000\")\n\n    report = await framework.generate_spatial_workload(\n        concurrent_users=25,\n        requests_per_user=50,\n        test_duration_seconds=60\n    )\n\n    # Performance assertions\n    assert report['success_rate'] &gt; 0.95  # 95% success rate\n    assert report['p95_latency'] &lt; 2.0    # 95th percentile under 2 seconds\n    assert report['avg_latency'] &lt; 0.5    # Average under 500ms\n</code></pre>"},{"location":"day04_testing/#cicd-integration-patterns","title":"CI/CD Integration Patterns","text":"<pre><code># pytest.ini configuration for geospatial testing\n\"\"\"\n[tool:pytest]\nminversion = 6.0\naddopts = \n    -ra \n    -q \n    --strict-markers \n    --strict-config\n    --cov=src \n    --cov-branch \n    --cov-report=term-missing \n    --cov-report=html\n    --cov-fail-under=80\n    --hypothesis-show-statistics\nmarkers =\n    unit: Unit tests\n    integration: Integration tests\n    load_test: Load and performance tests\n    chaos: Chaos engineering tests\n    slow: Tests that take more than 5 seconds\n    spatial: Tests that require spatial data/operations\n    external: Tests that require external services\ntestpaths = tests\npython_files = test_*.py\npython_classes = Test*\npython_functions = test_*\nfilterwarnings =\n    error\n    ignore::UserWarning\n    ignore::DeprecationWarning\n\"\"\"\n\nclass GeospatialTestConfig:\n    \"\"\"Configuration for geospatial test environments.\"\"\"\n\n    @staticmethod\n    def setup_test_database():\n        \"\"\"Set up PostGIS test database.\"\"\"\n        import psycopg2\n        from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n\n        # Create test database with PostGIS\n        conn = psycopg2.connect(\n            host=\"localhost\",\n            user=\"postgres\", \n            password=\"password\"\n        )\n        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n        cursor = conn.cursor()\n\n        cursor.execute(\"DROP DATABASE IF EXISTS test_geospatial\")\n        cursor.execute(\"CREATE DATABASE test_geospatial\")\n\n        # Connect to test database and enable PostGIS\n        test_conn = psycopg2.connect(\n            host=\"localhost\",\n            user=\"postgres\",\n            password=\"password\",\n            database=\"test_geospatial\"\n        )\n        test_cursor = test_conn.cursor()\n        test_cursor.execute(\"CREATE EXTENSION IF NOT EXISTS postgis\")\n        test_conn.commit()\n\n        return \"postgresql://postgres:password@localhost/test_geospatial\"\n\n    @staticmethod\n    def setup_test_data():\n        \"\"\"Generate test spatial datasets.\"\"\"\n        import geopandas as gpd\n        from shapely.geometry import Point, Polygon\n\n        # Generate synthetic spatial data\n        test_features = []\n        for i in range(1000):\n            # Random points around San Francisco\n            lat = 37.7749 + random.uniform(-0.1, 0.1)\n            lon = -122.4194 + random.uniform(-0.1, 0.1)\n\n            feature = {\n                'id': f'feature_{i}',\n                'geometry': Point(lon, lat),\n                'properties': {\n                    'name': f'Test Feature {i}',\n                    'category': random.choice(['restaurant', 'shop', 'park']),\n                    'rating': random.uniform(1.0, 5.0)\n                }\n            }\n            test_features.append(feature)\n\n        # Create GeoDataFrame and save as test file\n        gdf = gpd.GeoDataFrame(test_features)\n        gdf.to_file('test_data/synthetic_features.geojson', driver='GeoJSON')\n\n        return gdf\n\n# GitHub Actions workflow for geospatial testing\n\"\"\"\nname: Geospatial Testing Pipeline\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    services:\n      postgres:\n        image: postgis/postgis:13-3.1\n        env:\n          POSTGRES_PASSWORD: password\n          POSTGRES_DB: test_geospatial\n        options: &gt;-\n          --health-cmd pg_isready\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n          - 5432:5432\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install GDAL\n      run: |\n        sudo apt-get update\n        sudo apt-get install -y gdal-bin libgdal-dev\n\n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install -r requirements-test.txt\n\n    - name: Run unit tests\n      run: pytest tests/ -m \"unit\" --cov=src --cov-report=xml\n\n    - name: Run integration tests\n      run: pytest tests/ -m \"integration\" \n      env:\n        DATABASE_URL: postgresql://postgres:password@localhost:5432/test_geospatial\n\n    - name: Run spatial accuracy tests\n      run: pytest tests/ -m \"spatial\" --hypothesis-show-statistics\n\n    - name: Upload coverage reports\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n        flags: unittests\n        name: codecov-umbrella\n\"\"\"\n</code></pre>"},{"location":"day04_testing/#data-driven-testing-for-geospatial-systems","title":"Data-Driven Testing for Geospatial Systems","text":""},{"location":"day04_testing/#test-data-management","title":"Test Data Management","text":"<pre><code>class SpatialTestDataManager:\n    \"\"\"Manages test datasets for geospatial testing.\"\"\"\n\n    def __init__(self, data_dir: Path):\n        self.data_dir = Path(data_dir)\n        self.datasets = {}\n\n    def register_dataset(self, name: str, generator_func: Callable):\n        \"\"\"Register a test dataset generator.\"\"\"\n        self.datasets[name] = generator_func\n\n    def get_dataset(self, name: str, **kwargs) -&gt; Any:\n        \"\"\"Get or generate a test dataset.\"\"\"\n        cache_key = f\"{name}_{hash(str(kwargs))}\"\n        cache_file = self.data_dir / f\"{cache_key}.pkl\"\n\n        if cache_file.exists():\n            with open(cache_file, 'rb') as f:\n                return pickle.load(f)\n\n        # Generate dataset\n        dataset = self.datasets[name](**kwargs)\n\n        # Cache for future use\n        cache_file.parent.mkdir(parents=True, exist_ok=True)\n        with open(cache_file, 'wb') as f:\n            pickle.dump(dataset, f)\n\n        return dataset\n\n# Test data generators\ndef generate_road_network(num_roads: int = 100, area_bounds: Tuple = None):\n    \"\"\"Generate synthetic road network for testing.\"\"\"\n    if area_bounds is None:\n        area_bounds = (37.7, -122.5, 37.8, -122.4)  # SF area\n\n    roads = []\n    for i in range(num_roads):\n        # Generate random road segments\n        start_lat = random.uniform(area_bounds[0], area_bounds[2])\n        start_lon = random.uniform(area_bounds[1], area_bounds[3])\n\n        # Random direction and length\n        bearing = random.uniform(0, 360)\n        length = random.uniform(0.001, 0.01)  # degrees\n\n        end_lat = start_lat + length * math.cos(math.radians(bearing))\n        end_lon = start_lon + length * math.sin(math.radians(bearing))\n\n        road = {\n            'id': f'road_{i}',\n            'geometry': f'LINESTRING({start_lon} {start_lat}, {end_lon} {end_lat})',\n            'properties': {\n                'name': f'Test Road {i}',\n                'speed_limit': random.choice([25, 35, 45, 55]),\n                'road_type': random.choice(['residential', 'arterial', 'highway'])\n            }\n        }\n        roads.append(road)\n\n    return roads\n\n# Usage in tests\n@pytest.fixture(scope=\"session\")\ndef spatial_test_data():\n    manager = SpatialTestDataManager(Path(\"test_data\"))\n    manager.register_dataset(\"road_network\", generate_road_network)\n    return manager\n\nclass TestWithSpatialData:\n    def test_road_network_loading(self, spatial_test_data):\n        \"\"\"Test road network loading with different dataset sizes.\"\"\"\n        small_network = spatial_test_data.get_dataset(\"road_network\", num_roads=10)\n        large_network = spatial_test_data.get_dataset(\"road_network\", num_roads=1000)\n\n        assert len(small_network) == 10\n        assert len(large_network) == 1000\n</code></pre>"},{"location":"day04_testing/#resources","title":"Resources","text":""},{"location":"day04_testing/#testing-frameworks-and-tools","title":"Testing Frameworks and Tools","text":"<ul> <li>pytest Documentation</li> <li>pytest-cov Coverage Plugin</li> <li>Hypothesis Property-Based Testing</li> <li>mutmut Mutation Testing</li> <li>locust Load Testing</li> </ul>"},{"location":"day04_testing/#geospatial-testing","title":"Geospatial Testing","text":"<ul> <li>PostGIS Testing Guide</li> <li>GDAL/OGR Test Suite</li> <li>Shapely Testing Patterns</li> </ul>"},{"location":"day04_testing/#quality-engineering","title":"Quality Engineering","text":"<ul> <li>FastAPI Testing Guide</li> <li>Test-Driven Development by Example</li> <li>Growing Object-Oriented Software, Guided by Tests</li> <li>Building Quality Software by NASA</li> </ul>"},{"location":"day04_testing/#chaos-engineering","title":"Chaos Engineering","text":"<ul> <li>Principles of Chaos Engineering</li> <li>Chaos Monkey for Spatial Systems</li> </ul> <p>This comprehensive testing module ensures that geospatial systems are robust, reliable, and performant under all conditions, from normal operation to extreme failure scenarios.</p>"},{"location":"day05_data/","title":"Day 05 - Data","text":""},{"location":"day05_data/#learning-objectives","title":"Learning Objectives","text":"<p>This module covers the sophisticated data processing techniques essential for enterprise geospatial systems, focusing on performance optimization, data serialization, and spatial indexing strategies used in production mapping and location intelligence platforms. By the end of this module, you will understand:</p> <ul> <li>Binary serialization formats including Protocol Buffers, FlatBuffers, and Avro for high-performance geospatial data exchange</li> <li>Coordinate reference systems (CRS) and transformation algorithms for global interoperability</li> <li>Spatial indexing algorithms including R-tree, Quadtree, H3, and S2 for efficient spatial queries</li> <li>Computational geometry algorithms for spatial operations, topology validation, and geometric transformations</li> <li>Stream processing patterns for real-time geospatial data pipelines and change detection</li> <li>Memory optimization techniques for processing large-scale geospatial datasets</li> <li>Data compression and encoding strategies for geospatial storage and transmission</li> </ul>"},{"location":"day05_data/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"day05_data/#geospatial-data-complexity","title":"Geospatial Data Complexity","text":"<p>Geometric Primitives and Topology: Modern geospatial systems must handle complex geometric relationships: - Simple features: Points, LineStrings, Polygons with well-defined topology - Complex geometries: MultiPolygons, GeometryCollections with potential self-intersections - 3D and temporal geometries: Z-coordinates, time-series spatial data, 4D spatiotemporal objects - Topology preservation: Ensuring geometric validity through transformations and operations</p> <p>Scale and Precision Challenges: - Multi-scale representation: From millimeter precision to global datasets - Floating-point arithmetic: Precision loss and numerical stability in coordinate transformations - Datum transformations: Accurate conversion between different geodetic reference systems - Coordinate order conventions: lon/lat vs lat/lon and their implications for data integrity</p>"},{"location":"day05_data/#data-serialization-for-geospatial-systems","title":"Data Serialization for Geospatial Systems","text":"<p>Protocol Buffers vs Traditional Formats:</p> Aspect Protocol Buffers GeoJSON Shapefile PostGIS Binary Size 60-80% smaller Baseline 40% smaller 70% smaller Parse Speed 3-10x faster Baseline 2x faster 5x faster Schema Evolution Excellent None Limited Good Language Support Excellent Universal Limited Limited Human Readable No Yes No No <p>When to Choose Each Format: - Protocol Buffers: High-volume APIs, microservices communication, performance-critical applications - GeoJSON: Web applications, simple integrations, debugging and visualization - Shapefile: Legacy system integration, GIS software compatibility - PostGIS Binary: Database storage, spatial analysis, complex geometric operations</p>"},{"location":"day05_data/#spatial-indexing-algorithms","title":"Spatial Indexing Algorithms","text":"<p>R-tree Family: - R-tree: Bounding box hierarchies for 2D spatial data - R*-tree: Optimized node splitting for better query performance - R+-tree: Non-overlapping rectangles for reduced false positives - Hilbert R-tree: Space-filling curves for improved clustering</p> <p>Grid-Based Indexing: - Quadtree: Recursive spatial subdivision for adaptive resolution - Geohash: Base-32 encoding of geographic coordinates - H3: Uber's hexagonal hierarchical spatial index - S2: Google's spherical geometry library for global indexing</p> <p>Performance Characteristics:</p> Index Type Insert O() Query O() Memory Use Case R-tree O(log n) O(log n + k) High Complex geometries Quadtree O(log n) O(log n + k) Medium Point data Geohash O(1) O(k) Low Web applications H3 O(1) O(k) Low Uber-scale analytics S2 O(1) O(k) Low Global applications"},{"location":"day05_data/#code-architecture-and-implementation","title":"Code Architecture and Implementation","text":""},{"location":"day05_data/#protocol-buffers-for-geospatial-data","title":"Protocol Buffers for Geospatial Data","text":"<pre><code>// src/day05_data/protobuf/road_segment.proto\nsyntax = \"proto3\";\n\npackage geospatial.roads;\n\n// Coordinate system information\nmessage CoordinateReferenceSystem {\n  int32 epsg_code = 1;\n  string wkt_definition = 2;\n  string authority = 3;\n}\n\n// High-precision coordinate with optional elevation and time\nmessage Coordinate {\n  double longitude = 1;\n  double latitude = 2;\n  optional double elevation = 3;\n  optional int64 timestamp = 4;  // Unix timestamp in microseconds\n}\n\n// Geometry types following OGC Simple Features\nmessage Geometry {\n  enum GeometryType {\n    POINT = 0;\n    LINESTRING = 1;\n    POLYGON = 2;\n    MULTIPOINT = 3;\n    MULTILINESTRING = 4;\n    MULTIPOLYGON = 5;\n    GEOMETRYCOLLECTION = 6;\n  }\n\n  GeometryType type = 1;\n  repeated Coordinate coordinates = 2;\n  repeated Geometry children = 3;  // For collections\n  optional CoordinateReferenceSystem crs = 4;\n}\n\n// Road segment with rich metadata\nmessage RoadSegment {\n  string id = 1;\n  string name = 2;\n  Geometry geometry = 3;\n\n  // Traffic and routing information\n  int32 speed_limit_kmh = 4;\n  RoadType road_type = 5;\n  Direction direction = 6;\n\n  // Connectivity information\n  repeated string connected_segment_ids = 7;\n  repeated Junction junctions = 8;\n\n  // Temporal information\n  int64 last_updated = 9;\n  int64 created_at = 10;\n\n  // Quality metrics\n  float accuracy_meters = 11;\n  int32 confidence_score = 12;  // 0-100\n\n  // Extended attributes (key-value pairs for flexibility)\n  map&lt;string, string&gt; attributes = 13;\n}\n\nenum RoadType {\n  UNKNOWN = 0;\n  HIGHWAY = 1;\n  ARTERIAL = 2;\n  COLLECTOR = 3;\n  RESIDENTIAL = 4;\n  SERVICE = 5;\n  PARKING = 6;\n  PEDESTRIAN = 7;\n  CYCLEWAY = 8;\n}\n\nenum Direction {\n  BIDIRECTIONAL = 0;\n  FORWARD = 1;\n  BACKWARD = 2;\n  CLOSED = 3;\n}\n\nmessage Junction {\n  string id = 1;\n  Coordinate location = 2;\n  JunctionType type = 3;\n  repeated string traffic_signals = 4;\n}\n\nenum JunctionType {\n  INTERSECTION = 0;\n  ROUNDABOUT = 1;\n  FORK = 2;\n  MERGE = 3;\n  OVERPASS = 4;\n  UNDERPASS = 5;\n}\n</code></pre>"},{"location":"day05_data/#advanced-spatial-indexing-implementation","title":"Advanced Spatial Indexing Implementation","text":"<pre><code>class HierarchicalSpatialIndex:\n    \"\"\"Multi-level spatial index combining multiple algorithms.\"\"\"\n\n    def __init__(self, bbox_bounds: Tuple[float, float, float, float]):\n        self.global_bounds = bbox_bounds\n\n        # Level 1: Coarse grid for global partitioning\n        self.grid_index = GeohashIndex(precision=6)\n\n        # Level 2: R-tree for medium-scale queries\n        self.rtree_indices = {}\n\n        # Level 3: Fine-grained quadtrees for high-resolution queries\n        self.quadtree_indices = {}\n\n        # Statistics for query optimization\n        self.query_stats = {\n            'grid_hits': 0,\n            'rtree_hits': 0,\n            'quadtree_hits': 0,\n            'total_queries': 0\n        }\n\n    def insert(self, feature_id: str, geometry: Geometry, properties: Dict):\n        \"\"\"Insert feature into hierarchical index.\"\"\"\n        bbox = geometry.bounds\n        centroid = geometry.centroid\n\n        # Level 1: Add to geohash grid\n        geohash = self.grid_index.encode(centroid.y, centroid.x)\n\n        # Level 2: Add to appropriate R-tree\n        if geohash not in self.rtree_indices:\n            self.rtree_indices[geohash] = rtree.index.Index()\n\n        self.rtree_indices[geohash].insert(\n            id(feature_id), bbox, obj=(feature_id, geometry, properties)\n        )\n\n        # Level 3: Add to quadtree for complex geometries\n        if geometry.area &gt; self._get_quadtree_threshold(bbox):\n            if geohash not in self.quadtree_indices:\n                grid_bounds = self.grid_index.decode_bbox(geohash)\n                self.quadtree_indices[geohash] = QuadTree(grid_bounds)\n\n            self.quadtree_indices[geohash].insert(feature_id, geometry)\n\n    def query_bbox(self, min_x: float, min_y: float, \n                   max_x: float, max_y: float) -&gt; List[str]:\n        \"\"\"Efficient bounding box query using hierarchical approach.\"\"\"\n        self.query_stats['total_queries'] += 1\n\n        # Determine optimal query strategy based on bbox size\n        bbox_area = (max_x - min_x) * (max_y - min_y)\n\n        if bbox_area &gt; 0.1:  # Large area - use grid index\n            return self._query_large_bbox(min_x, min_y, max_x, max_y)\n        elif bbox_area &gt; 0.001:  # Medium area - use R-tree\n            return self._query_medium_bbox(min_x, min_y, max_x, max_y)\n        else:  # Small area - use quadtree\n            return self._query_small_bbox(min_x, min_y, max_x, max_y)\n\n    def _query_large_bbox(self, min_x: float, min_y: float, \n                         max_x: float, max_y: float) -&gt; List[str]:\n        \"\"\"Query large bounding boxes using grid index.\"\"\"\n        self.query_stats['grid_hits'] += 1\n\n        # Get all geohash cells that intersect the bbox\n        intersecting_geohashes = self.grid_index.get_intersecting_cells(\n            min_x, min_y, max_x, max_y\n        )\n\n        results = []\n        for geohash in intersecting_geohashes:\n            if geohash in self.rtree_indices:\n                rtree_results = list(self.rtree_indices[geohash].intersection(\n                    (min_x, min_y, max_x, max_y), objects=True\n                ))\n                results.extend([r.object[0] for r in rtree_results])\n\n        return results\n\n    def _query_medium_bbox(self, min_x: float, min_y: float, \n                          max_x: float, max_y: float) -&gt; List[str]:\n        \"\"\"Query medium bounding boxes using R-tree.\"\"\"\n        self.query_stats['rtree_hits'] += 1\n\n        # Find relevant geohash cells\n        center_x, center_y = (min_x + max_x) / 2, (min_y + max_y) / 2\n        primary_geohash = self.grid_index.encode(center_y, center_x)\n\n        # Get neighboring cells for edge cases\n        candidate_geohashes = self.grid_index.get_neighbors(primary_geohash)\n        candidate_geohashes.append(primary_geohash)\n\n        results = []\n        for geohash in candidate_geohashes:\n            if geohash in self.rtree_indices:\n                rtree_results = list(self.rtree_indices[geohash].intersection(\n                    (min_x, min_y, max_x, max_y), objects=True\n                ))\n\n                # Perform exact geometry intersection\n                for result in rtree_results:\n                    feature_id, geometry, _ = result.object\n                    if geometry.intersects(box(min_x, min_y, max_x, max_y)):\n                        results.append(feature_id)\n\n        return results\n\nclass SpatialAnalysisEngine:\n    \"\"\"Advanced spatial analysis operations.\"\"\"\n\n    def __init__(self, spatial_index: HierarchicalSpatialIndex):\n        self.index = spatial_index\n        self.crs_transformer = CRSTransformer()\n\n    def buffer_analysis(self, geometry: Geometry, distance_meters: float,\n                       target_crs: int = 3857) -&gt; Geometry:\n        \"\"\"Perform accurate buffer analysis in projected coordinates.\"\"\"\n        # Transform to appropriate projected CRS for accurate distance calculation\n        if geometry.crs != target_crs:\n            transformed_geom = self.crs_transformer.transform(\n                geometry, source_crs=geometry.crs, target_crs=target_crs\n            )\n        else:\n            transformed_geom = geometry\n\n        # Perform buffer operation\n        buffered = transformed_geom.buffer(distance_meters)\n\n        # Transform back to original CRS if needed\n        if geometry.crs != target_crs:\n            buffered = self.crs_transformer.transform(\n                buffered, source_crs=target_crs, target_crs=geometry.crs\n            )\n\n        return buffered\n\n    def network_analysis(self, road_segments: List[RoadSegment]) -&gt; NetworkGraph:\n        \"\"\"Build routable network graph from road segments.\"\"\"\n        graph = NetworkGraph()\n\n        # Build topology\n        for segment in road_segments:\n            start_coord = segment.geometry.coords[0]\n            end_coord = segment.geometry.coords[-1]\n\n            start_node = graph.add_node(start_coord)\n            end_node = graph.add_node(end_coord)\n\n            # Calculate edge weight (travel time)\n            length_meters = self._calculate_length(segment.geometry)\n            speed_ms = segment.speed_limit_kmh / 3.6  # Convert to m/s\n            travel_time = length_meters / speed_ms\n\n            # Add edge with directionality\n            if segment.direction in [Direction.BIDIRECTIONAL, Direction.FORWARD]:\n                graph.add_edge(start_node, end_node, weight=travel_time, segment=segment)\n\n            if segment.direction in [Direction.BIDIRECTIONAL, Direction.BACKWARD]:\n                graph.add_edge(end_node, start_node, weight=travel_time, segment=segment)\n\n        return graph\n\n    def change_detection(self, old_features: List[Feature], \n                        new_features: List[Feature]) -&gt; ChangeSet:\n        \"\"\"Detect changes between two feature sets.\"\"\"\n        changes = ChangeSet()\n\n        # Build spatial indices for both datasets\n        old_index = self._build_spatial_index(old_features)\n        new_index = self._build_spatial_index(new_features)\n\n        # Detect additions\n        for new_feature in new_features:\n            if not self._find_matching_feature(new_feature, old_index):\n                changes.additions.append(new_feature)\n\n        # Detect deletions and modifications\n        for old_feature in old_features:\n            matching_feature = self._find_matching_feature(old_feature, new_index)\n            if matching_feature is None:\n                changes.deletions.append(old_feature)\n            elif not self._features_equal(old_feature, matching_feature):\n                changes.modifications.append((old_feature, matching_feature))\n\n        return changes\n</code></pre>"},{"location":"day05_data/#performance-optimization-patterns","title":"Performance Optimization Patterns","text":"<pre><code>class GeospatialMemoryOptimizer:\n    \"\"\"Memory optimization for large-scale geospatial processing.\"\"\"\n\n    def __init__(self, memory_limit_gb: float = 4.0):\n        self.memory_limit_bytes = memory_limit_gb * 1024 * 1024 * 1024\n        self.current_memory_usage = 0\n        self.geometry_cache = {}\n        self.compression_enabled = True\n\n    def process_large_dataset(self, dataset_path: str, \n                            processing_func: Callable,\n                            chunk_size: int = 10000) -&gt; Iterator[Any]:\n        \"\"\"Process large datasets in memory-efficient chunks.\"\"\"\n\n        with self._open_dataset(dataset_path) as dataset:\n            total_features = len(dataset)\n\n            for chunk_start in range(0, total_features, chunk_size):\n                chunk_end = min(chunk_start + chunk_size, total_features)\n\n                # Load chunk into memory\n                chunk = self._load_chunk(dataset, chunk_start, chunk_end)\n\n                # Monitor memory usage\n                chunk_memory = sys.getsizeof(chunk)\n                if self.current_memory_usage + chunk_memory &gt; self.memory_limit_bytes:\n                    self._cleanup_cache()\n\n                self.current_memory_usage += chunk_memory\n\n                try:\n                    # Process chunk\n                    results = processing_func(chunk)\n                    yield results\n\n                finally:\n                    # Clean up chunk memory\n                    del chunk\n                    self.current_memory_usage -= chunk_memory\n                    gc.collect()\n\n    def optimize_geometry_storage(self, geometries: List[Geometry]) -&gt; List[bytes]:\n        \"\"\"Compress geometries for efficient storage.\"\"\"\n        if not self.compression_enabled:\n            return [geom.wkb for geom in geometries]\n\n        optimized_geometries = []\n        for geom in geometries:\n            # Simplify geometry based on scale\n            simplified = self._adaptive_simplification(geom)\n\n            # Compress using Protocol Buffers\n            pb_geom = self._to_protobuf(simplified)\n            compressed = gzip.compress(pb_geom.SerializeToString())\n\n            optimized_geometries.append(compressed)\n\n        return optimized_geometries\n\n    def _adaptive_simplification(self, geometry: Geometry) -&gt; Geometry:\n        \"\"\"Simplify geometry based on its characteristics.\"\"\"\n        if geometry.geom_type == 'Point':\n            return geometry  # Points can't be simplified\n\n        # Calculate appropriate tolerance based on geometry bounds\n        bounds = geometry.bounds\n        diagonal = ((bounds[2] - bounds[0])**2 + (bounds[3] - bounds[1])**2)**0.5\n        tolerance = diagonal * 0.001  # 0.1% of diagonal\n\n        # Apply Douglas-Peucker simplification\n        simplified = geometry.simplify(tolerance, preserve_topology=True)\n\n        # Ensure simplified geometry is valid\n        if not simplified.is_valid:\n            simplified = simplified.buffer(0)  # Fix self-intersections\n\n        return simplified\n\nclass StreamingGeospatialProcessor:\n    \"\"\"Stream processing for real-time geospatial data.\"\"\"\n\n    def __init__(self):\n        self.processors = []\n        self.output_streams = []\n        self.metrics = {\n            'features_processed': 0,\n            'processing_errors': 0,\n            'average_latency_ms': 0\n        }\n\n    async def process_stream(self, input_stream: AsyncIterator[Feature]):\n        \"\"\"Process streaming geospatial data.\"\"\"\n        async for feature in input_stream:\n            start_time = time.time()\n\n            try:\n                # Apply processing pipeline\n                processed_feature = await self._apply_processors(feature)\n\n                # Send to output streams\n                await self._send_to_outputs(processed_feature)\n\n                # Update metrics\n                processing_time = (time.time() - start_time) * 1000\n                self._update_metrics(processing_time)\n\n            except Exception as e:\n                self.metrics['processing_errors'] += 1\n                logger.error(f\"Error processing feature {feature.id}: {e}\")\n\n    def add_processor(self, processor: 'StreamProcessor'):\n        \"\"\"Add a processing step to the pipeline.\"\"\"\n        self.processors.append(processor)\n\n    async def _apply_processors(self, feature: Feature) -&gt; Feature:\n        \"\"\"Apply all processors in sequence.\"\"\"\n        current_feature = feature\n\n        for processor in self.processors:\n            current_feature = await processor.process(current_feature)\n\n            if current_feature is None:\n                break  # Processor filtered out the feature\n\n        return current_feature\n\nclass GeospatialETLPipeline:\n    \"\"\"ETL pipeline for geospatial data integration.\"\"\"\n\n    def __init__(self):\n        self.extractors = {}\n        self.transformers = []\n        self.loaders = {}\n        self.data_quality_rules = []\n\n    def extract_from_source(self, source_type: str, source_config: Dict) -&gt; Iterator[Feature]:\n        \"\"\"Extract data from various geospatial sources.\"\"\"\n        if source_type not in self.extractors:\n            raise ValueError(f\"Unknown source type: {source_type}\")\n\n        extractor = self.extractors[source_type]\n        return extractor.extract(source_config)\n\n    def transform_data(self, features: Iterator[Feature]) -&gt; Iterator[Feature]:\n        \"\"\"Apply transformation pipeline to features.\"\"\"\n        for feature in features:\n            transformed_feature = feature\n\n            # Apply transformations in sequence\n            for transformer in self.transformers:\n                transformed_feature = transformer.transform(transformed_feature)\n\n                if transformed_feature is None:\n                    break  # Feature was filtered out\n\n            # Apply data quality rules\n            if transformed_feature and self._validate_quality(transformed_feature):\n                yield transformed_feature\n\n    def load_to_destination(self, features: Iterator[Feature], \n                           destination_type: str, destination_config: Dict):\n        \"\"\"Load transformed features to destination.\"\"\"\n        if destination_type not in self.loaders:\n            raise ValueError(f\"Unknown destination type: {destination_type}\")\n\n        loader = self.loaders[destination_type]\n        loader.load(features, destination_config)\n\n    def _validate_quality(self, feature: Feature) -&gt; bool:\n        \"\"\"Validate feature against data quality rules.\"\"\"\n        for rule in self.data_quality_rules:\n            if not rule.validate(feature):\n                logger.warning(f\"Feature {feature.id} failed quality rule: {rule.name}\")\n                return False\n\n        return True\n</code></pre>"},{"location":"day05_data/#performance-benchmarking-framework","title":"Performance Benchmarking Framework","text":""},{"location":"day05_data/#serialization-performance-analysis","title":"Serialization Performance Analysis","text":"<pre><code>class SerializationBenchmark:\n    \"\"\"Benchmark different serialization formats for geospatial data.\"\"\"\n\n    def __init__(self):\n        self.test_datasets = {\n            'points_1k': self._generate_points(1000),\n            'points_100k': self._generate_points(100000),\n            'roads_sf': self._load_sf_roads(),\n            'complex_polygons': self._generate_complex_polygons(1000)\n        }\n\n    def run_benchmark(self) -&gt; Dict[str, Dict[str, float]]:\n        \"\"\"Run comprehensive serialization benchmark.\"\"\"\n        results = {}\n\n        for dataset_name, features in self.test_datasets.items():\n            results[dataset_name] = {\n                'geojson': self._benchmark_geojson(features),\n                'protobuf': self._benchmark_protobuf(features),\n                'wkb': self._benchmark_wkb(features),\n                'flatbuffers': self._benchmark_flatbuffers(features)\n            }\n\n        return results\n\n    def _benchmark_geojson(self, features: List[Feature]) -&gt; Dict[str, float]:\n        \"\"\"Benchmark GeoJSON serialization.\"\"\"\n        # Serialize\n        start_time = time.time()\n        serialized = json.dumps({\n            'type': 'FeatureCollection',\n            'features': [f.__geo_interface__ for f in features]\n        })\n        serialize_time = time.time() - start_time\n\n        # Deserialize\n        start_time = time.time()\n        data = json.loads(serialized)\n        features_loaded = [Feature.from_dict(f) for f in data['features']]\n        deserialize_time = time.time() - start_time\n\n        return {\n            'serialize_time': serialize_time,\n            'deserialize_time': deserialize_time,\n            'size_bytes': len(serialized.encode('utf-8')),\n            'features_count': len(features_loaded)\n        }\n\n    def _benchmark_protobuf(self, features: List[Feature]) -&gt; Dict[str, float]:\n        \"\"\"Benchmark Protocol Buffers serialization.\"\"\"\n        # Convert to protobuf\n        start_time = time.time()\n        pb_features = []\n        for feature in features:\n            pb_feature = self._to_protobuf_feature(feature)\n            pb_features.append(pb_feature)\n\n        # Serialize collection\n        collection = FeatureCollection()\n        collection.features.extend(pb_features)\n        serialized = collection.SerializeToString()\n        serialize_time = time.time() - start_time\n\n        # Deserialize\n        start_time = time.time()\n        loaded_collection = FeatureCollection()\n        loaded_collection.ParseFromString(serialized)\n        features_loaded = [\n            self._from_protobuf_feature(f) for f in loaded_collection.features\n        ]\n        deserialize_time = time.time() - start_time\n\n        return {\n            'serialize_time': serialize_time,\n            'deserialize_time': deserialize_time,\n            'size_bytes': len(serialized),\n            'features_count': len(features_loaded)\n        }\n\nclass SpatialIndexBenchmark:\n    \"\"\"Benchmark spatial index performance.\"\"\"\n\n    def __init__(self):\n        self.test_data_sizes = [1000, 10000, 100000, 1000000]\n        self.query_sizes = [\n            (0.001, 0.001),  # Small area\n            (0.01, 0.01),    # Medium area  \n            (0.1, 0.1),      # Large area\n        ]\n\n    def benchmark_index_types(self) -&gt; pd.DataFrame:\n        \"\"\"Compare different spatial index implementations.\"\"\"\n        results = []\n\n        for data_size in self.test_data_sizes:\n            test_features = self._generate_test_features(data_size)\n\n            for index_type in ['rtree', 'quadtree', 'geohash', 'h3']:\n                # Build index\n                build_start = time.time()\n                index = self._build_index(index_type, test_features)\n                build_time = time.time() - build_start\n\n                # Test queries\n                for query_width, query_height in self.query_sizes:\n                    query_times = []\n\n                    # Run multiple queries for statistical significance\n                    for _ in range(100):\n                        bbox = self._generate_random_bbox(query_width, query_height)\n\n                        query_start = time.time()\n                        results_count = len(list(index.query_bbox(*bbox)))\n                        query_time = time.time() - query_start\n\n                        query_times.append(query_time)\n\n                    avg_query_time = sum(query_times) / len(query_times)\n\n                    results.append({\n                        'data_size': data_size,\n                        'index_type': index_type,\n                        'query_area': query_width * query_height,\n                        'build_time': build_time,\n                        'avg_query_time_ms': avg_query_time * 1000,\n                        'memory_mb': self._estimate_memory_usage(index)\n                    })\n\n        return pd.DataFrame(results)\n</code></pre>"},{"location":"day05_data/#real-world-integration-examples","title":"Real-World Integration Examples","text":""},{"location":"day05_data/#postgis-integration-with-streaming","title":"PostGIS Integration with Streaming","text":"<pre><code>class PostGISStreamingProcessor:\n    \"\"\"High-performance PostGIS integration with streaming support.\"\"\"\n\n    def __init__(self, connection_pool: asyncpg.Pool):\n        self.pool = connection_pool\n        self.prepared_statements = {}\n\n    async def stream_large_query(self, query: str, parameters: List = None,\n                                chunk_size: int = 10000) -&gt; AsyncIterator[Dict]:\n        \"\"\"Stream large spatial query results to avoid memory issues.\"\"\"\n\n        async with self.pool.acquire() as connection:\n            # Use server-side cursor for large datasets\n            async with connection.transaction():\n                await connection.execute(f\"DECLARE large_cursor CURSOR FOR {query}\", \n                                       *(parameters or []))\n\n                while True:\n                    rows = await connection.fetch(\n                        f\"FETCH {chunk_size} FROM large_cursor\"\n                    )\n\n                    if not rows:\n                        break\n\n                    for row in rows:\n                        # Convert PostGIS binary to geometry\n                        geom_wkb = row['geometry']\n                        geometry = wkb.loads(bytes(geom_wkb))\n\n                        yield {\n                            'id': row['id'],\n                            'geometry': geometry,\n                            'properties': dict(row._mapping)\n                        }\n\n    async def bulk_insert_optimized(self, features: List[Feature], \n                                   table_name: str):\n        \"\"\"Optimized bulk insert using COPY and prepared statements.\"\"\"\n\n        # Prepare data for COPY\n        copy_data = []\n        for feature in features:\n            geometry_wkt = feature.geometry.wkt\n            properties_json = json.dumps(feature.properties)\n\n            copy_data.append(f\"{feature.id}\\t{geometry_wkt}\\t{properties_json}\")\n\n        copy_text = '\\n'.join(copy_data)\n\n        async with self.pool.acquire() as connection:\n            # Use COPY for maximum performance\n            await connection.copy_to_table(\n                table_name,\n                source=copy_text,\n                columns=['id', 'geometry', 'properties'],\n                format='text'\n            )\n\nclass CloudStorageGeospatialProcessor:\n    \"\"\"Process geospatial data from cloud storage efficiently.\"\"\"\n\n    def __init__(self, storage_client):\n        self.storage = storage_client\n        self.processing_cache = {}\n\n    async def process_cog_tiles(self, bucket: str, prefix: str,\n                               processing_func: Callable) -&gt; List[ProcessingResult]:\n        \"\"\"Process Cloud Optimized GeoTIFF tiles efficiently.\"\"\"\n\n        # List all COG files in prefix\n        objects = await self.storage.list_objects(bucket, prefix)\n        cog_files = [obj for obj in objects if obj.key.endswith('.tif')]\n\n        # Process files in parallel with concurrency control\n        semaphore = asyncio.Semaphore(10)  # Limit concurrent downloads\n\n        async def process_single_cog(cog_object):\n            async with semaphore:\n                # Stream COG data without full download\n                with rasterio.open(f\"/vsis3/{bucket}/{cog_object.key}\") as dataset:\n                    # Read specific bands/windows as needed\n                    result = await processing_func(dataset)\n                    return ProcessingResult(cog_object.key, result)\n\n        tasks = [process_single_cog(cog) for cog in cog_files]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Filter out exceptions and return successful results\n        return [r for r in results if isinstance(r, ProcessingResult)]\n</code></pre>"},{"location":"day05_data/#professional-development-exercises","title":"Professional Development Exercises","text":""},{"location":"day05_data/#exercise-1-build-a-multi-format-data-converter","title":"Exercise 1: Build a Multi-Format Data Converter","text":"<p>Create a comprehensive geospatial data conversion pipeline: - Support input formats: Shapefile, GeoJSON, KML, GPX, PostGIS - Support output formats: Protocol Buffers, FlatBuffers, Parquet, COG - Implement coordinate system transformations - Add data validation and quality checks - Include performance benchmarking and optimization</p>"},{"location":"day05_data/#exercise-2-implement-advanced-spatial-operations","title":"Exercise 2: Implement Advanced Spatial Operations","text":"<p>Build a spatial analysis engine with: - Buffer analysis with accurate distance calculations - Spatial joins with multiple join predicates - Network analysis for routing and accessibility - Voronoi diagrams and Delaunay triangulation - Spatial clustering algorithms (DBSCAN, K-means)</p>"},{"location":"day05_data/#exercise-3-create-a-real-time-geospatial-stream-processor","title":"Exercise 3: Create a Real-Time Geospatial Stream Processor","text":"<p>Design a streaming system that: - Ingests real-time location data (GPS tracks, IoT sensors) - Performs real-time geofencing and proximity detection - Implements sliding window analytics - Handles late-arriving and out-of-order data - Provides exactly-once processing guarantees</p>"},{"location":"day05_data/#exercise-4-build-a-distributed-spatial-index","title":"Exercise 4: Build a Distributed Spatial Index","text":"<p>Implement a distributed spatial indexing system: - Partition spatial data across multiple nodes - Implement consistent hashing for spatial keys - Add replication and fault tolerance - Support distributed spatial queries - Include monitoring and auto-scaling capabilities</p>"},{"location":"day05_data/#industry-context-and-applications","title":"Industry Context and Applications","text":""},{"location":"day05_data/#production-mapping-services","title":"Production Mapping Services","text":"<ul> <li>Google Maps: Distributed spatial indexing with S2 geometry</li> <li>OpenStreetMap: PostgreSQL/PostGIS with custom tile generation</li> <li>Mapbox: Vector tiles with real-time style processing</li> <li>HERE Technologies: High-definition mapping with sensor fusion</li> </ul>"},{"location":"day05_data/#location-intelligence-platforms","title":"Location Intelligence Platforms","text":"<ul> <li>Uber's H3: Hexagonal hierarchical spatial indexing for analytics</li> <li>Foursquare's Pilgrim: Real-time location intelligence SDK</li> <li>SafeGraph: POI data processing with privacy preservation</li> <li>Carto: Cloud-native spatial analytics platform</li> </ul>"},{"location":"day05_data/#autonomous-vehicle-systems","title":"Autonomous Vehicle Systems","text":"<ul> <li>Tesla's Neural Networks: Real-time spatial processing for perception</li> <li>Waymo's HD Maps: Centimeter-accurate spatial data processing</li> <li>nuTonomy's CityOS: Urban mobility spatial analytics</li> <li>Mobileye's REM: Crowdsourced HD mapping</li> </ul>"},{"location":"day05_data/#resources","title":"Resources","text":""},{"location":"day05_data/#technical-standards-and-specifications","title":"Technical Standards and Specifications","text":"<ul> <li>OGC Simple Features Specification</li> <li>Protocol Buffers Language Guide</li> <li>Cloud Optimized GeoTIFF</li> <li>Mapbox Vector Tile Specification</li> </ul>"},{"location":"day05_data/#spatial-algorithms-and-data-structures","title":"Spatial Algorithms and Data Structures","text":"<ul> <li>Computational Geometry: Algorithms and Applications</li> <li>Spatial Databases: A Tour by Shashi Shekhar</li> <li>R-Trees: Theory and Applications</li> </ul>"},{"location":"day05_data/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>High Performance Python by Micha Gorelick</li> <li>GDAL/OGR Performance Tips</li> <li>PostGIS Performance Tuning</li> </ul>"},{"location":"day05_data/#industry-applications","title":"Industry Applications","text":"<ul> <li>Uber's Engineering Blog on H3</li> <li>Google's S2 Geometry Library</li> <li>Mapbox's Guide to Vector Tiles</li> </ul> <p>This module provides the foundation for understanding how advanced data processing techniques enable scalable, high-performance geospatial systems that can handle the complexity and scale requirements of modern location-based applications.</p>"},{"location":"day06_perf/","title":"Day 06 - Perf","text":""},{"location":"day06_perf/#learning-objectives","title":"Learning Objectives","text":"<p>This module covers advanced performance engineering and reliability practices essential for enterprise geospatial systems operating at scale. By the end of this module, you will understand:</p> <ul> <li>Performance measurement and profiling techniques for identifying bottlenecks in geospatial applications</li> <li>Memory optimization strategies for processing large-scale spatial datasets efficiently</li> <li>Concurrency and parallelism patterns optimized for spatial operations and I/O-bound workloads  </li> <li>Caching architectures with spatial awareness and geographic distribution strategies</li> <li>Production observability including metrics, logging, tracing, and alerting for geospatial services</li> <li>Reliability engineering patterns including circuit breakers, bulkheads, and graceful degradation</li> <li>Capacity planning and auto-scaling for geospatial workloads with variable spatial distributions</li> <li>Database performance optimization for spatial queries and large geographic datasets</li> </ul>"},{"location":"day06_perf/#theoretical-foundation","title":"Theoretical Foundation","text":""},{"location":"day06_perf/#performance-characteristics-of-geospatial-systems","title":"Performance Characteristics of Geospatial Systems","text":"<p>Spatial Data Access Patterns: Geospatial applications exhibit unique performance characteristics: - Spatial locality: Queries often cluster around geographic regions - Multi-scale access: Same data accessed at different zoom levels and resolutions - Temporal patterns: Usage follows human activity patterns (diurnal, seasonal) - Hotspot phenomena: Popular locations create uneven load distribution</p> <p>Performance Bottlenecks in Spatial Systems: - I/O bound operations: Database queries, file system access, network tile fetching - CPU bound operations: Coordinate transformations, geometric computations, spatial indexing - Memory bound operations: Large dataset processing, spatial joins, buffer operations - Network bound operations: Tile serving, real-time location updates, distributed queries</p> <p>Scaling Challenges: - Geographic load balancing: Distributing work based on spatial boundaries - Cache efficiency: Balancing hit rates with storage costs across geographic regions - Data locality: Minimizing network overhead for spatially-related data - Query optimization: Spatial query plans with varying selectivity ratios</p>"},{"location":"day06_perf/#site-reliability-engineering-for-geospatial-services","title":"Site Reliability Engineering for Geospatial Services","text":"<p>Availability Requirements: - Consumer applications: 99.9% availability (8.77 hours/year downtime) - Enterprise systems: 99.95% availability (4.38 hours/year downtime)  - Safety-critical systems: 99.99% availability (52.6 minutes/year downtime) - Global mapping services: 99.999% availability (5.26 minutes/year downtime)</p> <p>Error Budgets and SLOs: - Latency SLOs: p95 &lt; 100ms for interactive mapping, p99 &lt; 500ms for routing - Throughput SLOs: 10,000+ QPS for tile serving, 1,000+ QPS for geocoding - Error rate SLOs: &lt; 0.1% for critical path operations, &lt; 1% for non-critical features</p> <p>Reliability Patterns: - Circuit breakers: Prevent cascade failures between geospatial services - Bulkhead isolation: Separate resource pools for different geographic regions - Graceful degradation: Fallback to cached or simplified data during outages - Chaos engineering: Systematic testing of failure scenarios in spatial systems</p>"},{"location":"day06_perf/#performance-measurement-and-profiling","title":"Performance Measurement and Profiling","text":""},{"location":"day06_perf/#advanced-profiling-techniques","title":"Advanced Profiling Techniques","text":"<pre><code>import cProfile\nimport pstats\nimport tracemalloc\nimport psutil\nimport time\nfrom typing import Dict, List, Callable, Any\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom functools import wraps\n\n@dataclass\nclass PerformanceMetrics:\n    \"\"\"Comprehensive performance metrics for geospatial operations.\"\"\"\n    duration_seconds: float\n    memory_peak_mb: float\n    memory_current_mb: float\n    cpu_percent: float\n    disk_io_read_mb: float\n    disk_io_write_mb: float\n    network_bytes_sent: int\n    network_bytes_recv: int\n    spatial_operations_count: int\n    cache_hit_rate: float\n\nclass GeospatialProfiler:\n    \"\"\"Advanced profiler for geospatial operations.\"\"\"\n\n    def __init__(self):\n        self.metrics_history = []\n        self.active_profiles = {}\n        self.process = psutil.Process()\n\n    @contextmanager\n    def profile_operation(self, operation_name: str):\n        \"\"\"Profile a geospatial operation with comprehensive metrics.\"\"\"\n        # Start resource monitoring\n        start_time = time.time()\n        tracemalloc.start()\n\n        # Baseline measurements\n        baseline_memory = self.process.memory_info().rss / 1024 / 1024  # MB\n        baseline_io = self.process.io_counters()\n        baseline_net = psutil.net_io_counters()\n\n        # CPU profiling\n        profiler = cProfile.Profile()\n        profiler.enable()\n\n        try:\n            yield self\n        finally:\n            # Stop profiling\n            profiler.disable()\n\n            # Collect final measurements\n            end_time = time.time()\n            final_memory = self.process.memory_info().rss / 1024 / 1024\n            final_io = self.process.io_counters()\n            final_net = psutil.net_io_counters()\n\n            # Memory profiling\n            current_mem, peak_mem = tracemalloc.get_traced_memory()\n            tracemalloc.stop()\n\n            # Calculate metrics\n            metrics = PerformanceMetrics(\n                duration_seconds=end_time - start_time,\n                memory_peak_mb=peak_mem / 1024 / 1024,\n                memory_current_mb=current_mem / 1024 / 1024,\n                cpu_percent=self.process.cpu_percent(),\n                disk_io_read_mb=(final_io.read_bytes - baseline_io.read_bytes) / 1024 / 1024,\n                disk_io_write_mb=(final_io.write_bytes - baseline_io.write_bytes) / 1024 / 1024,\n                network_bytes_sent=final_net.bytes_sent - baseline_net.bytes_sent,\n                network_bytes_recv=final_net.bytes_recv - baseline_net.bytes_recv,\n                spatial_operations_count=getattr(self, '_spatial_ops_count', 0),\n                cache_hit_rate=getattr(self, '_cache_hit_rate', 0.0)\n            )\n\n            # Store profile results\n            self.active_profiles[operation_name] = {\n                'metrics': metrics,\n                'profiler': profiler,\n                'timestamp': end_time\n            }\n\n            self.metrics_history.append((operation_name, metrics))\n\n    def get_hotspots(self, operation_name: str, top_n: int = 10) -&gt; List[Dict]:\n        \"\"\"Get performance hotspots for an operation.\"\"\"\n        if operation_name not in self.active_profiles:\n            return []\n\n        profiler = self.active_profiles[operation_name]['profiler']\n        stats = pstats.Stats(profiler)\n\n        # Sort by cumulative time\n        stats.sort_stats('cumulative')\n\n        # Extract top functions\n        hotspots = []\n        for func_info in stats.get_stats_profile().func_profiles.values():\n            if len(hotspots) &gt;= top_n:\n                break\n\n            hotspots.append({\n                'function': func_info.func_name,\n                'filename': func_info.filename,\n                'line_number': func_info.line_number,\n                'cumulative_time': func_info.cumulative_time,\n                'internal_time': func_info.internal_time,\n                'call_count': func_info.call_count\n            })\n\n        return hotspots\n\n    def generate_performance_report(self) -&gt; Dict[str, Any]:\n        \"\"\"Generate comprehensive performance report.\"\"\"\n        if not self.metrics_history:\n            return {}\n\n        # Aggregate metrics\n        total_operations = len(self.metrics_history)\n        avg_duration = sum(m.duration_seconds for _, m in self.metrics_history) / total_operations\n        max_memory = max(m.memory_peak_mb for _, m in self.metrics_history)\n        total_disk_io = sum(m.disk_io_read_mb + m.disk_io_write_mb for _, m in self.metrics_history)\n\n        # Identify bottlenecks\n        slow_operations = [\n            (name, metrics) for name, metrics in self.metrics_history \n            if metrics.duration_seconds &gt; avg_duration * 2\n        ]\n\n        memory_intensive = [\n            (name, metrics) for name, metrics in self.metrics_history\n            if metrics.memory_peak_mb &gt; max_memory * 0.8\n        ]\n\n        return {\n            'summary': {\n                'total_operations': total_operations,\n                'average_duration_seconds': avg_duration,\n                'max_memory_mb': max_memory,\n                'total_disk_io_mb': total_disk_io,\n                'overall_cache_hit_rate': sum(m.cache_hit_rate for _, m in self.metrics_history) / total_operations\n            },\n            'bottlenecks': {\n                'slow_operations': [(name, m.duration_seconds) for name, m in slow_operations],\n                'memory_intensive': [(name, m.memory_peak_mb) for name, m in memory_intensive]\n            },\n            'recommendations': self._generate_recommendations()\n        }\n\n    def _generate_recommendations(self) -&gt; List[str]:\n        \"\"\"Generate performance optimization recommendations.\"\"\"\n        recommendations = []\n\n        if not self.metrics_history:\n            return recommendations\n\n        avg_memory = sum(m.memory_peak_mb for _, m in self.metrics_history) / len(self.metrics_history)\n        avg_cache_hit = sum(m.cache_hit_rate for _, m in self.metrics_history) / len(self.metrics_history)\n\n        if avg_memory &gt; 1000:  # &gt; 1GB average\n            recommendations.append(\"Consider implementing streaming/chunked processing for large datasets\")\n\n        if avg_cache_hit &lt; 0.7:  # &lt; 70% cache hit rate\n            recommendations.append(\"Optimize caching strategy - consider larger cache size or better eviction policies\")\n\n        high_io_ops = [m for _, m in self.metrics_history if m.disk_io_read_mb + m.disk_io_write_mb &gt; 100]\n        if len(high_io_ops) &gt; len(self.metrics_history) * 0.3:\n            recommendations.append(\"High disk I/O detected - consider SSD storage or better I/O optimization\")\n\n        return recommendations\n\ndef performance_monitor(operation_type: str = \"spatial_operation\"):\n    \"\"\"Decorator for monitoring performance of spatial operations.\"\"\"\n    def decorator(func: Callable) -&gt; Callable:\n        @wraps(func)\n        async def async_wrapper(*args, **kwargs):\n            profiler = GeospatialProfiler()\n            with profiler.profile_operation(f\"{operation_type}_{func.__name__}\"):\n                result = await func(*args, **kwargs)\n\n                # Log performance metrics\n                metrics = profiler.metrics_history[-1][1]\n                logger.info(f\"Performance metrics for {func.__name__}\", extra={\n                    'duration_seconds': metrics.duration_seconds,\n                    'memory_peak_mb': metrics.memory_peak_mb,\n                    'operation_type': operation_type\n                })\n\n                return result\n\n        @wraps(func)\n        def sync_wrapper(*args, **kwargs):\n            profiler = GeospatialProfiler()\n            with profiler.profile_operation(f\"{operation_type}_{func.__name__}\"):\n                result = func(*args, **kwargs)\n\n                # Log performance metrics\n                metrics = profiler.metrics_history[-1][1]\n                logger.info(f\"Performance metrics for {func.__name__}\", extra={\n                    'duration_seconds': metrics.duration_seconds,\n                    'memory_peak_mb': metrics.memory_peak_mb,\n                    'operation_type': operation_type\n                })\n\n                return result\n\n        # Return appropriate wrapper based on function type\n        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper\n\n    return decorator\n</code></pre>"},{"location":"day06_perf/#memory-optimization-strategies","title":"Memory Optimization Strategies","text":"<pre><code>import gc\nimport weakref\nfrom typing import Optional, Dict, Any, Iterator\nfrom collections import OrderedDict\nimport numpy as np\nfrom shapely.geometry import Geometry\nimport gzip\nimport pickle\n\nclass SpatialMemoryManager:\n    \"\"\"Advanced memory management for geospatial operations.\"\"\"\n\n    def __init__(self, max_memory_gb: float = 4.0):\n        self.max_memory_bytes = max_memory_gb * 1024 * 1024 * 1024\n        self.geometry_cache = SpatialLRUCache(max_size_mb=1000)\n        self.compression_ratio = 0.3  # Estimated compression ratio\n        self.weak_refs = weakref.WeakValueDictionary()\n\n    @contextmanager\n    def memory_limit_context(self, operation_name: str):\n        \"\"\"Context manager for memory-limited operations.\"\"\"\n        initial_memory = self._get_memory_usage()\n\n        try:\n            yield self\n        finally:\n            current_memory = self._get_memory_usage()\n            memory_delta = current_memory - initial_memory\n\n            if memory_delta &gt; self.max_memory_bytes * 0.8:\n                logger.warning(f\"High memory usage in {operation_name}: {memory_delta / 1024 / 1024:.1f} MB\")\n                self._aggressive_cleanup()\n\n    def optimize_geometry_collection(self, geometries: List[Geometry]) -&gt; 'OptimizedGeometryCollection':\n        \"\"\"Optimize a collection of geometries for memory efficiency.\"\"\"\n        # Analyze geometry characteristics\n        total_coords = sum(len(geom.coords) if hasattr(geom, 'coords') else 0 for geom in geometries)\n        avg_complexity = total_coords / len(geometries) if geometries else 0\n\n        if avg_complexity &gt; 1000:  # High complexity geometries\n            return self._create_compressed_collection(geometries)\n        elif len(geometries) &gt; 10000:  # Many simple geometries\n            return self._create_indexed_collection(geometries)\n        else:\n            return self._create_standard_collection(geometries)\n\n    def _create_compressed_collection(self, geometries: List[Geometry]) -&gt; 'CompressedGeometryCollection':\n        \"\"\"Create memory-efficient compressed geometry collection.\"\"\"\n        compressed_data = []\n\n        for geom in geometries:\n            # Serialize and compress geometry\n            wkb_data = geom.wkb\n            compressed = gzip.compress(wkb_data)\n            compressed_data.append(compressed)\n\n        return CompressedGeometryCollection(compressed_data, self.compression_ratio)\n\n    def _create_indexed_collection(self, geometries: List[Geometry]) -&gt; 'IndexedGeometryCollection':\n        \"\"\"Create spatially-indexed geometry collection.\"\"\"\n        # Build spatial index with minimal memory footprint\n        spatial_index = rtree.index.Index()\n        geometry_store = {}\n\n        for i, geom in enumerate(geometries):\n            # Store only bounding box in index\n            spatial_index.insert(i, geom.bounds)\n\n            # Store geometry with weak reference\n            geometry_store[i] = geom\n            self.weak_refs[f\"geom_{i}\"] = geom\n\n        return IndexedGeometryCollection(spatial_index, geometry_store)\n\n    def stream_large_dataset(self, dataset_path: str, \n                           chunk_size: int = 10000) -&gt; Iterator[List[Geometry]]:\n        \"\"\"Stream large datasets with memory management.\"\"\"\n        with open(dataset_path, 'rb') as f:\n            while True:\n                with self.memory_limit_context(f\"stream_chunk\"):\n                    chunk = self._read_chunk(f, chunk_size)\n                    if not chunk:\n                        break\n\n                    # Process chunk\n                    processed_chunk = self._process_geometry_chunk(chunk)\n                    yield processed_chunk\n\n                    # Explicit cleanup\n                    del chunk, processed_chunk\n                    gc.collect()\n\n    def _aggressive_cleanup(self):\n        \"\"\"Perform aggressive memory cleanup.\"\"\"\n        # Clear caches\n        self.geometry_cache.clear()\n\n        # Force garbage collection\n        gc.collect()\n\n        # Clear weak references\n        self.weak_refs.clear()\n\n        # Log memory recovery\n        current_memory = self._get_memory_usage()\n        logger.info(f\"Memory cleanup completed. Current usage: {current_memory / 1024 / 1024:.1f} MB\")\n\nclass SpatialLRUCache:\n    \"\"\"LRU cache optimized for spatial data.\"\"\"\n\n    def __init__(self, max_size_mb: int = 1000):\n        self.max_size_bytes = max_size_mb * 1024 * 1024\n        self.current_size = 0\n        self.cache = OrderedDict()\n        self.size_tracker = {}\n\n    def get(self, key: str) -&gt; Optional[Any]:\n        \"\"\"Get item from cache and move to end (most recently used).\"\"\"\n        if key in self.cache:\n            # Move to end\n            value = self.cache.pop(key)\n            self.cache[key] = value\n            return value\n        return None\n\n    def put(self, key: str, value: Any, estimated_size: int = None):\n        \"\"\"Put item in cache with size tracking.\"\"\"\n        if estimated_size is None:\n            estimated_size = self._estimate_size(value)\n\n        # Remove existing item if present\n        if key in self.cache:\n            self.current_size -= self.size_tracker[key]\n            del self.cache[key]\n            del self.size_tracker[key]\n\n        # Evict items if necessary\n        while self.current_size + estimated_size &gt; self.max_size_bytes and self.cache:\n            self._evict_lru()\n\n        # Add new item\n        self.cache[key] = value\n        self.size_tracker[key] = estimated_size\n        self.current_size += estimated_size\n\n    def _evict_lru(self):\n        \"\"\"Evict least recently used item.\"\"\"\n        if self.cache:\n            key, _ = self.cache.popitem(last=False)  # FIFO - least recently used\n            size = self.size_tracker.pop(key)\n            self.current_size -= size\n\n    def _estimate_size(self, value: Any) -&gt; int:\n        \"\"\"Estimate memory size of cached value.\"\"\"\n        try:\n            return len(pickle.dumps(value))\n        except:\n            # Fallback estimation\n            if isinstance(value, (str, bytes)):\n                return len(value)\n            elif isinstance(value, (list, tuple)):\n                return sum(self._estimate_size(item) for item in value[:10])  # Sample first 10\n            else:\n                return 1024  # Conservative estimate\n</code></pre>"},{"location":"day06_perf/#concurrency-and-parallelism-optimization","title":"Concurrency and Parallelism Optimization","text":"<pre><code>import asyncio\nimport multiprocessing as mp\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\nfrom typing import List, Callable, Any, Awaitable\nimport numpy as np\nfrom functools import partial\n\nclass SpatialConcurrencyManager:\n    \"\"\"Optimized concurrency management for spatial operations.\"\"\"\n\n    def __init__(self):\n        self.cpu_count = mp.cpu_count()\n        self.io_thread_pool = ThreadPoolExecutor(max_workers=min(32, self.cpu_count * 4))\n        self.cpu_process_pool = ProcessPoolExecutor(max_workers=self.cpu_count)\n        self.semaphores = {\n            'disk_io': asyncio.Semaphore(4),  # Limit concurrent disk operations\n            'network_io': asyncio.Semaphore(20),  # Network operations\n            'cpu_intensive': asyncio.Semaphore(self.cpu_count),  # CPU-bound operations\n            'memory_intensive': asyncio.Semaphore(2)  # Memory-intensive operations\n        }\n\n    async def parallel_spatial_operation(self, \n                                       geometries: List[Geometry], \n                                       operation: Callable,\n                                       chunk_size: int = None,\n                                       operation_type: str = 'cpu_intensive') -&gt; List[Any]:\n        \"\"\"Execute spatial operations in parallel with optimal batching.\"\"\"\n\n        if chunk_size is None:\n            chunk_size = max(1, len(geometries) // (self.cpu_count * 2))\n\n        # Split geometries into chunks\n        chunks = [geometries[i:i + chunk_size] for i in range(0, len(geometries), chunk_size)]\n\n        # Choose execution strategy based on operation type\n        if operation_type == 'cpu_intensive':\n            return await self._cpu_parallel_execution(chunks, operation)\n        elif operation_type == 'io_bound':\n            return await self._io_parallel_execution(chunks, operation)\n        else:\n            return await self._mixed_parallel_execution(chunks, operation)\n\n    async def _cpu_parallel_execution(self, chunks: List[List[Geometry]], \n                                    operation: Callable) -&gt; List[Any]:\n        \"\"\"Execute CPU-intensive operations using process pool.\"\"\"\n        loop = asyncio.get_event_loop()\n\n        # Prepare serializable operation\n        if hasattr(operation, '__self__'):\n            # Handle bound methods by converting to function\n            operation = partial(operation.__func__, operation.__self__)\n\n        tasks = []\n        for chunk in chunks:\n            async with self.semaphores['cpu_intensive']:\n                task = loop.run_in_executor(\n                    self.cpu_process_pool,\n                    self._process_geometry_chunk,\n                    chunk, operation\n                )\n                tasks.append(task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Flatten results and handle exceptions\n        flattened_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                logger.error(f\"CPU operation failed: {result}\")\n                continue\n            flattened_results.extend(result)\n\n        return flattened_results\n\n    async def _io_parallel_execution(self, chunks: List[List[Geometry]], \n                                   operation: Callable) -&gt; List[Any]:\n        \"\"\"Execute I/O-bound operations using thread pool.\"\"\"\n        loop = asyncio.get_event_loop()\n\n        tasks = []\n        for chunk in chunks:\n            async with self.semaphores['network_io']:\n                task = loop.run_in_executor(\n                    self.io_thread_pool,\n                    self._process_geometry_chunk,\n                    chunk, operation\n                )\n                tasks.append(task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Flatten and filter results\n        flattened_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                logger.error(f\"I/O operation failed: {result}\")\n                continue\n            flattened_results.extend(result)\n\n        return flattened_results\n\n    def _process_geometry_chunk(self, chunk: List[Geometry], operation: Callable) -&gt; List[Any]:\n        \"\"\"Process a chunk of geometries with the given operation.\"\"\"\n        try:\n            return [operation(geom) for geom in chunk]\n        except Exception as e:\n            logger.error(f\"Chunk processing failed: {e}\")\n            return []\n\n    async def adaptive_batch_processing(self, \n                                      data_stream: AsyncIterator[Geometry],\n                                      processor: Callable,\n                                      target_latency_ms: float = 100) -&gt; AsyncIterator[Any]:\n        \"\"\"Adaptive batch processing that adjusts batch size based on performance.\"\"\"\n\n        batch_size = 10  # Initial batch size\n        batch = []\n        processing_times = []\n\n        async for item in data_stream:\n            batch.append(item)\n\n            if len(batch) &gt;= batch_size:\n                # Process batch and measure time\n                start_time = time.time()\n\n                async with self.semaphores['cpu_intensive']:\n                    results = await self.parallel_spatial_operation(\n                        batch, processor, chunk_size=batch_size//2\n                    )\n\n                processing_time = (time.time() - start_time) * 1000  # ms\n                processing_times.append(processing_time)\n\n                # Yield results\n                for result in results:\n                    yield result\n\n                # Adapt batch size based on performance\n                batch_size = self._adapt_batch_size(\n                    batch_size, processing_time, target_latency_ms, processing_times\n                )\n\n                batch = []\n\n        # Process remaining items\n        if batch:\n            async with self.semaphores['cpu_intensive']:\n                results = await self.parallel_spatial_operation(batch, processor)\n                for result in results:\n                    yield result\n\n    def _adapt_batch_size(self, current_size: int, last_time: float, \n                         target_time: float, history: List[float]) -&gt; int:\n        \"\"\"Adapt batch size based on performance metrics.\"\"\"\n        if len(history) &lt; 3:\n            return current_size\n\n        avg_time = sum(history[-3:]) / 3\n\n        if avg_time &gt; target_time * 1.2:  # Too slow\n            return max(1, int(current_size * 0.8))\n        elif avg_time &lt; target_time * 0.5:  # Too fast, can increase\n            return min(1000, int(current_size * 1.2))\n        else:\n            return current_size  # Good performance, keep current size\n\nclass SpatialWorkloadBalancer:\n    \"\"\"Load balancer for spatial workloads across multiple workers.\"\"\"\n\n    def __init__(self, worker_count: int = None):\n        self.worker_count = worker_count or mp.cpu_count()\n        self.workers = []\n        self.workload_stats = {}\n\n    async def distribute_spatial_workload(self, \n                                        spatial_tasks: List[Dict],\n                                        distribution_strategy: str = 'geographic') -&gt; List[Any]:\n        \"\"\"Distribute spatial workload across workers.\"\"\"\n\n        if distribution_strategy == 'geographic':\n            task_groups = self._group_by_geography(spatial_tasks)\n        elif distribution_strategy == 'complexity':\n            task_groups = self._group_by_complexity(spatial_tasks)\n        else:\n            task_groups = self._round_robin_distribution(spatial_tasks)\n\n        # Execute task groups in parallel\n        tasks = []\n        for i, task_group in enumerate(task_groups):\n            worker_id = i % self.worker_count\n            task = self._execute_on_worker(worker_id, task_group)\n            tasks.append(task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        # Collect and flatten results\n        all_results = []\n        for result in results:\n            if isinstance(result, Exception):\n                logger.error(f\"Worker task failed: {result}\")\n                continue\n            all_results.extend(result)\n\n        return all_results\n\n    def _group_by_geography(self, tasks: List[Dict]) -&gt; List[List[Dict]]:\n        \"\"\"Group tasks by geographic proximity.\"\"\"\n        # Implementation would use spatial clustering\n        # For now, simple bbox-based grouping\n        groups = [[] for _ in range(self.worker_count)]\n\n        for i, task in enumerate(tasks):\n            if 'bbox' in task:\n                # Simple hash-based assignment based on bbox center\n                center_x = (task['bbox'][0] + task['bbox'][2]) / 2\n                center_y = (task['bbox'][1] + task['bbox'][3]) / 2\n                group_id = hash((int(center_x * 1000), int(center_y * 1000))) % self.worker_count\n                groups[group_id].append(task)\n            else:\n                groups[i % self.worker_count].append(task)\n\n        return groups\n</code></pre>"},{"location":"day06_perf/#advanced-caching-strategies","title":"Advanced Caching Strategies","text":"<pre><code>import redis\nimport json\nimport hashlib\nfrom typing import Optional, Dict, Any, Union\nimport geojson\nfrom shapely.geometry import shape, box\n\nclass SpatialCacheManager:\n    \"\"\"Multi-tier spatial caching with geographic awareness.\"\"\"\n\n    def __init__(self, redis_client: redis.Redis = None):\n        self.redis_client = redis_client or redis.Redis(host='localhost', port=6379, db=0)\n        self.local_cache = SpatialLRUCache(max_size_mb=500)\n        self.cache_stats = {\n            'hits': 0,\n            'misses': 0,\n            'evictions': 0,\n            'memory_pressure_events': 0\n        }\n\n    async def get_spatial_data(self, bbox: Tuple[float, float, float, float],\n                              zoom_level: int, layer: str) -&gt; Optional[Dict]:\n        \"\"\"Get spatial data with multi-tier caching.\"\"\"\n        cache_key = self._generate_spatial_key(bbox, zoom_level, layer)\n\n        # L1: Check local memory cache\n        local_result = self.local_cache.get(cache_key)\n        if local_result:\n            self.cache_stats['hits'] += 1\n            return local_result\n\n        # L2: Check Redis cache\n        redis_result = await self._get_from_redis(cache_key)\n        if redis_result:\n            # Store in local cache for faster future access\n            self.local_cache.put(cache_key, redis_result)\n            self.cache_stats['hits'] += 1\n            return redis_result\n\n        self.cache_stats['misses'] += 1\n        return None\n\n    async def set_spatial_data(self, bbox: Tuple[float, float, float, float],\n                              zoom_level: int, layer: str, data: Dict,\n                              ttl_seconds: int = 3600):\n        \"\"\"Set spatial data in multi-tier cache.\"\"\"\n        cache_key = self._generate_spatial_key(bbox, zoom_level, layer)\n\n        # Optimize data for caching\n        optimized_data = self._optimize_for_cache(data, zoom_level)\n\n        # Store in both cache tiers\n        self.local_cache.put(cache_key, optimized_data)\n        await self._set_in_redis(cache_key, optimized_data, ttl_seconds)\n\n    def _generate_spatial_key(self, bbox: Tuple[float, float, float, float],\n                             zoom_level: int, layer: str) -&gt; str:\n        \"\"\"Generate cache key for spatial data.\"\"\"\n        # Normalize bbox to reduce cache fragmentation\n        normalized_bbox = self._normalize_bbox(bbox, zoom_level)\n\n        key_data = {\n            'bbox': normalized_bbox,\n            'zoom': zoom_level,\n            'layer': layer\n        }\n\n        # Create hash for consistent key generation\n        key_string = json.dumps(key_data, sort_keys=True)\n        return f\"spatial:{hashlib.md5(key_string.encode()).hexdigest()}\"\n\n    def _normalize_bbox(self, bbox: Tuple[float, float, float, float],\n                       zoom_level: int) -&gt; Tuple[float, float, float, float]:\n        \"\"\"Normalize bbox to grid boundaries for better cache hit rates.\"\"\"\n        # Grid size based on zoom level\n        grid_size = 1.0 / (2 ** max(0, zoom_level - 10))\n\n        # Snap to grid\n        min_x = math.floor(bbox[0] / grid_size) * grid_size\n        min_y = math.floor(bbox[1] / grid_size) * grid_size\n        max_x = math.ceil(bbox[2] / grid_size) * grid_size\n        max_y = math.ceil(bbox[3] / grid_size) * grid_size\n\n        return (min_x, min_y, max_x, max_y)\n\n    def _optimize_for_cache(self, data: Dict, zoom_level: int) -&gt; Dict:\n        \"\"\"Optimize spatial data for caching based on zoom level.\"\"\"\n        if 'features' not in data:\n            return data\n\n        optimized_features = []\n        simplification_tolerance = self._get_simplification_tolerance(zoom_level)\n\n        for feature in data['features']:\n            if 'geometry' in feature:\n                # Simplify geometry based on zoom level\n                geom = shape(feature['geometry'])\n                simplified = geom.simplify(simplification_tolerance, preserve_topology=True)\n\n                optimized_feature = {\n                    'type': 'Feature',\n                    'geometry': simplified.__geo_interface__,\n                    'properties': self._filter_properties(feature.get('properties', {}), zoom_level)\n                }\n                optimized_features.append(optimized_feature)\n\n        return {\n            'type': 'FeatureCollection',\n            'features': optimized_features\n        }\n\n    async def invalidate_spatial_region(self, bbox: Tuple[float, float, float, float],\n                                       layer: str = None):\n        \"\"\"Invalidate cache for a spatial region.\"\"\"\n        # Find all cache keys that intersect with the bbox\n        pattern = f\"spatial:*\"\n        if layer:\n            pattern = f\"spatial:*{layer}*\"\n\n        # This is a simplified approach - production systems would use\n        # spatial indexing of cache keys for efficient invalidation\n        keys_to_invalidate = []\n        async for key in self.redis_client.scan_iter(pattern):\n            if await self._key_intersects_bbox(key, bbox):\n                keys_to_invalidate.append(key)\n\n        # Remove from both cache levels\n        if keys_to_invalidate:\n            await self.redis_client.delete(*keys_to_invalidate)\n            for key in keys_to_invalidate:\n                if key in self.local_cache.cache:\n                    del self.local_cache.cache[key]\n\nclass SpatialPreloadManager:\n    \"\"\"Intelligent preloading of spatial data based on usage patterns.\"\"\"\n\n    def __init__(self, cache_manager: SpatialCacheManager):\n        self.cache_manager = cache_manager\n        self.usage_patterns = {}\n        self.preload_queue = asyncio.Queue()\n        self.preload_worker_running = False\n\n    async def track_access_pattern(self, bbox: Tuple[float, float, float, float],\n                                  zoom_level: int, layer: str):\n        \"\"\"Track spatial data access patterns for intelligent preloading.\"\"\"\n        pattern_key = f\"{layer}:{zoom_level}\"\n\n        if pattern_key not in self.usage_patterns:\n            self.usage_patterns[pattern_key] = {\n                'access_count': 0,\n                'bboxes': [],\n                'last_access': time.time()\n            }\n\n        pattern = self.usage_patterns[pattern_key]\n        pattern['access_count'] += 1\n        pattern['bboxes'].append(bbox)\n        pattern['last_access'] = time.time()\n\n        # Keep only recent bboxes\n        if len(pattern['bboxes']) &gt; 1000:\n            pattern['bboxes'] = pattern['bboxes'][-500:]\n\n        # Trigger preloading for hot patterns\n        if pattern['access_count'] % 10 == 0:\n            await self._schedule_preload(pattern_key)\n\n    async def _schedule_preload(self, pattern_key: str):\n        \"\"\"Schedule preloading based on access patterns.\"\"\"\n        if not self.preload_worker_running:\n            asyncio.create_task(self._preload_worker())\n            self.preload_worker_running = True\n\n        pattern = self.usage_patterns[pattern_key]\n\n        # Predict next likely access areas\n        predicted_areas = self._predict_access_areas(pattern['bboxes'])\n\n        for bbox, confidence in predicted_areas:\n            if confidence &gt; 0.7:  # High confidence predictions\n                layer, zoom_str = pattern_key.split(':')\n                zoom_level = int(zoom_str)\n\n                await self.preload_queue.put({\n                    'bbox': bbox,\n                    'zoom_level': zoom_level,\n                    'layer': layer,\n                    'priority': confidence\n                })\n\n    def _predict_access_areas(self, recent_bboxes: List[Tuple]) -&gt; List[Tuple[Tuple, float]]:\n        \"\"\"Predict likely next access areas based on spatial patterns.\"\"\"\n        if len(recent_bboxes) &lt; 5:\n            return []\n\n        predictions = []\n\n        # Simple spatial trend analysis\n        recent = recent_bboxes[-10:]  # Last 10 accesses\n\n        # Calculate movement vector\n        if len(recent) &gt;= 2:\n            start_center = self._bbox_center(recent[0])\n            end_center = self._bbox_center(recent[-1])\n\n            dx = end_center[0] - start_center[0]\n            dy = end_center[1] - start_center[1]\n\n            # Predict next area based on movement\n            if abs(dx) &gt; 0.001 or abs(dy) &gt; 0.001:  # Significant movement\n                last_bbox = recent[-1]\n                predicted_center = (\n                    self._bbox_center(last_bbox)[0] + dx,\n                    self._bbox_center(last_bbox)[1] + dy\n                )\n\n                bbox_size = (\n                    last_bbox[2] - last_bbox[0],\n                    last_bbox[3] - last_bbox[1]\n                )\n\n                predicted_bbox = (\n                    predicted_center[0] - bbox_size[0]/2,\n                    predicted_center[1] - bbox_size[1]/2,\n                    predicted_center[0] + bbox_size[0]/2,\n                    predicted_center[1] + bbox_size[1]/2\n                )\n\n                predictions.append((predicted_bbox, 0.8))\n\n        return predictions\n\n    async def _preload_worker(self):\n        \"\"\"Background worker for preloading spatial data.\"\"\"\n        while True:\n            try:\n                # Wait for preload tasks\n                preload_task = await asyncio.wait_for(\n                    self.preload_queue.get(), timeout=60.0\n                )\n\n                # Check if data is already cached\n                cache_key = self.cache_manager._generate_spatial_key(\n                    preload_task['bbox'],\n                    preload_task['zoom_level'],\n                    preload_task['layer']\n                )\n\n                existing_data = await self.cache_manager.get_spatial_data(\n                    preload_task['bbox'],\n                    preload_task['zoom_level'],\n                    preload_task['layer']\n                )\n\n                if existing_data is None:\n                    # Fetch and cache data\n                    # This would call your actual data source\n                    data = await self._fetch_spatial_data(\n                        preload_task['bbox'],\n                        preload_task['zoom_level'],\n                        preload_task['layer']\n                    )\n\n                    if data:\n                        await self.cache_manager.set_spatial_data(\n                            preload_task['bbox'],\n                            preload_task['zoom_level'],\n                            preload_task['layer'],\n                            data\n                        )\n\n            except asyncio.TimeoutError:\n                # No preload tasks, worker can exit\n                self.preload_worker_running = False\n                break\n            except Exception as e:\n                logger.error(f\"Preload worker error: {e}\")\n</code></pre>"},{"location":"day06_perf/#production-monitoring-and-observability","title":"Production Monitoring and Observability","text":""},{"location":"day06_perf/#comprehensive-metrics-collection","title":"Comprehensive Metrics Collection","text":"<pre><code>from prometheus_client import Counter, Histogram, Gauge, Summary\nimport structlog\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Prometheus metrics for spatial operations\nSPATIAL_OPERATIONS_TOTAL = Counter(\n    'spatial_operations_total',\n    'Total number of spatial operations',\n    ['operation_type', 'layer', 'result_status']\n)\n\nSPATIAL_QUERY_DURATION = Histogram(\n    'spatial_query_duration_seconds',\n    'Time spent on spatial queries',\n    ['query_type', 'complexity_bucket'],\n    buckets=[0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 30.0, 60.0]\n)\n\nSPATIAL_CACHE_OPERATIONS = Counter(\n    'spatial_cache_operations_total',\n    'Cache operations',\n    ['cache_type', 'operation', 'result']\n)\n\nSPATIAL_MEMORY_USAGE = Gauge(\n    'spatial_memory_usage_bytes',\n    'Memory usage for spatial operations',\n    ['component']\n)\n\nSPATIAL_INDEX_SIZE = Gauge(\n    'spatial_index_size_total',\n    'Number of entries in spatial indices',\n    ['index_type']\n)\n\nclass SpatialObservabilityManager:\n    \"\"\"Comprehensive observability for spatial systems.\"\"\"\n\n    def __init__(self):\n        self.logger = structlog.get_logger()\n        self.tracer = trace.get_tracer(__name__)\n        self.active_spans = {}\n\n        # Setup distributed tracing\n        trace.set_tracer_provider(TracerProvider())\n        jaeger_exporter = JaegerExporter(\n            agent_host_name=\"localhost\",\n            agent_port=14268,\n        )\n        span_processor = BatchSpanProcessor(jaeger_exporter)\n        trace.get_tracer_provider().add_span_processor(span_processor)\n\n    @contextmanager\n    def trace_spatial_operation(self, operation_name: str, \n                               bbox: Tuple = None, layer: str = None):\n        \"\"\"Trace spatial operations with distributed tracing.\"\"\"\n        with self.tracer.start_as_current_span(operation_name) as span:\n            # Add spatial context to span\n            if bbox:\n                span.set_attribute(\"spatial.bbox.min_x\", bbox[0])\n                span.set_attribute(\"spatial.bbox.min_y\", bbox[1])\n                span.set_attribute(\"spatial.bbox.max_x\", bbox[2])\n                span.set_attribute(\"spatial.bbox.max_y\", bbox[3])\n                span.set_attribute(\"spatial.bbox.area\", \n                                 (bbox[2] - bbox[0]) * (bbox[3] - bbox[1]))\n\n            if layer:\n                span.set_attribute(\"spatial.layer\", layer)\n\n            # Record operation start\n            start_time = time.time()\n            SPATIAL_OPERATIONS_TOTAL.labels(\n                operation_type=operation_name,\n                layer=layer or 'unknown',\n                result_status='started'\n            ).inc()\n\n            try:\n                yield span\n\n                # Record successful completion\n                SPATIAL_OPERATIONS_TOTAL.labels(\n                    operation_type=operation_name,\n                    layer=layer or 'unknown',\n                    result_status='success'\n                ).inc()\n\n            except Exception as e:\n                # Record error\n                span.record_exception(e)\n                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n\n                SPATIAL_OPERATIONS_TOTAL.labels(\n                    operation_type=operation_name,\n                    layer=layer or 'unknown',\n                    result_status='error'\n                ).inc()\n\n                raise\n            finally:\n                # Record timing\n                duration = time.time() - start_time\n                complexity = self._classify_complexity(bbox, operation_name)\n\n                SPATIAL_QUERY_DURATION.labels(\n                    query_type=operation_name,\n                    complexity_bucket=complexity\n                ).observe(duration)\n\n    def log_spatial_event(self, event_type: str, **context):\n        \"\"\"Log spatial events with structured logging.\"\"\"\n        self.logger.info(\n            event_type,\n            **context,\n            timestamp=time.time()\n        )\n\n    def record_cache_operation(self, cache_type: str, operation: str, \n                             result: str, response_time: float = None):\n        \"\"\"Record cache operation metrics.\"\"\"\n        SPATIAL_CACHE_OPERATIONS.labels(\n            cache_type=cache_type,\n            operation=operation,\n            result=result\n        ).inc()\n\n        if response_time:\n            SPATIAL_QUERY_DURATION.labels(\n                query_type=f\"cache_{operation}\",\n                complexity_bucket=\"simple\"\n            ).observe(response_time)\n\n    def update_memory_metrics(self, component: str, memory_bytes: int):\n        \"\"\"Update memory usage metrics.\"\"\"\n        SPATIAL_MEMORY_USAGE.labels(component=component).set(memory_bytes)\n\n    def update_index_metrics(self, index_type: str, entry_count: int):\n        \"\"\"Update spatial index metrics.\"\"\"\n        SPATIAL_INDEX_SIZE.labels(index_type=index_type).set(entry_count)\n\n    def _classify_complexity(self, bbox: Tuple, operation: str) -&gt; str:\n        \"\"\"Classify operation complexity for metrics.\"\"\"\n        if not bbox:\n            return \"unknown\"\n\n        area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n\n        if area &lt; 0.001:\n            return \"simple\"\n        elif area &lt; 0.1:\n            return \"medium\"\n        else:\n            return \"complex\"\n\nclass SpatialHealthChecker:\n    \"\"\"Health monitoring for spatial system components.\"\"\"\n\n    def __init__(self, spatial_service, cache_manager, database_pool):\n        self.spatial_service = spatial_service\n        self.cache_manager = cache_manager\n        self.database_pool = database_pool\n        self.health_history = []\n\n    async def check_system_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Comprehensive system health check.\"\"\"\n        health_status = {\n            'timestamp': time.time(),\n            'overall_status': 'healthy',\n            'components': {}\n        }\n\n        # Check spatial service health\n        try:\n            service_health = await self._check_spatial_service()\n            health_status['components']['spatial_service'] = service_health\n        except Exception as e:\n            health_status['components']['spatial_service'] = {\n                'status': 'unhealthy',\n                'error': str(e)\n            }\n            health_status['overall_status'] = 'degraded'\n\n        # Check cache health\n        try:\n            cache_health = await self._check_cache_health()\n            health_status['components']['cache'] = cache_health\n        except Exception as e:\n            health_status['components']['cache'] = {\n                'status': 'unhealthy',\n                'error': str(e)\n            }\n            health_status['overall_status'] = 'degraded'\n\n        # Check database health\n        try:\n            db_health = await self._check_database_health()\n            health_status['components']['database'] = db_health\n        except Exception as e:\n            health_status['components']['database'] = {\n                'status': 'unhealthy',\n                'error': str(e)\n            }\n            health_status['overall_status'] = 'unhealthy'\n\n        # Store health history\n        self.health_history.append(health_status)\n        if len(self.health_history) &gt; 100:\n            self.health_history = self.health_history[-50:]\n\n        return health_status\n\n    async def _check_spatial_service(self) -&gt; Dict[str, Any]:\n        \"\"\"Check spatial service health.\"\"\"\n        start_time = time.time()\n\n        # Test basic query\n        test_bbox = (-122.5, 37.7, -122.4, 37.8)  # San Francisco area\n        results = await self.spatial_service.query_bbox(*test_bbox)\n\n        response_time = time.time() - start_time\n\n        return {\n            'status': 'healthy' if response_time &lt; 5.0 else 'slow',\n            'response_time_seconds': response_time,\n            'result_count': len(results) if results else 0\n        }\n\n    async def _check_cache_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check cache system health.\"\"\"\n        # Test cache operations\n        test_key = f\"health_check_{time.time()}\"\n        test_data = {'test': True, 'timestamp': time.time()}\n\n        # Write test\n        start_time = time.time()\n        await self.cache_manager.set_spatial_data(\n            (-1, -1, 1, 1), 10, \"health_check\", test_data, ttl_seconds=60\n        )\n        write_time = time.time() - start_time\n\n        # Read test\n        start_time = time.time()\n        retrieved_data = await self.cache_manager.get_spatial_data(\n            (-1, -1, 1, 1), 10, \"health_check\"\n        )\n        read_time = time.time() - start_time\n\n        # Calculate cache hit rate\n        hit_rate = (\n            self.cache_manager.cache_stats['hits'] / \n            max(1, self.cache_manager.cache_stats['hits'] + self.cache_manager.cache_stats['misses'])\n        )\n\n        return {\n            'status': 'healthy' if write_time &lt; 0.1 and read_time &lt; 0.05 else 'slow',\n            'write_time_seconds': write_time,\n            'read_time_seconds': read_time,\n            'hit_rate': hit_rate,\n            'data_consistency': retrieved_data == test_data\n        }\n\n    async def _check_database_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check database health.\"\"\"\n        start_time = time.time()\n\n        async with self.database_pool.acquire() as connection:\n            # Test simple query\n            result = await connection.fetch(\"SELECT 1 as health_check\")\n\n        response_time = time.time() - start_time\n\n        return {\n            'status': 'healthy' if response_time &lt; 1.0 else 'slow',\n            'response_time_seconds': response_time,\n            'connection_pool_size': len(self.database_pool._holders),\n            'active_connections': len([h for h in self.database_pool._holders if h._con])\n        }\n</code></pre>"},{"location":"day06_perf/#professional-development-exercises","title":"Professional Development Exercises","text":""},{"location":"day06_perf/#exercise-1-build-a-performance-analysis-dashboard","title":"Exercise 1: Build a Performance Analysis Dashboard","text":"<p>Create a comprehensive performance monitoring system: - Real-time metrics visualization for spatial operations - Performance regression detection and alerting - Resource utilization tracking and capacity planning - Automated performance benchmarking and reporting</p>"},{"location":"day06_perf/#exercise-2-implement-auto-scaling-for-spatial-workloads","title":"Exercise 2: Implement Auto-Scaling for Spatial Workloads","text":"<p>Design an auto-scaling system that: - Monitors spatial query patterns and load distribution - Scales compute resources based on geographic demand - Implements predictive scaling for known traffic patterns - Balances cost optimization with performance requirements</p>"},{"location":"day06_perf/#exercise-3-create-a-multi-region-caching-strategy","title":"Exercise 3: Create a Multi-Region Caching Strategy","text":"<p>Build a globally distributed caching system: - Geographic cache distribution and replication - Intelligent cache warming based on usage patterns - Cross-region cache invalidation and consistency - Performance optimization for global user bases</p>"},{"location":"day06_perf/#exercise-4-develop-a-chaos-engineering-framework","title":"Exercise 4: Develop a Chaos Engineering Framework","text":"<p>Implement chaos engineering for spatial systems: - Network partition simulation between geographic regions - Database failover testing with spatial data consistency - Cache failure scenarios and graceful degradation - Load spike simulation and recovery testing</p>"},{"location":"day06_perf/#industry-context-and-real-world-applications","title":"Industry Context and Real-World Applications","text":""},{"location":"day06_perf/#production-performance-requirements","title":"Production Performance Requirements","text":"<p>Google Maps Performance Standards: - Tile serving: p99 &lt; 50ms globally - Search queries: p95 &lt; 200ms - Route calculation: p99 &lt; 2 seconds - 99.9% availability with global redundancy</p> <p>Uber's Spatial Computing Scale: - 15+ billion location updates daily - Sub-second H3 spatial indexing - Real-time driver matching algorithms - 99.99% availability for safety-critical operations</p> <p>Tesla's Autonomous Driving Requirements: - Real-time HD map processing: &lt; 10ms latency - Sensor fusion with map data: 60+ FPS - Neural network inference: &lt; 5ms - Zero tolerance for safety-critical failures</p>"},{"location":"day06_perf/#enterprise-performance-optimization","title":"Enterprise Performance Optimization","text":"<p>Financial Services (Trading Systems): - Geospatial market data: microsecond latency requirements - Regulatory compliance with audit trails - Multi-region disaster recovery - Real-time risk calculations with geographic factors</p> <p>Logistics and Supply Chain: - Route optimization: millions of calculations per second - Real-time package tracking at global scale - Warehouse automation with spatial optimization - Predictive analytics for demand forecasting</p> <p>Smart Cities and IoT: - Sensor data processing: millions of updates per minute - Real-time traffic optimization - Emergency response coordination - Energy grid optimization with spatial factors</p>"},{"location":"day06_perf/#resources","title":"Resources","text":""},{"location":"day06_perf/#performance-engineering","title":"Performance Engineering","text":"<ul> <li>High Performance Python by Micha Gorelick</li> <li>Site Reliability Engineering by Google</li> <li>Designing Data-Intensive Applications by Martin Kleppmann</li> </ul>"},{"location":"day06_perf/#monitoring-and-observability","title":"Monitoring and Observability","text":"<ul> <li>Prometheus Monitoring</li> <li>OpenTelemetry Documentation</li> <li>Grafana Visualization</li> <li>Jaeger Distributed Tracing</li> </ul>"},{"location":"day06_perf/#database-performance","title":"Database Performance","text":"<ul> <li>PostGIS Performance Tuning</li> <li>PostgreSQL Performance Optimization</li> <li>Spatial Database Optimization</li> </ul>"},{"location":"day06_perf/#industry-performance-practices","title":"Industry Performance Practices","text":"<ul> <li>Uber's Engineering Blog</li> <li>Google's SRE Practices</li> <li>Netflix's Performance Engineering</li> </ul> <p>This module provides the foundation for building and operating high-performance, reliable geospatial systems that can scale to serve millions of users while maintaining strict performance and availability requirements.</p>"},{"location":"day07_mock/","title":"Day 07 - Mock","text":""},{"location":"day07_mock/#learning-objectives","title":"Learning Objectives","text":"<p>This capstone project synthesizes all concepts from the previous modules into a comprehensive enterprise-grade geospatial service. By the end of this module, you will have demonstrated:</p> <ul> <li>Full-stack geospatial application development with production-ready architecture patterns</li> <li>API design and implementation following RESTful principles and OpenAPI specifications</li> <li>Spatial data modeling and query optimization for large-scale road network datasets</li> <li>Performance engineering and scalability considerations for high-throughput spatial services</li> <li>Testing strategies and quality assurance for mission-critical geospatial applications</li> <li>Documentation and maintainability practices for enterprise software development</li> <li>Integration with enterprise infrastructure including monitoring, logging, and deployment</li> </ul>"},{"location":"day07_mock/#project-overview-real-time-road-network-intelligence-service","title":"Project Overview: Real-Time Road Network Intelligence Service","text":""},{"location":"day07_mock/#business-context","title":"Business Context","text":"<p>You are building a core infrastructure service for a logistics company that needs real-time access to road network data for: - Route optimization algorithms requiring sub-second response times - Fleet management systems tracking thousands of vehicles - Dynamic pricing models based on traffic and road conditions - Regulatory compliance for commercial vehicle routing - Emergency response coordination requiring highest availability</p>"},{"location":"day07_mock/#technical-requirements","title":"Technical Requirements","text":"<p>Performance Targets: - Query latency: p95 &lt; 100ms, p99 &lt; 200ms for bounding box queries - Throughput: 1,000+ QPS sustained load, 5,000+ QPS peak capacity - Availability: 99.9% uptime with graceful degradation - Data consistency: Strong consistency for updates, eventual consistency for reads - Scalability: Horizontal scaling to support 10x traffic growth</p> <p>Functional Requirements: - Spatial queries: Efficient bounding box queries with pagination - Network topology: Connected road analysis for routing algorithms - Dynamic updates: Real-time road condition and metadata updates - Multi-format support: GeoJSON for web clients, Protocol Buffers for internal services - Caching strategy: Multi-tier caching with spatial awareness</p>"},{"location":"day07_mock/#architecture-deep-dive","title":"Architecture Deep Dive","text":""},{"location":"day07_mock/#system-design-overview","title":"System Design Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Load Balancer \u2502    \u2502   API Gateway   \u2502    \u2502   Web Clients   \u2502\n\u2502   (HAProxy)     \u2502\u2500\u2500\u2500\u2500\u2502   (Nginx)       \u2502\u2500\u2500\u2500\u2500\u2502   (React SPA)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u2502                       \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                       \u2502              \u2502  Mobile Apps    \u2502\n         \u2502                       \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  (iOS/Android)  \u2502\n         \u2502                       \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502\n         \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502              \u2502   CDN/Edge      \u2502\n         \u2502              \u2502   (CloudFlare)  \u2502\n         \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502\n         \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    FastAPI Application Layer                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Spatial API   \u2502   Network API   \u2502   Updates API   \u2502 Admin API \u2502\n\u2502   Service       \u2502   Service       \u2502   Service       \u2502 Service   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   Spatial       \u2502   Graph         \u2502   Event         \u2502 Config    \u2502\n\u2502   Queries       \u2502   Analysis      \u2502   Processing    \u2502 Mgmt      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502                       \u2502\n         \u25bc                       \u25bc                       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Redis Cache   \u2502    \u2502   PostgreSQL    \u2502    \u2502   Monitoring    \u2502\n\u2502   (L1/L2)       \u2502    \u2502   + PostGIS     \u2502    \u2502   (Prometheus)  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502\u2022 Spatial tiles  \u2502    \u2502\u2022 Road segments  \u2502    \u2502\u2022 Metrics        \u2502\n\u2502\u2022 Query results  \u2502    \u2502\u2022 Spatial index  \u2502    \u2502\u2022 Tracing        \u2502\n\u2502\u2022 Metadata       \u2502    \u2502\u2022 Audit log      \u2502    \u2502\u2022 Alerting       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"day07_mock/#data-model-design","title":"Data Model Design","text":"<pre><code>from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, Text, Index\nfrom sqlalchemy.dialects.postgresql import UUID, JSONB\nfrom geoalchemy2 import Geometry\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship\nimport uuid\nfrom datetime import datetime\n\nBase = declarative_base()\n\nclass RoadSegment(Base):\n    \"\"\"Enterprise-grade road segment model with comprehensive metadata.\"\"\"\n\n    __tablename__ = 'road_segments'\n\n    # Primary identification\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    external_id = Column(String(100), unique=True, nullable=False)  # From data provider\n\n    # Basic attributes\n    name = Column(String(255), nullable=True)\n    road_type = Column(String(50), nullable=False)  # highway, arterial, residential, etc.\n    speed_limit_kmh = Column(Integer, nullable=True)\n    direction = Column(String(20), nullable=False)  # bidirectional, forward, backward\n\n    # Geometry (using PostGIS)\n    geometry = Column(Geometry('LINESTRING', srid=4326), nullable=False)\n    length_meters = Column(Float, nullable=True)\n\n    # Network topology\n    start_node_id = Column(UUID(as_uuid=True), nullable=True)\n    end_node_id = Column(UUID(as_uuid=True), nullable=True)\n\n    # Traffic and conditions\n    traffic_level = Column(String(20), default='unknown')  # low, medium, high, blocked\n    surface_type = Column(String(50), nullable=True)  # asphalt, concrete, gravel, etc.\n    lanes_count = Column(Integer, nullable=True)\n\n    # Restrictions and regulations\n    truck_restricted = Column(Boolean, default=False)\n    hazmat_restricted = Column(Boolean, default=False)\n    height_limit_meters = Column(Float, nullable=True)\n    weight_limit_kg = Column(Float, nullable=True)\n\n    # Quality and metadata\n    data_source = Column(String(100), nullable=False)\n    accuracy_meters = Column(Float, nullable=True)\n    confidence_score = Column(Integer, default=50)  # 0-100\n\n    # Extended attributes (flexible schema)\n    attributes = Column(JSONB, nullable=True)\n\n    # Audit fields\n    created_at = Column(DateTime, default=datetime.utcnow, nullable=False)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n    created_by = Column(String(100), nullable=True)\n    updated_by = Column(String(100), nullable=True)\n    version = Column(Integer, default=1, nullable=False)\n\n    # Soft delete\n    is_active = Column(Boolean, default=True, nullable=False)\n    deleted_at = Column(DateTime, nullable=True)\n\n    # Spatial indexes for performance\n    __table_args__ = (\n        Index('idx_road_segments_geometry', geometry, postgresql_using='gist'),\n        Index('idx_road_segments_road_type', road_type),\n        Index('idx_road_segments_traffic_level', traffic_level),\n        Index('idx_road_segments_external_id', external_id),\n        Index('idx_road_segments_active', is_active),\n        Index('idx_road_segments_bbox', \n              'ST_Envelope(geometry)', postgresql_using='gist'),\n    )\n\nclass RoadNode(Base):\n    \"\"\"Junction/intersection points for network topology.\"\"\"\n\n    __tablename__ = 'road_nodes'\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    geometry = Column(Geometry('POINT', srid=4326), nullable=False)\n    node_type = Column(String(50), nullable=False)  # intersection, terminal, bridge\n    elevation_meters = Column(Float, nullable=True)\n\n    # Traffic control\n    has_traffic_light = Column(Boolean, default=False)\n    has_stop_sign = Column(Boolean, default=False)\n    is_roundabout = Column(Boolean, default=False)\n\n    # Audit fields\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n\n    __table_args__ = (\n        Index('idx_road_nodes_geometry', geometry, postgresql_using='gist'),\n        Index('idx_road_nodes_type', node_type),\n    )\n\nclass TrafficEvent(Base):\n    \"\"\"Real-time traffic events and incidents.\"\"\"\n\n    __tablename__ = 'traffic_events'\n\n    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)\n    road_segment_id = Column(UUID(as_uuid=True), nullable=False)\n\n    event_type = Column(String(50), nullable=False)  # accident, construction, closure\n    severity = Column(String(20), nullable=False)  # low, medium, high, critical\n    description = Column(Text, nullable=True)\n\n    # Temporal extent\n    start_time = Column(DateTime, nullable=False)\n    end_time = Column(DateTime, nullable=True)\n    estimated_duration_minutes = Column(Integer, nullable=True)\n\n    # Spatial extent\n    affected_geometry = Column(Geometry('LINESTRING', srid=4326), nullable=True)\n    impact_radius_meters = Column(Float, nullable=True)\n\n    # Impact assessment\n    delay_minutes = Column(Integer, nullable=True)\n    speed_reduction_percent = Column(Integer, nullable=True)\n    lanes_blocked = Column(Integer, nullable=True)\n\n    # Source and verification\n    source = Column(String(100), nullable=False)\n    verified = Column(Boolean, default=False)\n\n    # Audit fields\n    created_at = Column(DateTime, default=datetime.utcnow)\n    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)\n\n    __table_args__ = (\n        Index('idx_traffic_events_road_segment', road_segment_id),\n        Index('idx_traffic_events_time_range', start_time, end_time),\n        Index('idx_traffic_events_severity', severity),\n        Index('idx_traffic_events_active', \n              \"start_time &lt;= NOW() AND (end_time IS NULL OR end_time &gt;= NOW())\"),\n    )\n</code></pre>"},{"location":"day07_mock/#service-layer-architecture","title":"Service Layer Architecture","text":"<pre><code>from abc import ABC, abstractmethod\nfrom typing import List, Dict, Optional, Tuple, AsyncIterator\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport asyncio\nfrom contextlib import asynccontextmanager\n\n@dataclass\nclass BoundingBox:\n    \"\"\"Type-safe bounding box with validation.\"\"\"\n    min_lat: float\n    min_lon: float\n    max_lat: float\n    max_lon: float\n\n    def __post_init__(self):\n        if not (-90 &lt;= self.min_lat &lt;= 90) or not (-90 &lt;= self.max_lat &lt;= 90):\n            raise ValueError(\"Latitude must be between -90 and 90\")\n        if not (-180 &lt;= self.min_lon &lt;= 180) or not (-180 &lt;= self.max_lon &lt;= 180):\n            raise ValueError(\"Longitude must be between -180 and 180\")\n        if self.min_lat &gt;= self.max_lat:\n            raise ValueError(\"min_lat must be less than max_lat\")\n        if self.min_lon &gt;= self.max_lon:\n            raise ValueError(\"min_lon must be less than max_lon\")\n\n    @property\n    def area(self) -&gt; float:\n        \"\"\"Calculate bounding box area in square degrees.\"\"\"\n        return (self.max_lat - self.min_lat) * (self.max_lon - self.min_lon)\n\n    @property\n    def center(self) -&gt; Tuple[float, float]:\n        \"\"\"Get center point as (lat, lon).\"\"\"\n        return (\n            (self.min_lat + self.max_lat) / 2,\n            (self.min_lon + self.max_lon) / 2\n        )\n\n@dataclass\nclass SpatialQueryOptions:\n    \"\"\"Configuration for spatial queries.\"\"\"\n    limit: int = 1000\n    offset: int = 0\n    include_geometry: bool = True\n    simplify_tolerance: Optional[float] = None\n    buffer_meters: Optional[float] = None\n\n    def __post_init__(self):\n        if self.limit &lt;= 0 or self.limit &gt; 10000:\n            raise ValueError(\"Limit must be between 1 and 10000\")\n        if self.offset &lt; 0:\n            raise ValueError(\"Offset must be non-negative\")\n\nclass RoadNetworkService:\n    \"\"\"Enterprise road network service with comprehensive functionality.\"\"\"\n\n    def __init__(self, \n                 database_pool: asyncpg.Pool,\n                 cache_manager: SpatialCacheManager,\n                 metrics_collector: SpatialObservabilityManager):\n        self.db_pool = database_pool\n        self.cache = cache_manager\n        self.metrics = metrics_collector\n        self.graph_analyzer = RoadNetworkAnalyzer()\n\n    async def query_roads_in_bbox(self, \n                                 bbox: BoundingBox,\n                                 options: SpatialQueryOptions = None) -&gt; Dict:\n        \"\"\"Query roads within bounding box with enterprise features.\"\"\"\n        options = options or SpatialQueryOptions()\n\n        with self.metrics.trace_spatial_operation(\"bbox_query\", \n                                                 bbox=(bbox.min_lon, bbox.min_lat, \n                                                      bbox.max_lon, bbox.max_lat)):\n            # Check cache first\n            cache_key = self._generate_cache_key(bbox, options)\n            cached_result = await self.cache.get_spatial_data(\n                (bbox.min_lon, bbox.min_lat, bbox.max_lon, bbox.max_lat),\n                zoom_level=self._infer_zoom_level(bbox.area),\n                layer=\"roads\"\n            )\n\n            if cached_result:\n                self.metrics.record_cache_operation(\"spatial\", \"get\", \"hit\")\n                return cached_result\n\n            # Query database\n            self.metrics.record_cache_operation(\"spatial\", \"get\", \"miss\")\n            result = await self._execute_spatial_query(bbox, options)\n\n            # Cache result\n            await self.cache.set_spatial_data(\n                (bbox.min_lon, bbox.min_lat, bbox.max_lon, bbox.max_lat),\n                zoom_level=self._infer_zoom_level(bbox.area),\n                layer=\"roads\",\n                data=result,\n                ttl_seconds=self._calculate_cache_ttl(bbox.area)\n            )\n\n            return result\n\n    async def find_connected_roads(self, road_id: str, \n                                  max_distance_km: float = 5.0) -&gt; List[str]:\n        \"\"\"Find roads connected to a given road segment.\"\"\"\n\n        with self.metrics.trace_spatial_operation(\"connectivity_analysis\"):\n            # Get the target road segment\n            async with self.db_pool.acquire() as conn:\n                road_query = \"\"\"\n                SELECT id, start_node_id, end_node_id, ST_AsGeoJSON(geometry) as geometry\n                FROM road_segments \n                WHERE id = $1 AND is_active = true\n                \"\"\"\n                road = await conn.fetchrow(road_query, road_id)\n\n                if not road:\n                    raise ValueError(f\"Road segment {road_id} not found\")\n\n                # Find connected roads through shared nodes\n                connected_query = \"\"\"\n                WITH target_nodes AS (\n                    SELECT start_node_id as node_id FROM road_segments WHERE id = $1\n                    UNION\n                    SELECT end_node_id as node_id FROM road_segments WHERE id = $1\n                ),\n                connected_roads AS (\n                    SELECT DISTINCT rs.id, rs.name, \n                           ST_Distance(rs.geometry::geography, target.geometry::geography) as distance_meters\n                    FROM road_segments rs\n                    CROSS JOIN (SELECT geometry FROM road_segments WHERE id = $1) target\n                    WHERE rs.is_active = true \n                      AND rs.id != $1\n                      AND (rs.start_node_id IN (SELECT node_id FROM target_nodes)\n                           OR rs.end_node_id IN (SELECT node_id FROM target_nodes)\n                           OR ST_DWithin(rs.geometry::geography, target.geometry::geography, $2))\n                )\n                SELECT id FROM connected_roads \n                WHERE distance_meters &lt;= $2\n                ORDER BY distance_meters\n                LIMIT 100\n                \"\"\"\n\n                connected_roads = await conn.fetch(\n                    connected_query, road_id, max_distance_km * 1000\n                )\n\n                return [str(row['id']) for row in connected_roads]\n\n    async def update_road_metadata(self, road_id: str, updates: Dict) -&gt; Dict:\n        \"\"\"Update road segment metadata with validation and audit trail.\"\"\"\n\n        with self.metrics.trace_spatial_operation(\"road_update\"):\n            # Validate updates\n            allowed_fields = {\n                'speed_limit_kmh', 'traffic_level', 'surface_type',\n                'truck_restricted', 'hazmat_restricted', 'attributes'\n            }\n\n            invalid_fields = set(updates.keys()) - allowed_fields\n            if invalid_fields:\n                raise ValueError(f\"Invalid fields: {invalid_fields}\")\n\n            # Validate values\n            if 'speed_limit_kmh' in updates:\n                speed_limit = updates['speed_limit_kmh']\n                if not isinstance(speed_limit, int) or speed_limit &lt; 0 or speed_limit &gt; 200:\n                    raise ValueError(\"Speed limit must be between 0 and 200 km/h\")\n\n            if 'traffic_level' in updates:\n                valid_levels = {'low', 'medium', 'high', 'blocked'}\n                if updates['traffic_level'] not in valid_levels:\n                    raise ValueError(f\"Traffic level must be one of: {valid_levels}\")\n\n            # Perform update with optimistic locking\n            async with self.db_pool.acquire() as conn:\n                async with conn.transaction():\n                    # Get current version\n                    current = await conn.fetchrow(\n                        \"SELECT version FROM road_segments WHERE id = $1 AND is_active = true\",\n                        road_id\n                    )\n\n                    if not current:\n                        raise ValueError(f\"Road segment {road_id} not found\")\n\n                    # Build update query\n                    set_clauses = []\n                    params = [road_id, current['version']]\n                    param_idx = 3\n\n                    for field, value in updates.items():\n                        set_clauses.append(f\"{field} = ${param_idx}\")\n                        params.append(value)\n                        param_idx += 1\n\n                    set_clauses.extend([\n                        f\"updated_at = NOW()\",\n                        f\"version = version + 1\"\n                    ])\n\n                    update_query = f\"\"\"\n                    UPDATE road_segments \n                    SET {', '.join(set_clauses)}\n                    WHERE id = $1 AND version = $2 AND is_active = true\n                    RETURNING id, version, updated_at\n                    \"\"\"\n\n                    result = await conn.fetchrow(update_query, *params)\n\n                    if not result:\n                        raise ValueError(\"Update failed - segment may have been modified\")\n\n                    # Invalidate cache\n                    await self._invalidate_road_cache(road_id)\n\n                    # Log audit event\n                    self.metrics.log_spatial_event(\n                        \"road_updated\",\n                        road_id=road_id,\n                        updates=updates,\n                        new_version=result['version']\n                    )\n\n                    return {\n                        'id': str(result['id']),\n                        'version': result['version'],\n                        'updated_at': result['updated_at'].isoformat()\n                    }\n\n    async def stream_roads_in_region(self, \n                                   bbox: BoundingBox,\n                                   batch_size: int = 1000) -&gt; AsyncIterator[List[Dict]]:\n        \"\"\"Stream roads in large regions efficiently.\"\"\"\n\n        with self.metrics.trace_spatial_operation(\"streaming_query\"):\n            offset = 0\n\n            while True:\n                options = SpatialQueryOptions(\n                    limit=batch_size,\n                    offset=offset,\n                    include_geometry=True\n                )\n\n                batch = await self.query_roads_in_bbox(bbox, options)\n                features = batch.get('features', [])\n\n                if not features:\n                    break\n\n                yield features\n\n                if len(features) &lt; batch_size:\n                    break\n\n                offset += batch_size\n\n                # Prevent runaway queries\n                if offset &gt; 100000:\n                    raise ValueError(\"Query too large - use smaller bounding box\")\n\n    async def _execute_spatial_query(self, bbox: BoundingBox, \n                                   options: SpatialQueryOptions) -&gt; Dict:\n        \"\"\"Execute optimized spatial query against database.\"\"\"\n\n        query = \"\"\"\n        SELECT \n            id,\n            external_id,\n            name,\n            road_type,\n            speed_limit_kmh,\n            direction,\n            traffic_level,\n            surface_type,\n            lanes_count,\n            truck_restricted,\n            hazmat_restricted,\n            length_meters,\n            confidence_score,\n            attributes,\n            created_at,\n            updated_at\n        \"\"\"\n\n        if options.include_geometry:\n            if options.simplify_tolerance:\n                query += f\", ST_AsGeoJSON(ST_Simplify(geometry, {options.simplify_tolerance})) as geometry\"\n            else:\n                query += \", ST_AsGeoJSON(geometry) as geometry\"\n\n        query += \"\"\"\n        FROM road_segments\n        WHERE is_active = true\n          AND geometry &amp;&amp; ST_MakeEnvelope($1, $2, $3, $4, 4326)\n          AND ST_Intersects(geometry, ST_MakeEnvelope($1, $2, $3, $4, 4326))\n        ORDER BY road_type, name\n        LIMIT $5 OFFSET $6\n        \"\"\"\n\n        # Count query for total\n        count_query = \"\"\"\n        SELECT COUNT(*) as total\n        FROM road_segments\n        WHERE is_active = true\n          AND geometry &amp;&amp; ST_MakeEnvelope($1, $2, $3, $4, 4326)\n          AND ST_Intersects(geometry, ST_MakeEnvelope($1, $2, $3, $4, 4326))\n        \"\"\"\n\n        async with self.db_pool.acquire() as conn:\n            # Execute queries in parallel\n            roads_task = conn.fetch(\n                query, \n                bbox.min_lon, bbox.min_lat, bbox.max_lon, bbox.max_lat,\n                options.limit, options.offset\n            )\n\n            count_task = conn.fetchval(\n                count_query,\n                bbox.min_lon, bbox.min_lat, bbox.max_lon, bbox.max_lat\n            )\n\n            roads, total_count = await asyncio.gather(roads_task, count_task)\n\n            # Convert to GeoJSON format\n            features = []\n            for road in roads:\n                feature = {\n                    'type': 'Feature',\n                    'id': str(road['id']),\n                    'properties': {\n                        'external_id': road['external_id'],\n                        'name': road['name'],\n                        'road_type': road['road_type'],\n                        'speed_limit_kmh': road['speed_limit_kmh'],\n                        'direction': road['direction'],\n                        'traffic_level': road['traffic_level'],\n                        'surface_type': road['surface_type'],\n                        'lanes_count': road['lanes_count'],\n                        'truck_restricted': road['truck_restricted'],\n                        'hazmat_restricted': road['hazmat_restricted'],\n                        'length_meters': road['length_meters'],\n                        'confidence_score': road['confidence_score'],\n                        'attributes': road['attributes'],\n                        'created_at': road['created_at'].isoformat() if road['created_at'] else None,\n                        'updated_at': road['updated_at'].isoformat() if road['updated_at'] else None\n                    }\n                }\n\n                if options.include_geometry and road['geometry']:\n                    import json\n                    feature['geometry'] = json.loads(road['geometry'])\n\n                features.append(feature)\n\n            return {\n                'type': 'FeatureCollection',\n                'features': features,\n                'count': len(features),\n                'total': total_count,\n                'bbox': [bbox.min_lon, bbox.min_lat, bbox.max_lon, bbox.max_lat],\n                'limit': options.limit,\n                'offset': options.offset\n            }\n</code></pre>"},{"location":"day07_mock/#api-implementation","title":"API Implementation","text":"<pre><code>from fastapi import FastAPI, HTTPException, Depends, Query, Path, status\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.middleware.gzip import GZipMiddleware\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel, Field, validator\nfrom typing import Optional, List, Dict, Any\nimport uvicorn\nimport asyncio\nimport json\n\nclass SpatialQueryRequest(BaseModel):\n    \"\"\"Request model for spatial queries with validation.\"\"\"\n\n    min_lat: float = Field(..., ge=-90, le=90, description=\"Minimum latitude\")\n    min_lon: float = Field(..., ge=-180, le=180, description=\"Minimum longitude\")\n    max_lat: float = Field(..., ge=-90, le=90, description=\"Maximum latitude\")\n    max_lon: float = Field(..., ge=-180, le=180, description=\"Maximum longitude\")\n\n    limit: int = Field(1000, ge=1, le=10000, description=\"Maximum number of results\")\n    offset: int = Field(0, ge=0, description=\"Number of results to skip\")\n\n    include_geometry: bool = Field(True, description=\"Include geometry in response\")\n    simplify_tolerance: Optional[float] = Field(None, ge=0, le=0.1, \n                                               description=\"Geometry simplification tolerance\")\n\n    @validator('max_lat')\n    def validate_lat_range(cls, v, values):\n        if 'min_lat' in values and v &lt;= values['min_lat']:\n            raise ValueError('max_lat must be greater than min_lat')\n        return v\n\n    @validator('max_lon')\n    def validate_lon_range(cls, v, values):\n        if 'min_lon' in values and v &lt;= values['min_lon']:\n            raise ValueError('max_lon must be greater than min_lon')\n        return v\n\nclass RoadUpdateRequest(BaseModel):\n    \"\"\"Request model for road updates.\"\"\"\n\n    speed_limit_kmh: Optional[int] = Field(None, ge=0, le=200)\n    traffic_level: Optional[str] = Field(None, regex='^(low|medium|high|blocked)$')\n    surface_type: Optional[str] = Field(None, max_length=50)\n    truck_restricted: Optional[bool] = None\n    hazmat_restricted: Optional[bool] = None\n    attributes: Optional[Dict[str, Any]] = None\n\nclass APIResponse(BaseModel):\n    \"\"\"Standard API response wrapper.\"\"\"\n\n    success: bool = True\n    data: Any = None\n    message: str = \"\"\n    request_id: Optional[str] = None\n    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())\n\n# FastAPI application setup\napp = FastAPI(\n    title=\"Road Network Intelligence Service\",\n    description=\"Enterprise-grade road network API for logistics and navigation\",\n    version=\"1.0.0\",\n    docs_url=\"/docs\",\n    redoc_url=\"/redoc\"\n)\n\n# Middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],  # Configure for production\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\napp.add_middleware(GZipMiddleware, minimum_size=1000)\n\n# Global dependencies\nasync def get_road_service() -&gt; RoadNetworkService:\n    \"\"\"Dependency injection for road network service.\"\"\"\n    # This would be initialized with proper database pool and cache\n    return RoadNetworkService(db_pool, cache_manager, metrics_collector)\n\n@app.get(\"/api/v1/roads/bbox\", \n         response_model=APIResponse,\n         summary=\"Query roads in bounding box\",\n         description=\"Retrieve road segments within a geographic bounding box\")\nasync def query_roads_bbox(\n    min_lat: float = Query(..., ge=-90, le=90, description=\"Minimum latitude\"),\n    min_lon: float = Query(..., ge=-180, le=180, description=\"Minimum longitude\"), \n    max_lat: float = Query(..., ge=-90, le=90, description=\"Maximum latitude\"),\n    max_lon: float = Query(..., ge=-180, le=180, description=\"Maximum longitude\"),\n    limit: int = Query(1000, ge=1, le=10000, description=\"Maximum results\"),\n    offset: int = Query(0, ge=0, description=\"Results offset\"),\n    include_geometry: bool = Query(True, description=\"Include geometry\"),\n    simplify_tolerance: Optional[float] = Query(None, ge=0, le=0.1),\n    service: RoadNetworkService = Depends(get_road_service)\n):\n    \"\"\"Query roads within a bounding box with comprehensive options.\"\"\"\n\n    try:\n        # Validate bounding box\n        if max_lat &lt;= min_lat:\n            raise HTTPException(400, \"max_lat must be greater than min_lat\")\n        if max_lon &lt;= min_lon:\n            raise HTTPException(400, \"max_lon must be greater than min_lon\")\n\n        # Check area limit to prevent abuse\n        area = (max_lat - min_lat) * (max_lon - min_lon)\n        if area &gt; 1.0:  # Roughly 111km x 111km at equator\n            raise HTTPException(400, \"Bounding box too large - maximum 1 square degree\")\n\n        bbox = BoundingBox(min_lat, min_lon, max_lat, max_lon)\n        options = SpatialQueryOptions(\n            limit=limit,\n            offset=offset,\n            include_geometry=include_geometry,\n            simplify_tolerance=simplify_tolerance\n        )\n\n        result = await service.query_roads_in_bbox(bbox, options)\n\n        return APIResponse(\n            data=result,\n            message=f\"Found {result['count']} roads (total: {result['total']})\"\n        )\n\n    except ValueError as e:\n        raise HTTPException(400, str(e))\n    except Exception as e:\n        logger.error(f\"Bbox query failed: {e}\")\n        raise HTTPException(500, \"Internal server error\")\n\n@app.get(\"/api/v1/roads/{road_id}/connected\",\n         response_model=APIResponse,\n         summary=\"Find connected roads\",\n         description=\"Find roads connected to a specific road segment\")\nasync def get_connected_roads(\n    road_id: str = Path(..., description=\"Road segment ID\"),\n    max_distance_km: float = Query(5.0, ge=0.1, le=50.0, \n                                  description=\"Maximum search distance in kilometers\"),\n    service: RoadNetworkService = Depends(get_road_service)\n):\n    \"\"\"Find roads connected to a given road segment.\"\"\"\n\n    try:\n        connected_roads = await service.find_connected_roads(road_id, max_distance_km)\n\n        return APIResponse(\n            data={\n                'road_id': road_id,\n                'connected_roads': connected_roads,\n                'count': len(connected_roads),\n                'max_distance_km': max_distance_km\n            },\n            message=f\"Found {len(connected_roads)} connected roads\"\n        )\n\n    except ValueError as e:\n        raise HTTPException(404, str(e))\n    except Exception as e:\n        logger.error(f\"Connected roads query failed: {e}\")\n        raise HTTPException(500, \"Internal server error\")\n\n@app.patch(\"/api/v1/roads/{road_id}\",\n           response_model=APIResponse,\n           summary=\"Update road metadata\",\n           description=\"Update road segment metadata and attributes\")\nasync def update_road(\n    road_id: str = Path(..., description=\"Road segment ID\"),\n    updates: RoadUpdateRequest = ...,\n    service: RoadNetworkService = Depends(get_road_service)\n):\n    \"\"\"Update road segment metadata with validation and audit trail.\"\"\"\n\n    try:\n        # Convert to dict, excluding None values\n        update_data = {k: v for k, v in updates.dict().items() if v is not None}\n\n        if not update_data:\n            raise HTTPException(400, \"No valid updates provided\")\n\n        result = await service.update_road_metadata(road_id, update_data)\n\n        return APIResponse(\n            data=result,\n            message=\"Road updated successfully\"\n        )\n\n    except ValueError as e:\n        raise HTTPException(400, str(e))\n    except Exception as e:\n        logger.error(f\"Road update failed: {e}\")\n        raise HTTPException(500, \"Internal server error\")\n\n@app.get(\"/api/v1/roads/stream/bbox\",\n         summary=\"Stream roads in bounding box\",\n         description=\"Stream large result sets efficiently\")\nasync def stream_roads_bbox(\n    min_lat: float = Query(..., ge=-90, le=90),\n    min_lon: float = Query(..., ge=-180, le=180),\n    max_lat: float = Query(..., ge=-90, le=90),\n    max_lon: float = Query(..., ge=-180, le=180),\n    batch_size: int = Query(1000, ge=100, le=5000),\n    service: RoadNetworkService = Depends(get_road_service)\n):\n    \"\"\"Stream roads for large datasets using NDJSON format.\"\"\"\n\n    try:\n        bbox = BoundingBox(min_lat, min_lon, max_lat, max_lon)\n\n        async def generate_stream():\n            yield '{\"type\": \"stream_start\", \"bbox\": ' + json.dumps([min_lon, min_lat, max_lon, max_lat]) + '}\\n'\n\n            batch_count = 0\n            feature_count = 0\n\n            async for batch in service.stream_roads_in_region(bbox, batch_size):\n                batch_count += 1\n                feature_count += len(batch)\n\n                for feature in batch:\n                    yield json.dumps(feature) + '\\n'\n\n                # Progress indicator every 10 batches\n                if batch_count % 10 == 0:\n                    yield f'{{\"type\": \"progress\", \"batches\": {batch_count}, \"features\": {feature_count}}}\\n'\n\n            yield f'{{\"type\": \"stream_end\", \"total_features\": {feature_count}, \"total_batches\": {batch_count}}}\\n'\n\n        return StreamingResponse(\n            generate_stream(),\n            media_type=\"application/x-ndjson\",\n            headers={\n                \"Content-Disposition\": f\"attachment; filename=roads_{min_lat}_{min_lon}_{max_lat}_{max_lon}.ndjson\"\n            }\n        )\n\n    except ValueError as e:\n        raise HTTPException(400, str(e))\n    except Exception as e:\n        logger.error(f\"Streaming query failed: {e}\")\n        raise HTTPException(500, \"Internal server error\")\n\n@app.get(\"/health\",\n         summary=\"Health check\",\n         description=\"Service health status\")\nasync def health_check(service: RoadNetworkService = Depends(get_road_service)):\n    \"\"\"Comprehensive health check endpoint.\"\"\"\n\n    health_status = await service.check_system_health()\n\n    status_code = 200 if health_status['overall_status'] == 'healthy' else 503\n\n    return APIResponse(\n        data=health_status,\n        message=f\"Service is {health_status['overall_status']}\"\n    ), status_code\n\n@app.get(\"/metrics\",\n         summary=\"Prometheus metrics\",\n         description=\"Metrics endpoint for monitoring\")\nasync def get_metrics():\n    \"\"\"Prometheus metrics endpoint.\"\"\"\n    from prometheus_client import generate_latest, CONTENT_TYPE_LATEST\n\n    return Response(\n        generate_latest(),\n        media_type=CONTENT_TYPE_LATEST\n    )\n\n# Application lifecycle\n@app.on_event(\"startup\")\nasync def startup_event():\n    \"\"\"Initialize application resources.\"\"\"\n    logger.info(\"Starting Road Network Intelligence Service\")\n\n    # Initialize database pool\n    # Initialize cache manager  \n    # Initialize metrics collector\n    # Run health checks\n\n    logger.info(\"Service startup completed\")\n\n@app.on_event(\"shutdown\")\nasync def shutdown_event():\n    \"\"\"Cleanup application resources.\"\"\"\n    logger.info(\"Shutting down Road Network Intelligence Service\")\n\n    # Close database connections\n    # Close cache connections\n    # Flush metrics\n\n    logger.info(\"Service shutdown completed\")\n\nif __name__ == \"__main__\":\n    uvicorn.run(\n        \"api:app\",\n        host=\"0.0.0.0\",\n        port=8000,\n        reload=True,\n        log_level=\"info\"\n    )\n</code></pre>"},{"location":"day07_mock/#performance-engineering-and-optimization","title":"Performance Engineering and Optimization","text":""},{"location":"day07_mock/#database-optimization","title":"Database Optimization","text":"<pre><code>-- Comprehensive indexing strategy for spatial queries\nCREATE INDEX CONCURRENTLY idx_road_segments_spatial_optimized \nON road_segments USING GIST (geometry) \nWHERE is_active = true;\n\nCREATE INDEX CONCURRENTLY idx_road_segments_composite_lookup\nON road_segments (road_type, traffic_level, is_active, created_at)\nWHERE is_active = true;\n\n-- Materialized view for frequently accessed road statistics\nCREATE MATERIALIZED VIEW road_network_stats AS\nSELECT \n    road_type,\n    COUNT(*) as segment_count,\n    AVG(length_meters) as avg_length,\n    SUM(length_meters) as total_length,\n    ST_Extent(geometry) as overall_bounds,\n    AVG(confidence_score) as avg_confidence\nFROM road_segments \nWHERE is_active = true \nGROUP BY road_type;\n\nCREATE UNIQUE INDEX ON road_network_stats (road_type);\n\n-- Optimized query for bbox searches with explain analyze\nEXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)\nSELECT id, name, road_type, ST_AsGeoJSON(geometry) as geometry\nFROM road_segments\nWHERE is_active = true\n  AND geometry &amp;&amp; ST_MakeEnvelope(-122.5, 37.7, -122.4, 37.8, 4326)\n  AND ST_Intersects(geometry, ST_MakeEnvelope(-122.5, 37.7, -122.4, 37.8, 4326))\nORDER BY road_type, name\nLIMIT 1000;\n\n-- Partitioning strategy for large datasets\nCREATE TABLE road_segments_partitioned (\n    LIKE road_segments INCLUDING ALL\n) PARTITION BY RANGE (created_at);\n\nCREATE TABLE road_segments_2024_q1 PARTITION OF road_segments_partitioned\nFOR VALUES FROM ('2024-01-01') TO ('2024-04-01');\n\nCREATE TABLE road_segments_2024_q2 PARTITION OF road_segments_partitioned  \nFOR VALUES FROM ('2024-04-01') TO ('2024-07-01');\n</code></pre>"},{"location":"day07_mock/#caching-strategy-implementation","title":"Caching Strategy Implementation","text":"<pre><code>class RoadNetworkCacheManager:\n    \"\"\"Specialized caching for road network data.\"\"\"\n\n    def __init__(self, redis_client: redis.Redis):\n        self.redis = redis_client\n        self.local_cache = {}\n        self.cache_stats = defaultdict(int)\n\n    async def cache_road_segment(self, road_id: str, road_data: Dict, \n                               ttl_seconds: int = 3600):\n        \"\"\"Cache individual road segment with spatial indexing.\"\"\"\n\n        # Store in Redis with spatial information\n        cache_key = f\"road:{road_id}\"\n\n        # Add spatial metadata for cache invalidation\n        if 'geometry' in road_data:\n            geom = shape(road_data['geometry'])\n            bbox = geom.bounds\n\n            # Store road in spatial grid cells for invalidation\n            grid_cells = self._get_grid_cells(bbox)\n\n            pipeline = self.redis.pipeline()\n\n            # Store road data\n            pipeline.setex(cache_key, ttl_seconds, json.dumps(road_data))\n\n            # Add to spatial grid cells\n            for cell_id in grid_cells:\n                pipeline.sadd(f\"grid:{cell_id}\", road_id)\n                pipeline.expire(f\"grid:{cell_id}\", ttl_seconds + 300)\n\n            await pipeline.execute()\n\n    async def invalidate_spatial_region(self, bbox: Tuple[float, float, float, float]):\n        \"\"\"Invalidate cache for all roads in a spatial region.\"\"\"\n\n        grid_cells = self._get_grid_cells(bbox)\n        roads_to_invalidate = set()\n\n        # Collect all roads in affected grid cells\n        for cell_id in grid_cells:\n            cell_roads = await self.redis.smembers(f\"grid:{cell_id}\")\n            roads_to_invalidate.update(cell_roads)\n\n        # Remove from cache\n        if roads_to_invalidate:\n            cache_keys = [f\"road:{road_id}\" for road_id in roads_to_invalidate]\n            await self.redis.delete(*cache_keys)\n\n            self.cache_stats['invalidated'] += len(cache_keys)\n\n    def _get_grid_cells(self, bbox: Tuple[float, float, float, float], \n                       grid_size: float = 0.01) -&gt; List[str]:\n        \"\"\"Calculate grid cells that intersect with bounding box.\"\"\"\n        min_x, min_y, max_x, max_y = bbox\n\n        cells = []\n        x = min_x\n        while x &lt;= max_x:\n            y = min_y\n            while y &lt;= max_y:\n                cell_id = f\"{int(x/grid_size)}:{int(y/grid_size)}\"\n                cells.append(cell_id)\n                y += grid_size\n            x += grid_size\n\n        return cells\n\nclass ConnectionPoolManager:\n    \"\"\"Optimized database connection management.\"\"\"\n\n    def __init__(self):\n        self.pools = {}\n        self.pool_stats = defaultdict(int)\n\n    async def create_pool(self, database_url: str, pool_name: str = \"default\"):\n        \"\"\"Create optimized connection pool for spatial queries.\"\"\"\n\n        pool = await asyncpg.create_pool(\n            database_url,\n            min_size=5,\n            max_size=20,\n            max_queries=50000,\n            max_inactive_connection_lifetime=300,\n            command_timeout=30,\n            server_settings={\n                'jit': 'off',  # Disable JIT for consistent performance\n                'application_name': f'road_network_service_{pool_name}',\n                'shared_preload_libraries': 'pg_stat_statements,auto_explain',\n                'auto_explain.log_min_duration': '1000ms',\n                'auto_explain.log_analyze': 'true'\n            }\n        )\n\n        self.pools[pool_name] = pool\n        return pool\n\n    @asynccontextmanager\n    async def get_connection(self, pool_name: str = \"default\"):\n        \"\"\"Get connection with automatic retry and monitoring.\"\"\"\n\n        if pool_name not in self.pools:\n            raise ValueError(f\"Pool {pool_name} not found\")\n\n        pool = self.pools[pool_name]\n        start_time = time.time()\n\n        try:\n            async with pool.acquire() as conn:\n                # Set spatial query optimizations\n                await conn.execute(\"SET enable_seqscan = false\")\n                await conn.execute(\"SET work_mem = '256MB'\")\n                await conn.execute(\"SET effective_cache_size = '4GB'\")\n\n                self.pool_stats[f'{pool_name}_acquired'] += 1\n                yield conn\n\n        except Exception as e:\n            self.pool_stats[f'{pool_name}_errors'] += 1\n            raise\n        finally:\n            acquisition_time = time.time() - start_time\n            self.pool_stats[f'{pool_name}_avg_acquisition_time'] = (\n                self.pool_stats.get(f'{pool_name}_avg_acquisition_time', 0) * 0.9 +\n                acquisition_time * 0.1\n            )\n</code></pre>"},{"location":"day07_mock/#production-deployment-and-operations","title":"Production Deployment and Operations","text":""},{"location":"day07_mock/#docker-configuration","title":"Docker Configuration","text":"<pre><code># Multi-stage build for production optimization\nFROM python:3.11-slim as builder\n\n# Install system dependencies for spatial libraries\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libgdal-dev \\\n    libgeos-dev \\\n    libproj-dev \\\n    gcc \\\n    g++ \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Set GDAL environment variables\nENV GDAL_CONFIG=/usr/bin/gdal-config\nENV GEOS_CONFIG=/usr/bin/geos-config\n\n# Create virtual environment\nRUN python -m venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Production stage\nFROM python:3.11-slim\n\n# Install runtime dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    libgdal28 \\\n    libgeos-c1v5 \\\n    libproj19 \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy virtual environment\nCOPY --from=builder /opt/venv /opt/venv\nENV PATH=\"/opt/venv/bin:$PATH\"\n\n# Create non-root user\nRUN groupadd -r appuser &amp;&amp; useradd -r -g appuser appuser\n\n# Set working directory\nWORKDIR /app\n\n# Copy application code\nCOPY src/ ./src/\nCOPY alembic.ini .\nCOPY alembic/ ./alembic/\n\n# Set ownership\nRUN chown -R appuser:appuser /app\n\n# Switch to non-root user\nUSER appuser\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\\n    CMD curl -f http://localhost:8000/health || exit 1\n\n# Expose port\nEXPOSE 8000\n\n# Production command\nCMD [\"gunicorn\", \"src.day07_mock.api:app\", \"-w\", \"4\", \"-k\", \"uvicorn.workers.UvicornWorker\", \"--bind\", \"0.0.0.0:8000\"]\n</code></pre>"},{"location":"day07_mock/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># kubernetes/deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: road-network-service\n  labels:\n    app: road-network-service\n    version: v1.0.0\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: road-network-service\n  template:\n    metadata:\n      labels:\n        app: road-network-service\n        version: v1.0.0\n    spec:\n      containers:\n      - name: api\n        image: road-network-service:1.0.0\n        ports:\n        - containerPort: 8000\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: url\n        - name: REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: redis-secret\n              key: url\n        - name: LOG_LEVEL\n          value: \"INFO\"\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"2Gi\"\n            cpu: \"2000m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /app/config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: road-network-config\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: road-network-service\nspec:\n  selector:\n    app: road-network-service\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8000\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: road-network-ingress\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/rate-limit: \"1000\"\n    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\nspec:\n  tls:\n  - hosts:\n    - api.roadnetwork.company.com\n    secretName: tls-secret\n  rules:\n  - host: api.roadnetwork.company.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: road-network-service\n            port:\n              number: 80\n</code></pre>"},{"location":"day07_mock/#testing-strategy-and-quality-assurance","title":"Testing Strategy and Quality Assurance","text":""},{"location":"day07_mock/#comprehensive-test-suite","title":"Comprehensive Test Suite","text":"<pre><code>import pytest\nimport asyncio\nfrom httpx import AsyncClient\nfrom unittest.mock import AsyncMock, patch\nimport json\n\nclass TestRoadNetworkAPI:\n    \"\"\"Comprehensive API test suite.\"\"\"\n\n    @pytest.fixture\n    async def client(self):\n        \"\"\"Test client with mocked dependencies.\"\"\"\n\n        # Mock database pool\n        mock_pool = AsyncMock()\n        mock_conn = AsyncMock()\n        mock_pool.acquire.return_value.__aenter__.return_value = mock_conn\n\n        # Mock cache manager\n        mock_cache = AsyncMock()\n\n        # Mock metrics collector\n        mock_metrics = AsyncMock()\n\n        with patch('src.day07_mock.api.get_road_service') as mock_service:\n            service_instance = RoadNetworkService(mock_pool, mock_cache, mock_metrics)\n            mock_service.return_value = service_instance\n\n            async with AsyncClient(app=app, base_url=\"http://test\") as ac:\n                yield ac, service_instance, mock_conn\n\n    @pytest.mark.asyncio\n    async def test_bbox_query_success(self, client):\n        \"\"\"Test successful bounding box query.\"\"\"\n\n        ac, service, mock_conn = client\n\n        # Mock database response\n        mock_conn.fetch.return_value = [\n            {\n                'id': '123e4567-e89b-12d3-a456-426614174000',\n                'external_id': 'road_001',\n                'name': 'Main Street',\n                'road_type': 'arterial',\n                'speed_limit_kmh': 50,\n                'direction': 'bidirectional',\n                'traffic_level': 'medium',\n                'surface_type': 'asphalt',\n                'lanes_count': 2,\n                'truck_restricted': False,\n                'hazmat_restricted': False,\n                'length_meters': 1500.0,\n                'confidence_score': 85,\n                'attributes': {'surface_condition': 'good'},\n                'created_at': datetime(2024, 1, 1),\n                'updated_at': datetime(2024, 1, 1),\n                'geometry': '{\"type\":\"LineString\",\"coordinates\":[[-122.4,37.7],[-122.39,37.71]]}'\n            }\n        ]\n\n        mock_conn.fetchval.return_value = 1  # Total count\n\n        response = await ac.get(\"/api/v1/roads/bbox\", params={\n            'min_lat': 37.7,\n            'min_lon': -122.4,\n            'max_lat': 37.8,\n            'max_lon': -122.3,\n            'limit': 100\n        })\n\n        assert response.status_code == 200\n        data = response.json()\n\n        assert data['success'] is True\n        assert 'data' in data\n        assert data['data']['type'] == 'FeatureCollection'\n        assert len(data['data']['features']) == 1\n\n        feature = data['data']['features'][0]\n        assert feature['type'] == 'Feature'\n        assert feature['properties']['name'] == 'Main Street'\n        assert feature['properties']['road_type'] == 'arterial'\n        assert 'geometry' in feature\n\n    @pytest.mark.asyncio\n    async def test_bbox_query_validation_errors(self, client):\n        \"\"\"Test bounding box validation errors.\"\"\"\n\n        ac, _, _ = client\n\n        # Invalid latitude range\n        response = await ac.get(\"/api/v1/roads/bbox\", params={\n            'min_lat': 91,  # Invalid\n            'min_lon': -122.4,\n            'max_lat': 37.8,\n            'max_lon': -122.3\n        })\n        assert response.status_code == 422\n\n        # Invalid bounding box (min &gt;= max)\n        response = await ac.get(\"/api/v1/roads/bbox\", params={\n            'min_lat': 37.8,\n            'min_lon': -122.3,\n            'max_lat': 37.7,  # Less than min_lat\n            'max_lon': -122.4\n        })\n        assert response.status_code == 400\n\n        # Bounding box too large\n        response = await ac.get(\"/api/v1/roads/bbox\", params={\n            'min_lat': 37.0,\n            'min_lon': -123.0,\n            'max_lat': 38.5,  # More than 1 degree difference\n            'max_lon': -121.5\n        })\n        assert response.status_code == 400\n\n    @pytest.mark.asyncio\n    async def test_connected_roads_query(self, client):\n        \"\"\"Test connected roads functionality.\"\"\"\n\n        ac, service, mock_conn = client\n\n        road_id = '123e4567-e89b-12d3-a456-426614174000'\n\n        # Mock road exists check\n        mock_conn.fetchrow.return_value = {\n            'id': road_id,\n            'start_node_id': 'node_1',\n            'end_node_id': 'node_2',\n            'geometry': '{\"type\":\"LineString\",\"coordinates\":[[-122.4,37.7],[-122.39,37.71]]}'\n        }\n\n        # Mock connected roads\n        mock_conn.fetch.return_value = [\n            {'id': 'connected_road_1'},\n            {'id': 'connected_road_2'}\n        ]\n\n        response = await ac.get(f\"/api/v1/roads/{road_id}/connected\")\n\n        assert response.status_code == 200\n        data = response.json()\n\n        assert data['success'] is True\n        assert data['data']['road_id'] == road_id\n        assert len(data['data']['connected_roads']) == 2\n        assert 'connected_road_1' in data['data']['connected_roads']\n\n    @pytest.mark.asyncio\n    async def test_road_update_success(self, client):\n        \"\"\"Test successful road update.\"\"\"\n\n        ac, service, mock_conn = client\n\n        road_id = '123e4567-e89b-12d3-a456-426614174000'\n\n        # Mock current version check\n        mock_conn.fetchrow.side_effect = [\n            {'version': 1},  # Current version\n            {  # Update result\n                'id': road_id,\n                'version': 2,\n                'updated_at': datetime(2024, 1, 2)\n            }\n        ]\n\n        update_data = {\n            'speed_limit_kmh': 60,\n            'traffic_level': 'high'\n        }\n\n        response = await ac.patch(f\"/api/v1/roads/{road_id}\", json=update_data)\n\n        assert response.status_code == 200\n        data = response.json()\n\n        assert data['success'] is True\n        assert data['data']['id'] == road_id\n        assert data['data']['version'] == 2\n\n    @pytest.mark.asyncio\n    async def test_road_update_validation_errors(self, client):\n        \"\"\"Test road update validation.\"\"\"\n\n        ac, _, _ = client\n\n        road_id = '123e4567-e89b-12d3-a456-426614174000'\n\n        # Invalid speed limit\n        response = await ac.patch(f\"/api/v1/roads/{road_id}\", json={\n            'speed_limit_kmh': 250  # Too high\n        })\n        assert response.status_code == 422\n\n        # Invalid traffic level\n        response = await ac.patch(f\"/api/v1/roads/{road_id}\", json={\n            'traffic_level': 'invalid_level'\n        })\n        assert response.status_code == 422\n\n    @pytest.mark.asyncio\n    async def test_health_check(self, client):\n        \"\"\"Test health check endpoint.\"\"\"\n\n        ac, service, _ = client\n\n        # Mock health check response\n        with patch.object(service, 'check_system_health') as mock_health:\n            mock_health.return_value = {\n                'overall_status': 'healthy',\n                'components': {\n                    'database': {'status': 'healthy'},\n                    'cache': {'status': 'healthy'}\n                }\n            }\n\n            response = await ac.get(\"/health\")\n\n            assert response.status_code == 200\n            data = response.json()\n\n            assert data['success'] is True\n            assert data['data']['overall_status'] == 'healthy'\n\nclass TestSpatialQueries:\n    \"\"\"Spatial query performance and correctness tests.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_large_bbox_query_performance(self):\n        \"\"\"Test performance with large bounding box queries.\"\"\"\n\n        # This would use a real test database with sample data\n        service = await create_test_service()\n\n        bbox = BoundingBox(37.0, -122.5, 38.0, -121.5)\n        options = SpatialQueryOptions(limit=5000)\n\n        start_time = time.time()\n        result = await service.query_roads_in_bbox(bbox, options)\n        duration = time.time() - start_time\n\n        # Performance assertion\n        assert duration &lt; 1.0  # Should complete within 1 second\n        assert result['count'] &lt;= 5000\n        assert 'features' in result\n\n    @pytest.mark.asyncio  \n    async def test_spatial_accuracy(self):\n        \"\"\"Test spatial query accuracy and edge cases.\"\"\"\n\n        service = await create_test_service()\n\n        # Test exact boundary conditions\n        bbox = BoundingBox(37.7749, -122.4194, 37.7750, -122.4193)\n        result = await service.query_roads_in_bbox(bbox, SpatialQueryOptions())\n\n        # Verify all returned roads actually intersect the bbox\n        for feature in result['features']:\n            geom = shape(feature['geometry'])\n            bbox_geom = box(bbox.min_lon, bbox.min_lat, bbox.max_lon, bbox.max_lat)\n            assert geom.intersects(bbox_geom)\n\n# Load testing with locust\nclass RoadNetworkLoadTest(HttpUser):\n    \"\"\"Load testing scenarios for road network API.\"\"\"\n\n    wait_time = between(1, 3)\n\n    def on_start(self):\n        \"\"\"Setup for load test user.\"\"\"\n        self.sf_bbox = {\n            'min_lat': 37.7,\n            'min_lon': -122.5,\n            'max_lat': 37.8,\n            'max_lon': -122.4\n        }\n\n    @task(3)\n    def query_small_bbox(self):\n        \"\"\"Simulate small bounding box queries (most common).\"\"\"\n\n        # Random small area in San Francisco\n        center_lat = random.uniform(37.75, 37.78)\n        center_lon = random.uniform(-122.45, -122.42)\n\n        params = {\n            'min_lat': center_lat - 0.001,\n            'min_lon': center_lon - 0.001,\n            'max_lat': center_lat + 0.001,\n            'max_lon': center_lon + 0.001,\n            'limit': 100\n        }\n\n        with self.client.get(\"/api/v1/roads/bbox\", params=params) as response:\n            if response.status_code != 200:\n                response.failure(f\"Expected 200, got {response.status_code}\")\n\n    @task(1)\n    def query_connected_roads(self):\n        \"\"\"Simulate connected roads queries.\"\"\"\n\n        # Use known road ID for testing\n        road_id = \"test_road_123\"\n\n        with self.client.get(f\"/api/v1/roads/{road_id}/connected\") as response:\n            if response.status_code not in [200, 404]:  # 404 acceptable for missing roads\n                response.failure(f\"Unexpected status: {response.status_code}\")\n\n    @task(1)\n    def health_check(self):\n        \"\"\"Simulate health checks.\"\"\"\n\n        with self.client.get(\"/health\") as response:\n            if response.status_code != 200:\n                response.failure(f\"Health check failed: {response.status_code}\")\n</code></pre>"},{"location":"day07_mock/#professional-development-assessment","title":"Professional Development Assessment","text":""},{"location":"day07_mock/#capstone-evaluation-criteria","title":"Capstone Evaluation Criteria","text":"<p>Technical Excellence (40%): - Code quality, organization, and documentation - Proper use of design patterns and architectural principles - Performance optimization and scalability considerations - Error handling and edge case management</p> <p>API Design (25%): - RESTful design principles and consistency - Request/response validation and error messages - OpenAPI documentation completeness - Proper HTTP status codes and headers</p> <p>Spatial Functionality (20%): - Accuracy of spatial queries and operations - Efficient use of spatial indexes and databases - Proper coordinate system handling - Network topology analysis implementation</p> <p>Production Readiness (15%): - Logging, monitoring, and observability - Configuration management and environment handling - Security considerations and input validation - Testing coverage and quality</p>"},{"location":"day07_mock/#deployment-and-demonstration","title":"Deployment and Demonstration","text":"<p>Live Demo Requirements: 1. Service deployment on cloud infrastructure (AWS/GCP/Azure) 2. Database setup with sample road network data 3. Monitoring dashboard showing real-time metrics 4. Load testing results demonstrating performance targets 5. API documentation with interactive examples</p> <p>Code Review Checklist: - [ ] Clean, readable, and well-documented code - [ ] Comprehensive error handling and validation - [ ] Efficient database queries with proper indexing - [ ] Multi-level caching implementation - [ ] Comprehensive test coverage (&gt;80%) - [ ] Production-ready logging and monitoring - [ ] Security best practices followed - [ ] Performance targets met under load</p>"},{"location":"day07_mock/#industry-context-and-career-advancement","title":"Industry Context and Career Advancement","text":""},{"location":"day07_mock/#real-world-applications","title":"Real-World Applications","text":"<p>Logistics and Transportation: - UPS ORION: Route optimization using real-time road data - Uber/Lyft: Dynamic routing and ETA calculations - FedEx: Fleet management with traffic-aware routing - Amazon Logistics: Last-mile delivery optimization</p> <p>Autonomous Vehicles: - Tesla Autopilot: Real-time map updates for navigation - Waymo: HD mapping for autonomous driving - Cruise: Urban environment mapping and navigation</p> <p>Smart Cities: - Traffic Management: Real-time traffic optimization - Emergency Services: Optimal routing for emergency vehicles - Urban Planning: Infrastructure analysis and planning</p>"},{"location":"day07_mock/#career-progression","title":"Career Progression","text":"<p>Entry Level (0-2 years): - Junior GIS Developer - Spatial Data Analyst - Backend Developer (geospatial focus)</p> <p>Mid Level (2-5 years): - Senior GIS Engineer - Geospatial Software Engineer - Location Intelligence Engineer - Mapping Platform Developer</p> <p>Senior Level (5+ years): - Principal Geospatial Engineer - Geospatial Architecture Lead - Location Services Technical Lead - Director of Spatial Engineering</p> <p>Specialized Roles: - Cartographic Engineer - Spatial Database Administrator - GIS Solutions Architect - Autonomous Vehicle Mapping Engineer</p>"},{"location":"day07_mock/#further-learning-and-resources","title":"Further Learning and Resources","text":""},{"location":"day07_mock/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Spatial Machine Learning: ML algorithms for geospatial data</li> <li>Real-time Streaming: Apache Kafka with spatial data</li> <li>Distributed Computing: Spark and Dask for large-scale geospatial processing</li> <li>Computer Vision: Satellite imagery analysis and processing</li> </ul>"},{"location":"day07_mock/#industry-certifications","title":"Industry Certifications","text":"<ul> <li>AWS Certified Solutions Architect (with geospatial focus)</li> <li>Google Cloud Professional Data Engineer</li> <li>ESRI Technical Certification</li> <li>Open Source Geospatial Foundation (OSGeo) Certification</li> </ul>"},{"location":"day07_mock/#professional-organizations","title":"Professional Organizations","text":"<ul> <li>Open Source Geospatial Foundation (OSGeo)</li> <li>Urban and Regional Information Systems Association (URISA)</li> <li>American Society for Photogrammetry and Remote Sensing (ASPRS)</li> <li>Association of Geographic Information Laboratories in Europe (AGILE)</li> </ul> <p>This capstone project demonstrates mastery of enterprise geospatial engineering principles and prepares you for senior roles in the growing field of location intelligence and spatial computing.</p>"}]}